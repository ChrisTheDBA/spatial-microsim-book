---
title: "Preparing input data"
layout: default
---

# Data preparation {#data-prep}

```{r, echo=FALSE}
library(png)
library(grid)
# Applications of spatial microsimulation - to be completed!

## Updating cross-tabulated census data

## Economic forecasting

## Small area estimation

## Transport modelling

## Dynamic spatial microsimulation

## An input into agent based models
```


```{r, echo=FALSE}
# With the foundations built in the previous chapters now (hopefully) firmly in-place,
# we progress in this chapter to actually *run* a spatial microsimulation model
# This is designed to form the foundation of a spatial microsimulation course.
```


```{r, echo=FALSE}
# This next chapter is where people get their hands dirty for the first time -
# could be the beginning of part 2 if the book's divided into parts. 
```

This chapter focuses on the mininum input datasets needed for the spatial microsimulation.
Correctly loading, manipulating and assessing these datasets will be critical to
the performance of your models and the ease of modifying them to include new
inputs.  Fortunately R is an accomplished tool for data reformatting,
as we shall see. The data objects loaded in the following steps also
provide the basis for Chapter 5, in which we
undertake spatial microsimulation. If you have more or less precise datasets, 
just adapt the methods to your case. Indeed, we present here the philosophy behind 
an often used spatial microsimulation, but each case is specific and it is pretty 
impossible to generalize the theory to each kind of datasets you could have. However, 
for a lot of applications, the datasets available are similar to the ones presented here.

In a previously phase of your study, you have to determine ideally what 
you will need for your research and fix a kind of "target dataset" containing
the needed variables, but only these ones. So, when knowing the form
of your desire dataset, you will not get lost in some potential additional 
and available variables that you will not need in the future. After that, 
you can search data. If you find what you want, you don't need spatial 
microsimulation. But, if you find some information about a sample of your
target population and some with the desired spatial aggregation, but some
in a more aggregated way, than you will have to proceed in a spatial
microsimulation.

```{r, echo=FALSE}
## Accessing the input data
```


As with most spatial microsimulation models, the input consists of
microdata --- a non-geographical individual-level dataset --- and the
constraint table which represents a series of geographical zones. In some cases,
you have a geographical information in your microdata, but not the needed one.
For example, you can have the province of each individual but need its municipality.
The data used in this Chapter and throughout the book can be
downloaded from the book's [GitHub repository](https://github.com/Robinlovelace/spatial-microsim-book). From this page, click on the
'Download ZIP' button to the right and extract the folder into a
sensible place on your computer, such as the Desktop.  From there, you will want
to run R from the project's root directory: open the folder in a file browser
and double click on `spatial-microsim-book.Rproj`.  This should cause RStudio to
be opened at this location, with all the input data files easily accessible to R
through *relative file paths* (previously, you have to install the R-studio software).

```{r studio,  fig.cap="The RStudio interface with the 'spatial-microsim-book' project loaded", echo=FALSE, message=FALSE}
grid.raster(readPNG("figures/studio-basic.png"))
```


To ease reproducibility of the analysis when working with real data, it is
recommended that the process begins with a copy of the *raw* dataset on one's
hard disc.  Rather than modifying this file, modified ('cleaned') versions
should be saved as separate files. This ensures that after any mistakes, one can
always recover information that otherwise could have been lost and makes the
project fully reproducible. In this chapter, however, a relatively clean and
very tiny dataset from SimpleWorld is used.  We will see in Chapter
\ref{CakeMap} how to deal with larger and more messy data.  Here the focus is on the principles.

```{r, echo=FALSE}
# It sounds trivial, but the *precise* origin of the input data
# should be described. Comments in code that loads the data (and resulting publications),
# allows you or others to recall the raw information. # going on a little -> rm
# Show directory structure plot from Gillespie here
# 2. Remove excess information
```

The process of loading, checking and preparing the input datasets for spatial
microsimulation is generally a linear process, encapsulating the following
stages:

1. Decide on the constraint and target variables
2. Loading the data 
3. Re-categorise individual-level data
4. Set variable and value names
5. 'Flatten' individual-level data 

'Stripping down' the datasets so that they only contain the bare essential
information will enable focus on the information that really matters. The input datasets in the example used for this chapter
are already bare, but in real world surveys there may be literally hundreds of variables clogging up the analysis and your mind.
It is good practice to remove
excess data at the outset. Provided you have a good workflow,
keeping all original data files unmodified for future
reference, it will be easy to go back and add extra variables
at a later stage. Following Occam's razor
which favors simple solutions (Blumer et al. 1987),
it is often best to start with a minimum
of data and add complexity subsequently
rather than vice versa.

```{r, echo=FALSE}
# Reference for occam's razor!
```

Spatial microsimulation is the result of combining
individual and aggregate-level data, and both must
be given equal consideration when designing a modelling
strategy. The critical data constraint in many cases will
be the number of *linking variables*. Linking variables
are variables shared between the individual and aggregate level
datasets and can therefore be used to *constrain* the
weights allocated to the individuals for each zone.
If there are no shared variables between the aggregate and
individual-level data, you cannot do a pertinent spatial microsimulation. 
Indeed, in this case, your only alternative is to consider only the
marginal distribution of the variable and make a random selection with 
the distribution used as a probability. However, this implicitly consider
that there are no correlations between the variables defined by this way.
If there are only a few shared variables (e.g. age, sex,
employment status), your options are limited. In each case, the accuracy
of the result depends on a wide range of things are choice you make during the process.
In some cases, if you have age and sex, but need only to add one characteristic, 
these two shared variables will be the most pertinent you could have. Increasingly,
however, there are many linking variables to chose from
as questionnaires become broader and more available. In this
case the choice of constraints becomes important: which and
how many to use,
their order and their relationship to the target variable
should all be considered at the outset when choosing the
input data and constraints.

## Selecting target and constraint variables {#Selecting}


The geographically aggregated data should be the
first consideration when deciding on the
input data. This is because the geographical data
is essential for making the model spatial: without
good geographical data, there is no way to
allocate the individuals to different geographical zones.

The first critical consideration for the constraint data
is coverage: ideally close to 100% of each zone's total
population should be included in the counts. Also, the
survey must have been completed by a number of residents  proportional to the
total number of inhabitants in every geographical zone under consideration. It is important
to recognise that many geographically aggregated datasets
may be unsuitable for spatial microsimulation due to sample
size: geographical information based on a small sample of
a zone's population is not a good basis for
spatial microsimulation.

To take one example,
estimates of the *proportion* of people who do not
get sufficient sleep in the Australian
state of Victoria, from the 2011 VicHealth Indicators
[Survey](http://data.aurin.org.au/dataset?q=sleep),
is not a suitable constraint variable.
**Constraint variables must be integer counts**,
not percentages (or frequencies), and must contain sufficient
categories for each variable. Indeed, ideally, each possible
category must be represented. It is normal not to have
a baby with a high degree qualification, but it is good if
all realizable category. The sleep dataset
meets neither of these criteria. It is based on
a small sample of individuals (on average 300 per
geographic zone, less than 1% of the total population).
Also it is a binary variable, dividing people into
'adequate' and 'inadequate' sleep categories. Instead,
geographical datasets from the 2011 Census should be
used. These may contain fewer variables, but they will
provide counts for different categories and have almost
100% coverage of the population.

To continue with the Australian example, imagine we are
interested only in Census data at the SA2 level,
the second smallest unit for the release of Census data.
A browse of the available datasets on an
[online portal](http://data.aurin.org.au/dataset?q=2011-08+SA2)
reveals that information is available on a wide range of
topics, including industry of employment, education level
and total personal weekly income (divided into 10 bands from
$1 - $199 to $2000+). Which of these dozens of variables do we
select of the analysis? It depends on the research question and
the target variable or variables of interest.

If we are interested in geographic variability in economic
inequality, for example, the income data would first on the
list to select. Additional variables would likely include
level of education, type of employment and age, to discover
the wider context of the problem. If the research question
related to energy and sustainability, to take another example,
variables related to distance travelled to work and housing
would be of greater relevance. In each of these examples
the *target variable* determines the selection of constraints.
The target variable is the thing that we would like spatial
microdata on, as illustrated by the following two research
questions.

How does income inequality vary from place to place?
To answer this question it is not sufficient to have
aggregate-level statistics (e.g. average household income).
So income per person must be simulated for each zone, using
spatial microsimulation. Sampling from a national population
provides a more realistic distribution than simply assuming
every person of a given income band earns a certain amount:
this is clearly not the case as income distributions are
not flat (they tend to have positive skew). 

How does energy use vary over geographical space?
This question is more complicated as there is no single
variable on 'energy use' that is collected by statistical
agencies at the aggregate, let alone individual, level.
Rather, energy use is a *composite target variable* that
is composed of energy use in transport, household heating
and cooling and other things. For each type of energy use,
the question must eventually be simplified to coincide with
survey questions that are actually asked in Census surveys
(e.g. 'where do you work?', from which distance travelled
to work may be ascertained). Such considerations should guide
the selection of aggregate level (generally Census-based)
geographic count data. Of course, available datasets
vary greatly from country to country so selection of
appropriate datasets is highly context dependent.

A parallel stage is to consider the individual-level datasets
that are available. Again, if there are dataset

```{r, echo=FALSE}
# (Dumont, 2014). What is the end of this sentence? Can be deleted or?
```

The following considerations should inform the selection of
individual-level microdata:

- Linking variables: are there enough
variables in the individual-level data that are
also present in the geographically aggregated constraints?
Even when many linking variables are available, it is
important to determine the fit between the categories in each.
If age is reported in five year bands in the aggregate-level
data, but only in 20 year bands in the individual-level data,
for example,
this could be problematic if the research question is highly
aged dependent.
- Representiveness: is the sample population representative of the areas under investigation?
- Sample size and diversity: the input dataset must be sufficiently large and diverse to mitigate the *empty cell* problem.
- Target variables: are these, or proxies for them, included in the dataset?

In addition to these essential characteristics of the
individual-level dataset, there are qualities that are desirable but not  required. These include:

- Continuity of the survey: will the survey be repeated on
a regular basis into the future? If so, the analysis can form
the basis of ongoing work to track change over time, perhaps
as part of a *dynamic microsimulation model*.
- Geographic variables: although it is the purpose of spatial
microsimulation to allocate geographic variables to
individual-level data, it is still useful to have some
geographic information. In many national UK surveys, for
example, the region (the coarsest geographical level)
of the respondent is reported. This information can be useful in
identifying the extent to which the difference between the
geographic extent of the survey and microsimulation study area
affects the results. This is an understudied area of knowledge
where more work is needed.

In terms of the *number* of constraints that is appropriate
there is no 'magic number' that is correct in every case.
It is also important to note that the number of variables
is not a particularly good measure of how well constrained
the data will be. The total number of *categories* used to
constrain the weights is in fact more important.

For example, a model constrained by two variables,
age and income bands,
each containing 10 categories (20 constraint categories
in total) will be better constrained
than a model constrained by 5 binary variables such as
male/female, young/old etc. That is not to say that the
former set of constraints is *better* (as emphasised above,
that depends on the research question), simply that the
weights will be more finely constrained.

A special type of
constraint variable is **cross-tabulated** categories.
This involves subdividing the categories in one variable
by the categories of another. Cross tabulated
constraint variables are well describing the reality 
and best seen as a single variable.
```{r, echo=FALSE}
# (Dumont, 2014). Are you sure about that? 
Because in one way, individual level data can be consider as a big
cross tabulated data set, but, I don't know... no so convinced
```
If there are 5 age categories and 2 sex categories, this
can be seen as a single constraint variable (age/sex)
with 10 categories. Clearly in terms of retaining
detailed local information (e.g. all young males moving out
of a zone), cross-tabulated variables are preferable.
This raises the question? what happens when a single
variable (e.g age) is used in multiple cross-tabulated
constraints (e.g. age/sex and age/income). In this case
```{r, echo=FALSE}
# (Dumont, 2014). Pay attention, for the moment it is the first 
time that IPF is mentionned, so the reader do not know what it is
```
IPF can still be used, but the result may not converge
to a single answer because the gender distribution may
be disrupted by the age/income constraint. Further work
is needed to test this but from theory we can be sure:
a three way cross-tabulated constraint (age/sex/income)
would be ideal in this case. Note that a contingency table 
give more information than two marginal distributions.

Even when measuring constraint variables by categories rather
than the cruder measure of number variables, there is still
no concensus about the optimal amount. 
Norman (1999) advocates
including the ``maximum amount of
information in the constraints'', implying that
the more constraint categories the merrier.
Harland (2012)
warns that over-constraining the model can cause
problems. In any case, there is a clear link between
the quality and quantity of constraint variables used
in the model and the fidelity of the output microdata.

```{r, echo=FALSE}
# TODO: link to the location of a discussion of the empty cell problem
```

If constraint variables come from different sources, check the coherence 
of these datasets. In fact, if the total number of individuals is not
the same in all datasets, the procedure will get lost.


After suitable constraint variables have been chosen ---
and remember that the constraints can be changed at a later
stage to improve the model or for testing ---
the next stage is to load the data.
Following the SimpleWorld example, we
load the individual-level dataset first. This
is because individual-level data from surveys
is often
more difficult
to format. Individual-level datasets are often larger
and more complex than the constraint data, which
are usually simply integer counts of different categories.
Of course, it is possible that the
data you have are not suitable for
spatial microsimulation because they lack linking variables.
We assume that you have already checked this. The
checking process for the datasets used in this chapter is simple: both aggregate
and individual-level tables contain age (in continuous form
in the microdata, as two categories in the aggregate data)
and sex, so they can by combined. 
Loading the data involves transfering information from a
local hard disc into R's *environment*, where
it is available in memory.

## Loading input data {#Loading} 

Real-world individual-level data may be provided in a variety of formats
but ultimately needs to be loaded into R as a *data frame* object.

In this case the dataset is loaded from a `.csv` file:

```{r}
# Load the individual-level data
ind <- read.csv("data/SimpleWorld/ind.csv") 
class(ind) # verify the data type of the object
ind # print the individual-level data
```

```{r, echo=FALSE}
### Loading and checking aggregate-level data
```

Constraint data are usually made available one variable at a time,
so these are read in one file at a time:

```{r}
con_age <- read.csv("data/SimpleWorld/age.csv")
con_sex <- read.csv("data/SimpleWorld/sex.csv")
```

We have loaded the aggregate constraints. As with the individual level data, is
worth inspecting each object to ensure that they make sense before continuing.
Taking a look at `age_con`, we can see that this data set consists of 2
variables for 3 zones:

```{r}
con_age
```

This tells us that there 12, 10 and 11 individuals in zones 1, 2 and 3,
respectively, with different proportions of young and old people. Zone 2, for
example, is heavily dominated by older people: there are 8 people over 50 whilst
there are only 2 young people (under 49) in the zone.

Even at this stage there is a potential for errors to be introduced.  A classic
mistake with areal data is that the order in which zones are loaded changes from
one table to the next. The constraint data should therefore come with some kind of *zone
id*, an identifying code that will eventually allow the attribute data to be
combined with polygon shapes in GIS software.

```{r, echo=FALSE}
# Make the constraint data contain an 'id' column, possibly scrambled 
```

If we're sure that the row numbers match between the age and sex tables (we are
sure in this case), the next important test is to check that the total
populations are equal for both sets of variables.  Ideally both the *total*
study area populations and *row totals* should match. If the *row totals* match,
this is a very good sign that not only confirms that the zones are listed in the
same order, but also that each variable is sampling from the same *population
base*. These tests are conducted in the following lines of code:

```{r}
sum(con_age)
sum(con_sex) 

rowSums(con_age)
rowSums(con_sex)
rowSums(con_age) == rowSums(con_sex)
```

The results of the previous operations are encouraging. The total population is
the same for each constraint overall and for each area (row) for both
constraints.  If the total populations between constraint variables do not match
(e.g. because the population bases are different) this is problematic.
Appropriate steps to normalise the errant constraint variables are described in
the CakeMap Chapter (\ref{CakeMap}).

## Subsetting to remove excess information {#subsetting-prep}

In the above code, `data.frame` objects containing precisely the information
required for the next stage were loaded.  More often, superfluous information
will need to be removed from the data and subsets taken. It is worth removing
superfluous variables earl, to avoid over-complicating and slowing-down the
analysis.  For example, if `ind` had 100 variables of which only the 1st, 3rd and 4th were of
interest (in that they match the constraint variables), the following command could be used to update the object. Only the relevant variables corresponding to
columns (1, 3 and 4 in this case) are retained: `ind <- ind[, c(1, 3, 4)]`.
Alternatively, `ind$age <- NULL` removes the age variable.

Although `ind` is small and simple it will behave in the same way as a much
larger dataset, providing opportunities for testing subsetting syntax in R.  It
is common, for example, to take a subset of the working *population base*: those
aged 16 and 74 in full-time employment. Methods for doing this are provided in
the Appendix (
[](#subsetting)
).

## Re-cateorising individual-level variables {#re-categorise}

Before transforming the individual-level dataset `ind` into a form that can be
compared with the aggregate-level constraints, we must ensure that each dataset
contains the same information. It can be more challenging to re-categorise
individual-level variables than to re-name or combine aggregate-level variables,
so the former should usually be set first.  An obvious difference between the
individual and aggregate versions of the `age` variable is that the former is of
type `integer` whereas the latter is composed of discrete bins: 0 to 49 and 50+.
We can categories the variable into these bins using 
`cut()`:^[The
combination of curved and square brackets in the output from the `cut` function
may seem strange but this is
an International Standard: curved brackets mean 'excluding' and square brackets
mean 'including'. The output `(49, 120]`, for example, means 'from
49 (excluding 49, but including 49.001) to 120 (including the exact value 120)'.
See http://en.wikipedia.org/wiki/ISO_31-11 for more
on international standards on mathematical notation.]

```{r}
# Test binning the age variable
cut(ind$age, breaks = c(0, 49, 120))
```

If we wanted to change these category labels to something more readable,
we can do this by adding another argument to the `cut` function:

```{r}
# Convert age into a categorical variable
(ind$age <- cut(ind$age, breaks = c(0, 49, 120),
  labels = c("a0_49", "a50+"))) 
```

Users should beware that `cut` results in a vector of class *factor*, which
can cause problems later down the line.

## Matching individual and aggregate level data names {#matching}

Before combining the newly recategorised individual-level data with the
aggregate constraints, it is useful to for the category labels to match up.
This may seem trivial, but will save time in the long run. Here is the problem:

```{r}
levels(ind$age)
names(con_age)
```

Note that the names are subtly different. To solve this issue, we can
simply change the names of the constraint variable, assuming they
are in the correct order:

```{r}
names(con_age) <- levels(ind$age) # rename aggregate variables
```

With both the age and sex constraint variable names now matching the
category labels of the individual-level data, we can proceed to create a
single constraint object we label `cons`. We do this with `cbind()`:

```{r}
cons <- cbind(con_age, con_sex)
cons[1:2, ] # display the constraints for the first two zones
```

## 'Flattening' the individual level data {#flattening}

We have made steps towards combining the individual and aggregate datasets and
now only need to deal with 2 objects (`ind` and `cons`) which now share
category and variable names.
However, these datasets cannot possibly be compared because they are of different dimensions:

```{r}
dim(ind)
dim(cons)
```

The above code confirms this: we have one individual-level dataset comprising 5
individuals with 3 variables (2 of which are constraint variables and the other an ID) and one
aggregate-level constraint table called `cons`, representing 3 zones
with count data for 4 categories across 2 variables.

The dimensions of at least one of these objects must change
before they can be easily compared. To do this
we 'flatten' the individual-level dataset;
this means increasing its width
to match the constraint data.
This is a two-stage process. First,
`model.matrix()` is used to expand each variable into the number of columns as there are categories in each and assign. 
Knoblauch and Maloney (2012) provide a lengthier description of this
which is available online, for free.

Second, `colSums()` is used to take the sum of each column.^[As we shall see in Section \ref{ipfp},
only the former of these is needed if we use the
**ipfp** package for re-weighting the data, but both are presented to enable
a better understanding of how IPF works.]

```{r}
cat_age <- model.matrix(~ ind$age - 1)
cat_sex <- model.matrix(~ ind$sex - 1)[, c(2, 1)]

 # Combine age and sex category columns into single data frame
(ind_cat <- cbind(cat_age, cat_sex)) # brackets -> print result
```

Note that second call to `model.matrix` is suffixed with `[, c(2, 1)]`.
This is to swap the order of the columns: the column variables are produced
from `model.matrix` is alphabetic, whereas the order in which the variables
have been saved in the constraints object `cons` is `male` then `female`.
Such subtleties can be hard to notice yet completely change one's results
so be warned: the output from `model.matrix` will not always be compatible
with the constraint variables.

To check that the code worked properly,
let's count the number of individuals
represented in the new `ind_cat` variable, using `colSums`:

```{r}
colSums(ind_cat) # view the aggregated version of ind
ind_agg <- colSums(ind_cat) # save the result
```

The sum of both age and sex variables is 5 
(the total number of individuals): it worked! 
Looking at `ind_agg`, it is also clear that the object has the same 'width',
or number of columns,
`cons`. This means that the individual-level data can now be compared with
the aggregate-level data. We can check this by inspecting
each object (e.g. via `View(ind_agg)`). A more rigorous test is to see
if `ind_agg` can be combined with `ind_agg`, using `rbind`:

```{r}
rbind(cons[1,], ind_agg) # test compatibility of ind_agg and cons
```

If no error message is displayed on you computer, the answer is yes.
This shows us a direct comparison between the number of people in each
category of the constraint variables in zone and and in the individual level
dataset overall. Clearly, the fit is not very good, with only 5 individuals
in total existing in `ind_agg` (the total for each constraint) and 12
in zone 1. We can measure the size of this difference using measures of
*goodnes of fit*. A simple measure is total absolute error (TAE), calculated in this
case as `sum(abs(cons[1,] - ind_agg))`: the sum of the positive differences
between cell values in the individual and aggregate level data.

The purpose of the *reweighting* procedure in spatial microsimulation is
to minimise this difference (as measured in TAE above)
by adding high weights to the most representative individuals.

# References
