---
title: "CakeMap"
layout: default
---

# Spatial microsimulation in the wild {#CakeMap}

By now we have developed a good understanding of what spatial microsimulation
is, its applications and how it works. We have seen something of its
underlying theory and its implementation in R. But how can
the method be applied 'in the wild', on real datasets?

The purpose of this chapter is to answer this question
using real data to estimate cake consumption in
different parts of Leeds, UK. The example is deliberately rather absurd to make
it more memorable. The steps are presented in a generalisable way, to be
applicable to a wide range of datasets.

The input microdataset is a randomized ('jumbled') subset of the
2009 [Dental Health Survey](http://data.gov.uk/dataset/adult_dental_health_survey),
(DHS) which covers England, Northern Ireland and Wales. 1173 variables are available
in the DHS, many of which are potentially interesting
target variables not available at the local level.
These include weekly income, plaque buildup, and
oral health behaviours.
Potential linking variables include socio-economic
classification, and dozens of variables related to oral health.

```{r, echo=FALSE}
# (MD): Could be good to say the proportion of the population 
# included in this survey
```

In terms of constraint variables, we are more
limited: the Census is the only survey that provides
count data at the small area
level in the UK. Thus the 'domain' of available input data,
related to our research question involves two sources:

1. Non-geographical
individual-level survey, DHS --- the *microdata*.
2. Geographically
aggregated categorical count data from the census --- the *constraint tables*.

As discussed in Chapter  must first decide which variables should be used to link the two. We must
select the constraints from available linking variables.

## Selection of constraint variables

The selection of linking variables should not be arbitrary pre-ordained
by preconceptions. The decision of which constraints to use to
allocate individuals to zones should be context dependent:
if the research is on social exclusion, for example,
many variables could potentially be of interest, ranging from car ownership
and house tenancy though to age, gender and religion.
Often constraint variables must be decided not based on what 
would be ideal, but what datasets are available. The selection criteria
will vary from one project to the next, but there are some overriding
principles that apply to most projects:

1. **More the merrier**: each additional constraint used for
will further differentiate the spatial microdata from the input microdata.
If gender is the only constraint used, for example, the spatial microdata will
simply be a repetition of the input microdata but with small differences in
the gender ratio from one zone to the next. If five constraints are used
(e.g. age, gender, car ownership, tenancy and religion), the differences
between the spatial microdata from one zone to the next will be much more
pronounced and probably useful.

2. **Relevance to the target variable**: often spatial microsimulation is
used to generate local estimates of variables about which little geographically
disaggregated information is available. Income is a common example: we have
much information about income distributions, but little information about how
average values (let alone the distribution) of income varies from one small area
to the next. In this case income is the target variable. Therefore constraints
must be selected which are closely related to income for the output to resemble
reality. This is analogous to multiple regression (which can also be used
to estimate average income at the local level), where the correct
*explanatory variables* (i.e. constraint variables in spatial microsimulation)
must be selected to effectively predict the *dependent variable*. As with
regression models, there are techniques which can be used to identify the most
suitable constraint variables for a given target variable.

```{r, echo=FALSE}
# TODO: See section x
# may need to spell-out what constraint categories are for below
```

3. **Simplicity**: this criterion to some extent contradicts the first. Sometimes
more constraints do not result in better spatial microdata and problems associated
with 'over-fitting' can emerge. Spatial microsimulation models based on many
tens of constraint categories will take longer to run and require more time
to develop and modify. In addition, the chances of an error being
introduced during every phase of the project is increased with each additional
constraint. The extent to which increasing the number of constraint categories
improves the results of spatial microsimulation, either with additional
variables or by using cross-tabulated constraints (e.g. age/sex)
instead of single-variable constraints, has yet to be explored. It is therefore
difficult to provide general rules of thumb regarding simplicity other than
'do not over-complicate the model with excessive constraint variables and
constraints'.

So, we always need to reach an equilibrium between these principles. 
Indeed, we have to take into account `enough` and `pertinent` variables,
but not to many.

To exemplify these principles, let us consider the constraint variables available
in the CakeMap datasets. Clearly only variables available both in the individual-level and 
aggregate-level datasets can be chosen from. Five variables assigned to each
of the 916 individuals are available from the individual-level data,
about which data is also available from the census:

- 'Car': The number of working cars in the person's household.
- 'Sex' and 'ageband4': Gender and age group, in two separate variables. 
Age is divided into 6 groups ranging from '16--24' to '65--74'.^[R tip:
This information can be seen, once the dataset is loaded, by entering
`unique(ind$ageband4)` or, to see the counts in each category,
`summary(ind$ageband4)`. Because the variable is of type `factor`,
`levels(ind$ageband4)` will also provide this information.]
- 'NSSEC': National Statistics Socio-economic Classification: an categorical
variable classifying the individual's work into one of 10 groups including '97',
which means 'no answer' (`NA`).
- 'NCakes': the target variable, reported number of times that the respondent
consumes cake each week.

All of these variables, except for 'NCakes', have a corresponding constraint
variable to be loaded for the 124 Wards that constitute the Leeds Local Authority
in the UK. In real datasets it is rarely the case that the categories of the
individual and aggregate level data match perfectly from the outset and this
is the first problem we must overcome before running a spatial microsimulation
model of cake consumption in Leeds.

The code needed to run the main part of the example is contained within
'CakeMap.R'.
Note that this script makes frequent reference to files contained
in the folder 'data/CakeMap', where input data and processing scripts
for the project are stored.

## Preparing the input data {#CakePrep}

Often spatial microsimulation is presented in a way that suggests the data
arrived in a near perfect state, ready to be inserted directly into the model.
This is rarely the case: usually, one must spend time 
loading the data into R, dealing with missing values,
re-coding categorical variables and column
names, binning continuous variables and subsetting from the microdataset. In a
typical project, data preparation can take as long as the analysis
stage.  This section builds on Chapter 3 to illustrate strategies for data
cleaning on a complex project. To learn about the data cleaning steps that may
be useful to your data, we start from the beginning in this section, with a real
(anonymised) dataset that was downloaded from the internet.

The raw constraint variables for CakeMap were downloaded from
the Infuse website (http://infuse.mimas.ac.uk/).
These, logically enough, are stored in the 'cakeMap/data/' directory
as .csv files and contain the word 'raw' in the file name
to identify the original data. The file 'age-sex-raw.csv', for example is the raw
age and sex data that was downloaded. As the screenshot in
Figure 6.1 illustrates, these datasets are rather verbose and
require pre-processing. The resulting 'clean' constraints are saved in files such as
'con1.csv', which stands for 'constraint 1'.

![Example of raw aggregate-level input data for CakeMap aggregate data, downloaded from
http://infuse.mimas.ac.uk/.](figures/raw-data-screenshot.jpeg)

```{r, echo=FALSE}
# A little long-winded - cut down?
# stage ([Wickham, 2014](http://vita.had.co.nz/papers/tidy-data.html)).  This
# section builds on
# [Chapter 3](#DataPrep)
#   to illustrate strategies for data
# ([2014b](http://vita.had.co.nz/papers/tidy-data.html)) provides a more
```

To ensure reproducibility in the process of converting the raw data into
a form ready for spatial microsimulation, all of the steps have been saved.
Take a look at the R script files 'process-age.R', 'process-nssec.R' and
'process-car.R'. The contents of these scripts should provide an insight
into methods for data preparation in R. Wickham
(2014b) provides a more
general introduction to data reformatting.
The most difficult input dataset to deal with, in this example,
is the age/sex constraint data.
The steps used to clean it are saved in 'process-age.R', in the `data/CakeMap/`
folder. Take a look through this
file
and try to work out what is going on: the critical stage is grouping single year
age bands into larger groups such as 16--24.

```{r, echo=FALSE}
# TODO: add more here...
```

The end result of 'process-age.R' is a 'clean' .csv file, ready to be loaded and
used as the input into our spatial microsimulation model. Note that the last
line of 'process-age.R' is `write.csv(con1, "con1.csv", row.names = F)`.  This
is the first constraint that we load into R to reweight the individual-level
data in the next section.
The outputs from these data preparation steps are named
'con1.csv' to 'con3.csv'. For simplicity, all these were merged (by
'load-all.R')
into a single dataset called 'cons.csv'. All the input data for this section are
thus loaded with only two lines of code:

```{r}
ind <- read.csv("data/CakeMap/ind.csv")
cons <- read.csv("data/CakeMap/cons.csv")
```

Take a look at these input data using the techniques learned in the previous
section. To test your understanding, try to answer the following questions:

- What are the constraint variables?
- How many individuals are in the survey microdataset?
- How many zones will we generate spatial microdata for?

For bonus points that will test your R skills as well as your practical knowledge
of spatial microsimulation, try constructing queries in R that will automatically
answer these questions.

It is vital to understand the input datasets before trying to model them, so
take some time exploring the input. Only when these
datasets make sense (a pen and paper can help here, as well as R!) is it time to
generate the spatial microdata.

## Performing IPF on CakeMap data {#CakeIPF}

The `ipfp` reweighting strategy is concise, generalisable and computationally
efficient. On a modern laptop, the `ipfp` method was found to be *almost 40
times faster* than the 'IPFinR' method (section 4.1;
Lovelace, 2014) over 20
iterations on the CakeMap data,
completing in 2 seconds instead of over 1 minute.  This is a huge
time saving!^[These tests were conducted
using the `microbenchmark()` commands found
towards the end of the 'CakeMap.R' file.
The second of these benchmarks depends on files from
`smsim-course` (Lovelace, 2014), the repository of which can be downloaded from
(https://github.com/Robinlovelace/smsim-course).]

Thanks to the preparatory steps described above,
the IPF stage can be run on a single line. After the datasets are loaded in
the first half of 'CakeMap.R', the following code creates the weight matrix:

```{r, echo=FALSE}
source("R/CakeMap.R")
```

```{r, eval=FALSE}
weights <- apply(cons, 1, function(x)
  ipfp(x, ind_catt, x0, maxit = 20))
```

As with the SimpleWorld example, the correlation^[The function "cor" of R calculate the 
correlation coefficient of Pearson that measure the force of a linear correlation
between the two variables. Included between -1 and 1, the best value in 
our case is 1.] between the
constraint table and the aggregated results of the spatial microsimulation
can be checked to ensure that the reweighting process has worked correctly.
This demonstrates that the process has worked with an *r* value above 0.99:

```{r}
cor(as.numeric(cons), as.numeric(ind_agg))
```

This value is very close to 1, so we can consider that there is a big linear
correlation between the result and the constraint. We can verify it by plotting:

```{r}
plot(ind_agg,cons,pch=20,xlab="Result of the simulation",ylab="Constraint",main="Correspondance between and the simulation and the constraints")
```

With the best fit, we would have all constraints equal to the simulation, so a 
perfect line (this is why we consider a `linear` correlation). Only few points
are outside the area of the line. Note that we have 4,871,391 inhabitants in the
simulation and only 6 more in the constraints. That represents a relative
difference of less than $10^{-6}$. If we count category per category the
difference between the simulation and the constraint, we obtain a difference 
of 26,445.57, so 0.54%.

This is for the basic comparison, a more detailed analysis of the quality 
of the results is present in the next chapter.

## Integerisation {#CakeINT}

As before, weights of the IPF procedure are fractional, so must be *integerised*
to create whole individuals. The code presented in chapter 4 requires little
modification to do this: it is your task to convert the weight matrix generated
by the above lines of code into a spatial microdataset called, as before,
`ints_df` (hint: the `int_trs` function in 'R/functions.R' file will help).
The spatial microdata generated in 'R/CakeMapInts.R' contains the same information
as the individual-level dataset, but with the addition of the 'zone' variable,
which specifies which zone each individual inhabits.

The spatial microdata is thus *multilevel* data, operating at one level on
the level of individuals and at another at the level of zones. To generate
summary statistics about the individuals in each zone, functions must be
run on the data, one zone (group) at a time. `aggregate` provides one
way of doing this. After converting the 'NSSEC' socio-economic class variable
into a suitable numeric variable, `aggregate` can be used to identify the
variability in social class in each zone, using the `by =` argument to
specify how the results are grouped depending on the zone each individual inhabits:

```{r, echo=TRUE, message=FALSE}
source("R/CakeMapInts.R")
```

```{r, eval=FALSE}
aggregate(ints_df$NSSEC, by = list(ints_df$zone), sd,
  na.rm = TRUE)
```

```
##    Group.1        x
## 1         1 1.970570
## 2         2 2.027638
## 3         3 2.019839
```

```{r, eval=FALSE}
#(MD) Interpret the sd and say why we calculate it
```

In the above code the third argument refers to the function to be applied to the
input data. The fourth argument is simply an argument of this function, in this
case instructing the standard deviation function (`sd`) to ignore all `NA` values.
An alternative way to perform this operation, which is faster and more
concise, is using `tapply`:

```{r, eval=FALSE}
tapply(ints_df$NSSEC, ints_df$zone, sd, na.rm = TRUE)
```

Note that operations on `ints_df` can take a few seconds to complete.
This is because the object is large, taking up much RAM on
the computer. This can be seen by asking `object.size(ints_df)` or
`nrow(ints_df)`. The latter shows we have created a spatial microdataset
of 1.6 million individuals! Try comparing
this result with the size of the original survey dataset 'ind'. Keeping an eye
on such parameters will ensure that the model does not generate datasets too
large to handle.

Next we move on to a vital
consideration in spatial microsimulation models such as CakeMap: model checking and validation.

```{r, echo=FALSE}
save.image("cache-CakeMap.RData")
```

```{r, eval=FALSE}
#(MD) Make it with CO to compare the results?
```


