---
title: "CakeMap"
layout: default
---

# Spatial microsimulation in the wild {#CakeMap}

By now we have developed a good understanding of what spatial microsimulation
is, its applications and how it works. We have seen something of its
underlying theory and its implementation in R. But how can
the method can be applied 'in the wild', on real datasets?

The purpose of this chapter is to answer this question
using real data to estimate cake consumption in
different parts of Leeds, UK. The example is deliberately rather absurd to make
it more memorable. The steps are presented in a generalisable way, to be
applicable to a wide range of datasets.

The input microdataset is a randomised ('jumbled') subset of the
2009 [Dental Health Survey](http://data.gov.uk/dataset/adult_dental_health_survey),
(DHS) which covers England, Northern Ireland and Wales. xx variables are available
in the DHS including xx, xx and xxx. In terms of constraint variables, we are more
limited: the Census is the only survey that provides count data at the small area
level. From this 'domain' of available input data, with the non-geographical
individual-level DHS on one side --- the *microdata* --- and the geographically
aggregated categorical count data from the census --- the *constraint tables* ---
we must first decide which variables should be used to link the two. We must
select the 'linking variables', otherwise know as constraint variables.

## Selection of constraint variables

The selection of linking variables should not be arbitrary pre-ordained
by preconceptions. The decision of which constraints to use to
allocate individuals to zones should be context dependent:
if the research is on social exclusion, for example,
many variables could potentially be of interest, ranging from car ownership
and house tenancy though to age, gender and religion.
Often constraint variables must be decided not based on what would
would be ideal, but what datasets are available. The selection criteria
will vary from one project to the next, but there are some overriding
principles that apply to most projects:

1. **More the merrier**: each additional constraint used for
will further differentiate the spatial microdata from the input microdata.
If gender is the only constraint used, for example, the spatial microdata will
simply be a repetition of the input microdata but with small differences in
the gender ratio from one zone to the next. If five constraints are used
(e.g. age, gender, car ownership, tenancy and religion), the differences
between the spatial microdata from one zone to the next will be much more
pronounced and probably useful.

2. **Relevance to the target variable**: often spatial microsimulation is
used to generate local estimates of variables about which little geographically
disaggregated information is available. Income is a common example: we have
much information about income distributions, but little information about how
average values (let alone the distribution) of income varies from one small area
to the next. In this case income is the target variable. Therefore constraints
must be selected which are closely related to income for the output to resemble
reality. This is analogous to multiple regression (which can also be used
to estimate average income at the local level), where the correct
*explanatory variables* (i.e. constraint variables in spatial microsimulation)
must be selected to effectively predict the *dependent variable*. As with
regression models, there are techniques which can be used to identify the most
suitable constraint variables for a given target variable.

```{r, echo=FALSE}
# TODO: See section x
# may need to spell-out what constraint categories are for below
```

3. **Simplicity**: this criterion to some extent contradicts the first. Sometimes
more constraints do not result in better spatial microdata and problems associated
with 'over-fitting' can emerge. Spatial microsimulation models based on many
tens of constraint categories will take longer to run and require more time
to develop and modify. In addition, the chances of an error being
introduced during every phase of the project is increased with each additional
constraint. The extent to which increasing the number of constraint categories
improves the results of spatial microsimulation, either with additional
variables or by using cross-tabulated constraints (e.g. age/sex)
instead of single-variable constraints, has yet to be explored. It is therefore
difficult to provide general rules of thumb regarding simplicity other than
'do not overcomplicate the model with excessive constraint variables and
constraints'.

To exemplify these principles, let us consider the constraint variables available
in the CakeMap datasets. Clearly only variables available both in the individual-level and 
aggreate datasets can be chosen from. Five variables assigned to each
of the 916 individuals are available from the individual-level data,
about which data is also available from the census:

- 'Car': The number of working cars in the person's household.
- 'Sex' and 'ageband4': Gender and age group, in two separate variables. 
Age is divided into 6 groups ranging from '16--24' to '65--74'.^[R tip:
This information can be seen, once the dataset is loaded, by entering
`unique(ind$ageband4)` or, to see the counts in each category,
`summary(ind$ageband4)`. Because the variable is of type `factor`,
`levels(ind$ageband4)` will also provide this information.]
- 'NSSEC': National Statistics Socio-economic Classification: an categorical
variable classifying the individual's work into one of 10 groups including '97',
which means 'no answer' (`NA`).
- 'NCakes': the target variable, reported number of times that the respondent
consumes cake each week.

All of these variables, except for 'NCakes', have a corresponding constraint
variable to be loaded for the 124 Wards that constitute the Leeds Local Authority
in the UK. In real datasets it is rarely the case that the categories of the
individual and aggregate level data match perfectly from the outset and this
is the first problem we must overcome before running a spatial microsimulation
model of cake consumption in Leeds.

The code needed to run the main part of the example is contained within
'CakeMap.R'.
Note that this script makes frequent reference to files contained
in the folder 'data/CakeMap', where input data and processing scripts
for the project are stored.

## Preparing the input data {#CakePrep}

Often spatial microsimulation is presented in a way that suggests the data
arrived in a near perfect state, ready to be inserted directly into the model.
This is rarely the case: usually, one must spend time 
loading the data into R, re-coding categorical variables and column
names, binning continuous variables and subsetting from the microdataset. In a
typical project, data preparation can take as long as the analysis
stage.  This section builds on Chapter 3 to illustrate strategies for data
cleaning on a complex project. To learn about the data cleaning steps that may
be useful to your data, we start from the beginning in this section, with a real
(anonymised) dataset that was downloaded from the internet.

The raw constraint variables for CakeMap were downloaded from
the Infuse website (http://infuse.mimas.ac.uk/).
These, logically enough, are stored in the 'cakeMap/data/' directory
as .csv files and contain the word 'raw' in the file name
to identify the original data. The file 'age-sex-raw.csv', for example is the raw
age and sex data that was downloaded. As the screenshot in
Figure 3 illustrates, these datasets are rather verbose and
require pre-processing. The resulting 'clean' constraints are saved in files such as
'con1.csv', which stands for 'constraint 1'.

![Example of raw aggregate-level input data for CakeMap aggregate data, downloaded from
http://infuse.mimas.ac.uk/.](figures/raw-data-screenshot.jpeg)

```{r, echo=FALSE}
# A little long-winded - cut down?
# stage ([Wickham, 2014](http://vita.had.co.nz/papers/tidy-data.html)).  This
# section builds on [Chapter 3](#DataPrep) to illustrate strategies for data
# ([2014b](http://vita.had.co.nz/papers/tidy-data.html)) provides a more
```

To ensure reproducibility in the process of converting the raw data into
a form ready for spatial microsimulation, all of the steps have been saved.
Take a look at the R script files 'process-age.R', 'process-nssec.R' and
'process-car.R'. The contents of these scripts should provide an insight
into methods for data preparation in R. Wickham
(2014b) provides a more
general introduction to data reformatting.
The most difficult input dataset to deal with is the age/sex constraint data.
The steps used to clean it are saved in 'process-age.R', in the `data/CakeMap/`
folder. Take a look through this
file
and try to work out what is going on: the critical stage is grouping single year
age bands into larger groups such as 16--24.

```{r, echo=FALSE}
# TODO: add more here...
```

The end result of 'process-age.R' is a 'clean' .csv file, ready to be loaded and
used as the input into our spatial microsimulation model. Note that the last
line of 'process-age.R' is `write.csv(con1, "con1.csv", row.names = F)`.  This
is the first constraint that we load into R to reweight the individual-level
data in the next section.
The outputs from these data preparation steps are named
'con1.csv' to 'con3.csv'. For simplicity, all these were merged (by
'load-all.R')
into a single dataset called 'cons.csv'. All the input data for this section are
thus loaded with only two lines of code:

```{r}
ind <- read.csv("data/CakeMap/ind.csv")
cons <- read.csv("data/CakeMap/cons.csv")
```

Take a look at these input data using the techniques learned in the previous
section. To test your understanding, try to answer the following questions:

- What are the constraint variables?
- How many individuals are in the survey microdataset?
- How many zones will we generate spatial microdata for?

For bonus points that will test your R skills as well as your practical knowledge
of spatial microsimulation, try constructing queries in R that will automatically
answer these questions.

It is vital to understand the input datasets before trying to model them, so
take some time exploring the input. Only when these
datasets make sense (a pen and paper can help here, as well as R!) is it time to
generate the spatial microdata.

## Performing IPF on CakeMap data {#CakeIPF}

The `ipfp` reweighting strategy is concise, generalisable and computationally
efficient. On a modern laptop, the `ipfp` method was found to be *almost 40
times faster* than the 'IPFinR' method (section 4.1;
Lovelace, 2014) over 20
iterations on the CakeMap data,
completing in 2 seconds instead of over 1 minute.  This is a huge
time saving!^[These tests were conducted
using the `microbenchmark()` commands found
towards the end of the 'CakeMap.R' file.
The second of these benchmarks depends on files from
`smsim-course` (Lovelace, 2014), the repository of which can be downloaded from
(https://github.com/Robinlovelace/smsim-course).

Thanks to the preparatory steps described above,
the IPF stage can be run on a single line. After the datasets are loaded in
the first half of 'CakeMap.R', the following code creates the weight matrix:

```{r, echo=FALSE}
source("R/CakeMap.R")
```

```{r, eval=FALSE}
weights <- apply(cons, 1, function(x)
  ipfp(x, ind_catt, x0, maxit = 20))
```

As with the SimpleWorld example, the correlation between the
constraint table and the aggregated results of the spatial microsimulation
can be checked to ensure that the reweighting process has worked correctly.
This demonstrates that the process has worked with an *r* value above 0.99:

```{r}
cor(as.numeric(cons), as.numeric(ind_agg))
```

## Integerisation {#CakeINT}

As before, weights of the IPF procedure are fractional, so must be *integerised*
to create whole individuals. The code presented in chapter 4 requires little
modification to do this: it is your task to convert the weight matrix generated
by the above lines of code into a spatial microdataset called, as before,
`ints_df` (hint: the `int_trs` function in 'R/functions.R' file will help).
The spatial microdata generated in 'R/CakeMapInts.R' contain the same information
as the individual-level dataset, but with the addition of the 'zone' variable,
which specifies which zone each individual inhabits.

The spatial microdata is thus *multilevel* data, operating at one level on
the level of individuals and at another at the level of zones. To generate
summary statistics about the individuals in each zone, functions must be
run on the data, one zone (group) at a time. `aggregate` provides one
way of doing this. After converting the 'NSSEC' socio-economic class variable
into a suitable numeric variable, `aggregate` can be used to identify the
variability in social class in each zone, using the `by =` argument to
specify how the results are grouped depending on the zone each individual inhabits:

```{r, echo=TRUE, message=FALSE}
source("R/CakeMapInts.R")
```

```{r, eval=FALSE}
aggregate(ints_df$NSSEC, by = list(ints_df$zone), sd,
  na.rm = TRUE)
```

```
##    Group.1        x
## 1         1 1.970570
## 2         2 2.027638
## 3         3 2.019839
```

In the above code the third argument refers to the function to be applied to the
input data. The fourth argument is simply an argument of this function, in this
case instructing the standard deviation function (`sd`) to ignore all `NA` values.
An alternative way to perform this operation, which is faster and more
concise, is using `tapply`:

```{r, eval=FALSE}
tapply(ints_df$NSSEC, ints_df$zone, sd, na.rm = TRUE)
```

Note that operations on `ints_df` can take a few seconds to complete.
This is because the object is large, taking up much RAM on
the computer. This can be seen by asking `object.size(ints_df)` or
`nrow(ints_df)`. The latter shows we have created a spatial microdataset
of 1.6 million individuals! Try comparing
this result with the size of the original survey dataset 'ind'. Keeping an eye
on such parameters will ensure that the model does not generate datasets too
large to handle.

Next we move on to a vital
consideration in spatial microsimulation models such as CakeMap: validation.

## Model checking and validation {#svalidation}

To make an analogy with food safety standards, openness about mistakes is
conducive to high standards. Transparency in model verification is desirable for
similar reasons. The two main strategies are:

1. comparing the model results with knowledge of how it *should*
perform a-priori (model checking), and
2. comparison between the model results and empirical data (validation).

Pearson's coefficient of correlation ($r$) provides a fast and simple insight
into the fit between the simulated data and the constraints. In most cases
$r$ values greater than 0.9 should be sought in spatial microsimulation and
in many cases $r$ values exceeding 0.99 are possible, even after integerisation.
As a very basic test, let's observe the correlation between the constraint table
cells and the corresponding simulated cell values:

```{r}
cor(as.numeric(cons), as.numeric(ind_agg))
```

The total absolute error (TAE) is another commonly used measure of fit
which is the total sum of differences between the two datasets.
TAE is defined by the following formula and
R function, which can be used to measure the fit between any two
vectors or matrices of equal dimensions:^[Data frames will not
work in this function and must be converted to matrices with `as.matrix`.]

$$ TAE = \sum\limits_{ij}|U_{ij} - T_{ij}| $$

```{r}
tae <- function(observed, simulated){
  obs_vec <- as.numeric(observed)
  sim_vec <- as.numeric(simulated)
  sum(abs(obs_vec - sim_vec))
}
```

Standardised absolute error (SAE) is a related measure: $SAE = TAE/P$ where $P$ is the
total population of the study area (`tae(obs, sim)/sum(con1)` in R).  While
TAE is sensitive to the number of people within the model, SAE is not.

Pearson's *r*, TAE and SAE are just the simplest of a wide variety of
*goodness of fit* measures for comparing two datasets. The differences
between different measures are quite subtle and are discussed in detail in
Voas and Williamson (2001).
It is important to note that all such measures, that compare aggregate
count datasets, are *not* sufficient to ensure that the results of
spatial microsimulation are reliable: they are methods of *internal validation*
which simply show that the individual-level dataset has been
been reweighted to fit with a handful of constraint variables: i.e. that the
process has work under on its own terms.

Beyond typos or simple conceptual errors in model code, more fundamental
questions should be asked of spatial microsimulation models. The validity of the
assumptions on which they are built and the confidence one should have in the
results are important. For this we need external datasets.  Validation is
therefore a tricky topic, something not covered here but which is discussed in
Edwards et al. (2009). For more on this and  for (an albeit unreliable)
comparison between estimated cake consumption and external income estimates.

## Visualisations {#CakeVis}

Visualisation is an important part of communicating quantitative
data, especially so when the datasets are large and complex so not
conducive to description with tables or words.

Because we have generated spatial data, it is useful to create a map of the
results, to see how it varies from place to place.  The code used to do this
found in 'CakeMapPlot.R'. A vital function within this script is the
`inner_join` function, which depends on the **dplyr** package.


```{r, eval=FALSE}
wardsF <- inner_join(wardsF, wards@data, by = "id")
```

The above line of code by default selects all the data contained in the first
object (`wardsF`) and adds to it new variables from the second object based on
the linking variable.  Also in that script file you will encounter the function
`fortify`, the purpose of which is to convert the spatial data object into a
data frame.  The final map result of `CakeMapPlot.R' is illustrated below.

```{r, fig.cap="CakeMap results: estimated average cake consumption in Leeds", fig.width=5, fig.height=4, echo=FALSE}
library(png)
library(grid)
img <- readPNG("figures/CakeMap-lores.png")
grid.raster(img)
```

## Analysis and interpretation {#CakeAnalysis}

Once a spatial microdataset has been generated that we are happy with, we will
probably want to analyse it further.  This means exploring it --- its main
features, variability and links with other datasets. To illustrate this process
we will load an additional dataset and compare it with the estimates of cake
consumption per person  generated in the previous section at the ward level.  

The hypothesis we would like to test is that cake consumption is linked to
deprivation: More deprived people will eat unheathily and cake is a relatively
cheap 'comfort food'.  Assuming our simulated data is correct ---  a
questionable assumption but lets roll with it for now --- we can explore this at
the ward level thanks to a
dataset (http://www.neighbourhood.statistics.gov.uk)
on modelled income from neighbourhood statistics. The following code is taken
from the 'CakeMapPlot.R' script.

Because the income dataset was produced for old ward boundaries (they were
slightly modified for the 2011 census), we cannot merge with the spatial dataset
based on the new zone codes. Instead we rely on the name of the wards. The code
below provides a snapshot of these names and demonstrates how they can be joined
using `inner_join`.

```{r, eval=FALSE}
wards@data <- join(wards@data, imd)
summary(imd$NAME %in% wards$NAME)
##       Mode   FALSE    TRUE    NA's 
##    logical      55      71       0 
```


The above code first joins the two datasets together and then checks the result
by seeing how many matches names there are.  In practice the fit between old
names and new names is quite poor: only 71 out of 124. In a proper analysis we
would have to solve this problem (e.g. via the command `pmatch`, which stands
for partial match).  For the purposes of this exercise we will simply plot
income against simulated cake consumption to gain a feeling what it tells us
about the relationship between cake consumption and wealth.

![](figures/incomeCake.png)

**Scatterplot** illustrating the relationship between modelled average ward
income and simulated number of cakes eaten per person per week.

The question raised by this finding is: why?  Not why is cake consumption higher
in wealthy areas (this has not been established) but: why has the model resulted
in this correlation?  To explore this question we need to go back and look at
the individual level data. The most relevant constraint variable for income was
class.  When we look at the relationship between social class and cake
consumption in the Dental Health Survey, we find that there is indeed a link:
individuals in the highest three classes (1.1, 1.2, 2) have an average cake
intake of 3.9 cakes per week whereas the three lowest classes have an average
intake of 3.7. This is a relatively modest difference but, when averaging over
large areas, it helps explain the result.

As a bonus exercise, explore the class dependence of cake consumption in the
Dental Health Survey.

\clearpage
