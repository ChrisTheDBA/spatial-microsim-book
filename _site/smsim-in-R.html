<!DOCTYPE html>
<html>
  <head>
    <title>Spatial microsimulation in R &middot; Spatial Microsimulation with R.</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="www/bootstrap.min.css" rel="stylesheet">
    <link href="www/highlight.css" rel="stylesheet">

    <link href='http://fonts.googleapis.com/css?family=Inconsolata:400,700'
      rel='stylesheet' type='text/css'>
  </head>

  <body>

    <div class="container">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown">
              Table of contents<b class="caret"></b>
            </a>
            <ul class="dropdown-menu pull-right" role="menu">
              <li><a href="introduction.html">Introduction</a></li>
<li><a href="what-is-smsim.html">What is spatial microsimulation?</a></li>
<li><a href="SimpleWorld.html">SimpleWorld</a></li>
<li><a href="data-prep.html">Preparing input data</a></li>
<li><a href="smsim-in-R.html">Spatial microsimulation in R</a></li>
<li><a href="CakeMap.html">CakeMap</a></li>
<li><a href="smsim-for-abm.html">Spatial microsimulation for agent-based models</a></li>
<li><a href="additional.html">Additional tools and techniques</a></li>
<li><a href="appendix.html">Appendix</a></li>
<li><a href="glossary.html">Glossary</a></li>
<li><a href="references.html">References</a></li>

            </ul>
          </li>
        </ul>

        <h3 class="muted"><a href="/">Spatial Microsimulation with R</a> <small>by Robin Lovelace</small></h3>
        <hr>
      </div>

      <div class="row">
        <div class="col-sm-3" id="nav">
        <div class="well">
          Comming soon as a physical book</a>.
        </div>

        <h4>Contents</h4>
          <ul class="list-unstyled" id="toc"></ul>

          <hr>
          <p><a href="/contribute.html">How to contribute</a></p>

          <p><a class="btn btn-primary" href="https://github.com/RobinLovelace/spatial-microsim-book/edit/master/smsim-in-R.Rmd">Edit this page</a></p>
        </div>

        <div id="content" class="col-sm-8 pull-right">
          <h1 id="Smsim1">Spatial microsimulation in R</h1>
<p>In this chapter we progress from loading and preparing the input data to running a spatial microsimulation model. The SimpleWorld data, loaded in the previous chapter, is used. Being small and simple, the example enables understanding the process on a ‘human scale’ and allows experimentation without the worry of overloading your computer. However, the methods apply equally to larger and more complex projects. Therefore practicing the basic principles and methods of spatial microsimulation in R is the focus of this chapter. Time spent mastering these basics will make subsequent steps much easier.</p>
<p>How representative each individual is of each zone is determined by their <em>weight</em> for that zone. If we have <code>nrow(cons)</code> zones and <code>nrow(ind)</code> individuals (3 and 5, respectively, in SimpleWorld) we will create 15 weights. To start, we create an empty weight matrix, ready to be filled with numbers calculated through the IPF procedure:</p>
<pre class="sourceCode r"><code class="sourceCode r">weights &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="kw">nrow</span>(ind), <span class="dt">ncol =</span> <span class="kw">nrow</span>(cons))
<span class="kw">dim</span>(weights) <span class="co"># dimension of weight matrix: 5 rows by 3 columns</span>
<span class="co">#&gt; [1] 5 3</span></code></pre>
<h2 id="IpfinR">IPF in R</h2>
<p>The most established <em>deterministic</em> method to allocate individuals to zones is iterative proportional fitting (IPF). IPF involves calculating a series of weights that represent how representative each individual is of each zone. This is <em>reweighting</em>. The IPF algorithm can be written in R from scratch, as illustrated in Lovelace (2014), and as implemented in the smsim-course GitHub repository (<a href="https://github.com/Robinlovelace/spatial-microsim-book">https://github.com/Robinlovelace/spatial-microsim-book</a>). The code in this file, and the accompanying text, saves the weight matrix after every constraint for each iteration. While this makes the code relatively slow, it is useful for diagnosing issues with the reweighting process. However, to save computer and researcher time, we will skip directly to an alternative method that uses the <strong>ipfp</strong> package in this section.</p>
<p>Regardless of which implementation of used, IPF can be used to allocate the individual-level data loaded in the previous chapter to the three zones of SimpleWorld. IPF is mature, fast and has a long history. Interested readers are directed towards recent papers (e.g. Lovelace and Ballas, 2012; Pritchard and Miller, 2012) for more detail on the method and its underlying theory.</p>
<h2 id="ipfp">Reweighting with <strong>ipfp</strong></h2>
<p>IPF runs much faster and with less code using the <strong>ipfp</strong> package than in pure R. The <code>ipfp</code> function runs the IPF algorithm in the C language, taking aggregate constraints, individual level data and and an initial weight vector (<code>x0</code>) as inputs:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ipfp) <span class="co"># load ipfp library after install.packages(&quot;ipfp&quot;)</span>
cons &lt;-<span class="st"> </span><span class="kw">apply</span>(cons, <span class="dv">2</span>, as.numeric) <span class="co"># to 1d numeric data type</span>
<span class="kw">ipfp</span>(cons[<span class="dv">1</span>,], <span class="kw">t</span>(ind_cat), <span class="dt">x0 =</span> <span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(ind))) <span class="co"># run IPF</span>
<span class="co">#&gt; [1] 1.228 1.228 3.544 1.544 4.456</span></code></pre>
<p>It is impressive that the entire IPF process, which takes dozens of lines of code in pure R can been condensed into two lines: one to convert the input constraint dataset to <code>numeric</code><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> and one to perform the IPF operation itself. Note also that although we did not specify how many iterations to run, the above command ran the default of <code>maxit = 1000</code> iterations, despite convergence happening after 10 iterations. This can be seen by specifying the <code>maxit</code> and <code>verbose</code> arguments (the latter of which can be referred to lazily as <code>v</code>) in <code>ipfp</code>, as illustrated below (only the first line of R output is shown):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ipfp</span>(cons[<span class="dv">1</span>,], <span class="kw">t</span>(ind_cat), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(ind)), <span class="dt">maxit =</span> <span class="dv">20</span>, <span class="dt">v =</span> T)</code></pre>
<pre><code>## iteration 0:   0.141421
## iteration 1:    0.00367328
## iteration 2:  9.54727e-05
## ...
## iteration 9:  4.96507e-16
## iteration 10:    4.96507e-16</code></pre>
<p>Notice also that a <em>transposed</em> (via the <code>t()</code> function) version of the individual-level data (<code>ind_cat</code>) is used in <code>ipfp</code> to represent the individual-level data, instead of the <code>ind_agg</code> object used in the pure R version. To prevent having to transpose <code>ind_cat</code> every time <code>ipfp</code> is called, save the transposed version:</p>
<pre class="sourceCode r"><code class="sourceCode r">ind_catt &lt;-<span class="st"> </span><span class="kw">t</span>(ind_cat) <span class="co"># save transposed version of ind_cat</span></code></pre>
<p>Another object that can be saved prior to running <code>ipfp</code> on all zones (the rows of <code>cons</code>) is <code>rep(1, nrow(ind))</code>, simply a series of ones - one for each individual. We will call this object <code>x0</code> as its argument name representing the starting point of the weight estimates in <code>ipfp</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">x0 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(ind)) <span class="co"># save the initial vector</span></code></pre>
<p>To extend this process to all three zones we can wrap the line beginning <code>ipfp(...)</code> inside a <code>for</code> loop, saving the results each time into the weight variable we created earlier:</p>
<pre class="sourceCode r"><code class="sourceCode r">weights_maxit_2 &lt;-<span class="st"> </span>weights <span class="co"># create a copy of the weights object</span>
for(i in <span class="dv">1</span>:<span class="kw">ncol</span>(weights)){
  weights_maxit_2[,i] &lt;-<span class="st"> </span><span class="kw">ipfp</span>(cons[i,], ind_catt, x0, <span class="dt">maxit =</span> <span class="dv">2</span>)
}</code></pre>
<p>The above code uses <code>i</code> to iterate through the constraints, one row (zone) at a time, saving the output vector of weights for the individuals into columns of the weight matrix. To make this process even more concise (albeit less clear to R beginners), we can use R’s internal <code>for</code> loop, <code>apply</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">weights &lt;-<span class="st"> </span><span class="kw">apply</span>(cons, <span class="dt">MARGIN =</span> <span class="dv">1</span>, <span class="dt">FUN =</span> 
    function(x) <span class="kw">ipfp</span>(x, ind_catt, x0, <span class="dt">maxit =</span> <span class="dv">20</span>))</code></pre>
<p>In the above code R iterates through each row (hence the second argument <code>MARGIN</code> being <code>1</code>, <code>MARGIN = 2</code> would signify column-wise iteration). Thus <code>ipfp</code> is applied to each zone in turn, as with the <code>for</code> loop implementation. The speed savings of writing the function with different configurations are benchmarked in ‘parallel-ipfp.R’ in the ‘R’ folder of the book project directory. This shows that reducing the maximum iterations of <code>ipfp</code> from the default 1000 to 20 has the greatest performance benefit.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> To make the code run faster on large datasets, a parallel version of <code>apply</code> called <code>parApply</code> can be used. This is also tested in ‘parallel-ipfp.R’.</p>
<p>It is important to check that the weights obtained from IPF make sense. To do this, we multiply the weights of each individual by rows of the <code>ind_cat</code> matrix, for each zone. Again, this can be done using a for loop, but the apply method is more concise:</p>
<pre class="sourceCode r"><code class="sourceCode r">ind_agg &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(weights, <span class="dv">2</span>, function(x) <span class="kw">colSums</span>(x *<span class="st"> </span>ind_cat)))
<span class="kw">colnames</span>(ind_agg) &lt;-<span class="st"> </span><span class="kw">colnames</span>(cons) <span class="co"># make the column names equal</span></code></pre>
<p>As a preliminary test of fit, it makes sense to check a sample of the aggregated weighted data (<code>ind_agg</code>) against the same sample of the constraints. Let’s look at the results (one would use a subset of the results, e.g. `ind_agg[1:3, 1:5] for the first five values of the first 3 zones for larger constraint tables found in the real world):</p>
<pre class="sourceCode r"><code class="sourceCode r">ind_agg
<span class="co">#&gt;      a0_49 a50+ m f</span>
<span class="co">#&gt; [1,]     8    4 6 6</span>
<span class="co">#&gt; [2,]     2    8 4 6</span>
<span class="co">#&gt; [3,]     7    4 3 8</span>
cons
<span class="co">#&gt;      a0_49 a50+ m f</span>
<span class="co">#&gt; [1,]     8    4 6 6</span>
<span class="co">#&gt; [2,]     2    8 4 6</span>
<span class="co">#&gt; [3,]     7    4 3 8</span></code></pre>
<p>This is a good result: the constraints perfectly match the results generated using ipf, at least for the sample. To check that this is due to the <code>ipfp</code> algorithm improving the weights with each iteration, let us analyse the aggregate results generated from the alternative set of weights, generated with only 3 iterations of IPF:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Re-allocate the values of ind_agg (not column names - note &#39;[]&#39;)</span>
ind_agg[] &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(weights_maxit_2, <span class="dt">MARGIN =</span> <span class="dv">2</span>, 
  <span class="dt">FUN =</span> function(x) <span class="kw">colSums</span>(x *<span class="st"> </span>ind_cat)))
ind_agg[<span class="dv">1</span>:<span class="dv">2</span>, <span class="dv">1</span>:<span class="dv">3</span>]
<span class="co">#&gt;      a0_49  a50+ m</span>
<span class="co">#&gt; [1,] 8.003 3.997 6</span>
<span class="co">#&gt; [2,] 2.004 7.996 4</span></code></pre>
<p>Clearly the final weights after 3 iterations of IPF represent the constraint variables well, but do not match perfectly except in the second constraint. This shows the importance of considering number of iterations in the reweighting stage — too many iterations can be wasteful, too few may result in poor results. To reiterate, 20 iterations of IPF should be sufficient in most cases for the results to converge towards their final level of fit. More sophisticated ways of evaluating model fit are presented in Section .</p>
<h2>Alternative reweighting algorithms</h2>
<p>As described in the Introduction, IPF is just one strategy for obtaining a spatial microdataset. However, researchers (myself included) have tended to select one method that they are comfortable and stick with that for their models. This is understandable because setting-up the method is usually time consuming: most researchers rightly focus on applying the methods to the real world rather than fretting about the details. On the other hand, if alternative methods work better for a particular application, resistance to change can result in poor model fit. In the case of very large datasets, spatial microsimulation may not be possible unless certain methods, optimised to deal with large datasets, are used. Above all, there is no consensus about which methods are ‘best’ for different applications, so it is worth experimenting to identify which method is most suitable for each application.</p>
<h3>Spatial microsimulation as an objective function</h3>
<p>In general terms, an <em>objective function</em> is a formula that processes a series of input variables (<span class="math"><em>x</em></span>) using a number of parameters (<span class="math"><em>p</em><em>a</em><em>r</em></span>) to generate a single numeric output:</p>
<p><br /><span class="math">minimise <em>f</em><sub>0</sub>(<em>x</em>)</span><br /></p>
<p><br /><span class="math">subject to <em>f</em><sub><em>i</em></sub>(<em>x</em>) ≤ <em>p</em><em>a</em><em>r</em><sub><em>i</em></sub>,  <em>i</em> = 1, ..., <em>m</em></span><br /></p>
<p>The goal of objective function <span class="math"><em>f</em><sub>0</sub></span> such as that illustrated above is to enable the selection of the parameters <span class="math"><em>p</em><em>a</em><em>r</em><sub><em>i</em></sub></span> to <span class="math"><em>p</em><em>a</em><em>r</em><sub><em>m</em></sub></span> which minimize the result. The values that parameters may take are constrained by <em>constraint functions</em> (<span class="math"><em>f</em><sub><em>i</em></sub></span>) or fixed values (Boyd and Vandenberghe, 2009). The case of spatial microsimulation has relatively simple constraints: they are all positive or zero. Seeing spatial microsimulation as an objective function allows solutions to be found using established techniques of <em>constrained optimisation</em>. The main advantage of this re-framing is that it allows any optimisation algorithm to perform the reweighting. Key to this is interpreting individual weights as parameters (the vector <span class="math"><em>p</em><em>a</em><em>r</em></span>, of length <span class="math"><em>m</em></span> above) that are iteratively modified to optimise the fit between individual and aggregate-level data. The measure of fit is we use in this context is Total Absolute Error (TAE) and the objective function is as follows:</p>
<p><br /><span class="math">minimise <em>f</em>(<em>p</em><em>a</em><em>r</em>) = <em>T</em><em>A</em><em>E</em>(<em>s</em><em>i</em><em>m</em>, <em>c</em><em>o</em><em>n</em>)</span><br /></p>
<p><br /><span class="math">where <em>s</em><em>i</em><em>m</em> = <em>c</em><em>o</em><em>l</em><em>S</em><em>u</em><em>m</em><em>s</em>(<em>i</em><em>n</em><em>d</em>_<em>c</em><em>a</em><em>t</em> * <em>p</em><em>a</em><em>r</em>)</span><br /></p>
<p><br /><span class="math">subject to <em>p</em><em>a</em><em>r</em> ≥ 0</span><br /></p>
<p>Note that in the above, <span class="math"><em>p</em><em>a</em><em>r</em></span> is equivalent to the <code>weights</code> object we have created in previous sections to represent how representative each individual is of each zone. The main issue with this definition of reweighting is therefore the large number of free parameters: equal to the number of individual-level dataset. Clearly this can be very very large. To overcome this issue, we must ‘compress’ the individual level dataset to its essence, to contain only unique individuals with respect to the constraint variables (<em>constraint-unique</em> individuals).</p>
<p>The challenge is to convert the binary ‘model matrix’ form of the individual-level data (<code>ind_cat</code> in the previous examples) into a new matrix (<code>ind_num</code>) that has fewer rows of data. Information about the frequency of each constraint-unique individual is kept by increasing the value of the ‘1’ entries for each column for the replicated individuals by the number of other individuals who share the same combination of attributes. This may sound quite simple, so let’s use the example of SimpleWorld to illustrate the point.</p>
<h3>Reweighting with optim and GenSA</h3>
<p>The base R function <code>optim</code> provides a general purpose optimisation framework for numerically solving objective functions. Based on the objective function for spatial microsimulation described above, we can use any general optimisation algorithm for reweighting the individual-level dataset. But which to use?</p>
<p>Different reweighting strategies are suitable in different contexts and there is no clear winner for every occasion. However, testing a range of strategy makes it clear that certain algorithms are more efficient than others for spatial microsimulation. Figure x demonstrates this variability by plotting total absolute error as a function of number of iterations for various optimisation algorithms available from the base function <code>optim</code> and the <strong>GenSA</strong> package.</p>
<p><img src="figures/optim-its.png" /></p>
<p>Figure x shows the advantage of the IPF algorithm we have been using, which converges rapidly to zero error after only a few iterations. On the other end of the spectrum is R’s default optimisation algorithm, the Nelder-Mead method. Although the graph shows no improvement from one iteration to the next, it should be stated that the algorithm is just ‘warming up’ at this stage and than each iteration is very fast, as we shall see. After a few <em>hundred</em> iterations (which happen in the same time that other algorithms take for a single iteration), the Nelder-Mead method is effective, converging to close to zero in 400 iterations, taking far more iterations to converge to a value approximating zero than IPF. Next best in terms of iterations is <code>GenSA</code>, the Generalized Simulated Annealing Function from the <strong>GenSA</strong> package. GenSA attained a near-perfect fit after only two full iterations.</p>
<p>The remaining algorithms shown are, like Nelder-Mead, available from within R’s default optimisation function <code>optim</code>. The implementations with <code>method =</code> set to <code>&quot;BFGS&quot;</code> (short for the Broyden–Fletcher–Goldfarb–Shanno algorithm), <code>&quot;CG&quot;</code> (‘conjugate gradients’) performed roughly the same, steadily approaching but not reaching zero error after five iterations. Finally, the <code>SANN</code> method, also available in <code>optim</code>, performed most erratically of the methods tested. This is another implementation of simulated annealing which demonstrates that optimisiation functions that depend on random numbers do not always lead to improved fit from one iteration to the next.</p>
<p>The code used to test these alternative methods for reweighting are provided in the script ‘R/optim-tests-SimpleWorld.R’. The results should be reproducible on any computer, provided the book’s supplementary materials have been downloaded. There are many other optimisation algorithms available in R through a wide range of packages; new and improved functions are available all the time so enthusiastic readers are encouraged to experiment with this script files: it is perfectly feasible that an algorithm exists which outperforms all of those tested for this book. Also, it should be noted that the algorithms were tested on the extremely simple and rather contrived example dataset of SimpleWorld. Some algorithms perform better with larger datasets than others and the results can be highly sensitive to changes to the initial conditions such as the problem of ‘empty cells’.</p>
<p>Therefore these results, as with any modelling exercise, should be interpreted with a healthy dose of skepticism: just because an algorithm converges after few ‘iterations’ this does not mean it is inherently any faster or more useful than another. The results are context specific, so it is recommended that the tested framework in ‘R/optim-tests-SimpleWorld.R’ is used as a basis for further tests on algorithm performance on the datasets you are using. IPF has performed well in the situations I have tested it in (especially via the <code>ipfp</code> function, which performs disproportionately faster than the pure R implementation on large datasets) but this does not mean that it is always the best approach.</p>
<p>To overcome the caveat that the meaning of an ‘iteration’ changes dramatically from one algorithm to the next, further tests measured the time taken for each reweighting algorithm to run. The results are very different when iterations are seen as the independent variable (Figure xx). This figure Demonstrates that Nelder-Mead is fast at reaching a good approximation of the constraint data, despite taking many iterations. <code>GenSA</code>, on the other hand, is shown to be much slower than the others, despite only requiring 2 iterations to arrive at a good level of fit. The <code>SANN</code> method is not shown at all due to high TAE values for all times shown.</p>
<p><img src="figures/optim-time.png" /></p>
<h3>Combinatorial optimisation</h3>
<p>Combinatorial optimisation is an alternative to IPF for allocating individuals to zones. This strategy is <em>probabilistic</em> and results in integer weights, as opposed to the fractional weights generated by IPF. Combinatorial optimisation may be more appropriate for applications where input individual microdatasets are very large: the speed benefits of using the deterministic IPF algorithm shrink as the size of the survey dataset increases. An alternative to combinatorial optimisation, which builds on the fractional weights generated through IPF (or any other method of reweighting), is <em>intersation</em>. Integerisation largely reduces the need for combinatorial optimisation, as we shall see in the next section ().</p>
<p>There are two approaches for reweighting using combinatorial optimisation in R: shuffling individuals in and out of each area and combinatorial optimisation the <em>domain</em> of the solution space set to allow inter-only results…</p>
<p>The second approach to combinatorial optimisation in R depends on methods that allow only integer solutions to the general constrained optimisation formulae demonstrated in the previous section. <em>Integer programming</em> is the branch of computer science dedicated to this area, and it is associated with its own algorithms and approaches, some of which have been implemented in R (Zubizarreta, 2012).</p>
<p>To illustrate how the approach works in general terms, we can use the <code>data.type.int</code> argument of the <code>genoud</code> function in the <strong>rgenoud</strong> package. This ensures only integer results for a genetic algorithm to select parameters are selected:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2014</span>)
<span class="kw">genoud</span>(<span class="dt">nvars =</span> <span class="kw">nrow</span>(indu), <span class="dt">fn =</span> fun, <span class="dt">ind_num =</span> indu, <span class="dt">con =</span> cons[<span class="dv">1</span>,],
  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxit =</span> <span class="dv">1000</span>), <span class="dt">data.type.int =</span> <span class="ot">TRUE</span>,
  
  <span class="co"># Set min and maximum values of constraints with &#39;Domains&#39;</span>
  <span class="dt">D =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(indu)),<span class="kw">rep</span>(<span class="dv">100</span>, <span class="kw">nrow</span>(indu))), <span class="dt">ncol =</span> <span class="dv">2</span>))</code></pre>
<p>This rather long command, implemented in ‘optim-tests-SimpleWorld.R’, results in weights for the unique individuals 1 to 4 of 1, 4, 2 and 4 respectively. This means that a total population of 11 individuals is simulated for the zone, composed of 1 instance of individual 1 and ‘clones’ of the others. Given that the command takes 5 seconds to generate weights for only 4 unique individuals, and the fact that there are in fact 12 not 11 individuals in zone 1 in SimpleWorld, this implementation of <code>genoud</code> is clearly not production ready. <code>genoud</code> is used here to provide a practical demonstration of the possibilities of combinatorial optimisation in R, an area that has great potential to advance in the future.</p>
<p>For combinatorial optimisation algorithms designed for spatial microsimulation we must, for now, look for programs outside the R ‘ecosystem’. Harland (2013) provides a practical tutorial introducing the subject based on the Flexible Modelling Framework (FMF) Java program. It is possible to generate integer spatial microdata using IPF, however, using a technique we refer to imaginatively as integerisation. This is the topic of the next section.</p>
<h2 id="sintegerisation">Integerisation</h2>
<p>Integerisation is the process by which a vector of real numbers is converted into a vector of integers corresponding to the individuals present in synthetic spatial microdata. The length of the new vector must equal the population of the zone in question and individuals with high weights must be sampled proportionally more frequently than those with low weights for the operation to be effective. The following example illustrates how the process, when seen as a function called <span class="math"><em>i</em><em>n</em><em>t</em></span> would work on a vector of 3 weights:</p>
<p><br /><span class="math"><em>w</em><sub>1</sub> = (0.333, 0.667, 3)</span><br /></p>
<p><br /><span class="math"><em>i</em><em>n</em><em>t</em>(<em>w</em><sub>1</sub>) = (2, 3, 3, 3)</span><br /></p>
<p>This result was obtained by calculating the sum of the weights (4, which represents the total population of the zone) and sampling from these until the total population is reached. In this case individual 2 is selected once as they have a weight approaching 1, individual 3 was replicated (<em>cloned</em>) three times and individual 1 does not appear in the integerised dataset at all, as it has a low weight. In this case the outcome is straightforward because the numbers are small and simple. But what about in less clear-cut cases, such as <span class="math"><em>w</em><sub>2</sub> = (1.333, 1.333, 1.333)</span>? What is needed is an algorithm to undertake this process of <em>integerisation</em> in a systematic way to maximise the fit between the synthetic and constraint data for each zone.</p>
<p>In fact there are a number of integerisation strategies available. Lovelace and Ballas (2012) tested 5 of these and found that probabilistic integerisation methods consistently outperformed deterministic rivals. The details of these algorithms are described in the aforementioned paper and code is provided in the Supplementary Information. For the purposes of this course we will create a function to undertake the simplest of these, <em>proportional probabilities</em>:</p>
<pre class="sourceCode r"><code class="sourceCode r">int_pp &lt;-<span class="st"> </span>function(x){
  <span class="kw">sample</span>(<span class="kw">length</span>(x), <span class="dt">size =</span> <span class="kw">round</span>(<span class="kw">sum</span>(x)), <span class="dt">prob =</span> x, <span class="dt">replace =</span> T)
}</code></pre>
<p>To test this function let’s try it on the vectors of length 3 described in code:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">0</span>)
<span class="kw">int_pp</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">0.333</span>, <span class="fl">0.667</span>, <span class="dv">3</span>))
<span class="co">#&gt; [1] 2 3 3 3</span>
<span class="kw">int_pp</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">1.333</span>, <span class="fl">1.333</span>, <span class="fl">1.333</span>))
<span class="co">#&gt; [1] 1 2 1 1</span></code></pre>
<p>The first result was the same as that obtained through intuition; the second result represented individual 1 being clones three times, plus one instance of individual 2. This is not intuitive: one would expect at least one of each individual given that they all have the same weight.</p>
<p>It is important to emphasise that the results will change each time the code is run, because <code>sample</code> is a probabilistic (its output depends on a random number generator): changing the value inside the brackets proceeding <code>set.seed</code> results in many other combinations of individuals being selected — test this out in your code. This happens because the method relies on <em>pseudo random numbers</em> to select values probabilistically and <code>set.seed</code> specifies where the random number sequence should begin, ensuring repeat-ability. An issue with the <em>proportional probabilities</em> (PP) strategy is that completely unrepresentative combinations of individuals have a non-zero probability of being sampled: the method will output <span class="math">(1, 1, 1, 1)</span> once in every 21 thousand runs for <span class="math"><em>w</em><sub>1</sub></span> and once every <span class="math">81</span> runs for <span class="math"><em>w</em><sub>2</sub></span>, the same probability as for all other 81 (<span class="math">3<sup>4</sup></span>) permutations.</p>
<p>To overcome this issue Lovelace and Ballas (2012) developed a method which ensures that any individual with a weight above 1 would be sampled at least once, making the result <span class="math">(1, 1, 1, 1)</span> impossible in both cases. This method is <em>truncate, replicate, sample</em> (TRS) integerisation:</p>
<pre class="sourceCode r"><code class="sourceCode r">int_trs &lt;-<span class="st"> </span>function(x){
  truncated &lt;-<span class="st"> </span><span class="kw">which</span>(x &gt;=<span class="st"> </span><span class="dv">1</span>)
  replicated &lt;-<span class="st"> </span><span class="kw">rep</span>(truncated, <span class="kw">floor</span>(x[truncated]))
  r &lt;-<span class="st"> </span>x -<span class="st"> </span><span class="kw">floor</span>(x)
  def &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">sum</span>(x)) -<span class="st"> </span><span class="kw">length</span>(replicated) <span class="co"># deficit population</span>
  if(def ==<span class="st"> </span><span class="dv">0</span>){
    out &lt;-<span class="st"> </span>replicated
  } else {
    out &lt;-<span class="st"> </span><span class="kw">c</span>(replicated,
      <span class="kw">sample</span>(<span class="kw">length</span>(x), <span class="dt">size =</span> def, <span class="dt">prob =</span> r, <span class="dt">replace =</span> <span class="ot">FALSE</span>))
  }
  out
}</code></pre>
<p>To see how this new integerisation method and associated R function performed, we will run it on the same input vectors:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">int_trs</span>(<span class="kw">c</span>(<span class="fl">0.333</span>, <span class="fl">0.667</span>, <span class="dv">3</span>))
<span class="co">#&gt; [1] 3 3 3 2</span>
<span class="kw">int_trs</span>(<span class="kw">c</span>(<span class="fl">1.333</span>, <span class="fl">1.333</span>, <span class="fl">1.333</span>))
<span class="co">#&gt; [1] 1 2 3 3</span></code></pre>
<p>The range of possible outcomes is smaller using the TRS technique; the fit between the resulting microdata and the aggregate constraints will tend to be higher. Thus we use the TRS methodology, implemented through the function <code>int_trs</code>, for integerising the weights generated by IPF throughout the majority of this book.</p>
<p>Let’s use TRS to generate spatial microdata for SimpleWorld. Remember, we already have generated the weight matrix <code>weights</code>. The only challenge is to save the vector of sampled individual id numbers, alongside the zone number, into a single object from which the attributes of these individuals can be recovered. Two strategies for doing this are presented in the code below:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Method 1: using a for loop</span>
ints_df &lt;-<span class="st"> </span><span class="ot">NULL</span>
for(i in <span class="dv">1</span>:<span class="kw">nrow</span>(cons)){
  ints &lt;-<span class="st"> </span><span class="kw">int_trs</span>(weights[, i])
  ints_df &lt;-<span class="st"> </span><span class="kw">rbind</span>(ints_df, <span class="kw">data.frame</span>(<span class="dt">id =</span> ints, <span class="dt">zone =</span> i))
}

<span class="co"># Method 2: using apply</span>
ints &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">apply</span>(weights, <span class="dv">2</span>, int_trs)) <span class="co"># integerised result</span>
ints_df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> ints,
  <span class="dt">zone =</span> <span class="kw">rep</span>(<span class="dv">1</span>:<span class="kw">nrow</span>(cons), <span class="kw">colSums</span>(weights)))</code></pre>
<p>Both methods yield the same result for <code>ints_df</code>. The only differences being that Method 1 is perhaps more explicit and easier to understand whilst Method 2 is more concise.</p>
<p>The final remaining step is to re-allocate the attribute data from the individual-level data back into <code>ints_df</code>. To do this we use the <code>inner_join</code> function from the recently released <strong>dplyr</strong> package.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Assuming <strong>dplyr</strong> is loaded — with <code>library(plyr)</code> — one can read more about join by entering <code>?inner_join</code> in R.</p>
<pre class="sourceCode r"><code class="sourceCode r">ind_full &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/SimpleWorld/ind-full.csv&quot;</span>)
<span class="kw">library</span>(dplyr) <span class="co"># use install.packages(dplyr) if not installed</span>
ints_df &lt;-<span class="st"> </span><span class="kw">inner_join</span>(ints_df, ind_full)</code></pre>
<p><code>ints_df</code> represents the final spatial microdataset, representing the entirety of SimpleWorld’s population of 33 (this can be confirmed with <code>nrow(ints_df)</code>). To select individuals from one zone only is simple using R’s subsetting notation. To select all individuals generated for zone 2, for example, the following code is used. Note that this is the same as the output generated in Table 5 at the end of the SimpleWorld chapter — we have successfully modelled the inhabitants of a fictional planet, including income!</p>
<pre class="sourceCode r"><code class="sourceCode r">ints_df[ints_df$zone ==<span class="st"> </span><span class="dv">2</span>, ]
<span class="co">#&gt;    id zone age sex income</span>
<span class="co">#&gt; 13  1    2  59   m   2868</span>
<span class="co">#&gt; 14  2    2  54   m   2474</span>
<span class="co">#&gt; 15  4    2  73   f   3152</span>
<span class="co">#&gt; 16  4    2  73   f   3152</span>
<span class="co">#&gt; 17  4    2  73   f   3152</span>
<span class="co">#&gt; 18  4    2  73   f   3152</span>
<span class="co">#&gt; 19  5    2  49   f   2473</span>
<span class="co">#&gt; 20  4    2  73   f   3152</span>
<span class="co">#&gt; 21  5    2  49   f   2473</span>
<span class="co">#&gt; 22  2    2  54   m   2474</span></code></pre>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The integer data type fails because C requires <code>numeric</code> data to be converted into its <em>floating point</em> data class.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>These tests also show that any speed gains from using <code>apply</code> instead of <code>for</code> are negligible, so whether to use <code>for</code> or <code>apply</code> can be decided by personal preference.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The functions <code>merge</code> from the R’s base package and <code>join</code> from the <strong>plyr</strong> provide other ways of undertaking this step. <code>inner_join</code> is used in place of <code>merge</code> because <code>merge</code> does not maintain row order. <code>join</code> generates the same result, but is slower, hence the use of <code>inner_join</code> from the recently released and powerful <strong>dplyr</strong> package.<a href="#fnref3">↩</a></p></li>
</ol>
</div>

        </div>
      </div>

      <div class="footer">
        <hr>
        <p>&copy; Site design: Hadley Wickham. Powered by <a href="http://jekyllrb.com/">jekyll</a>,
          <a href="http://yihui.name/knitr/">knitr</a>, and
          <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>. Source
          available on <a href="https://github.com/robinlovelace/spatial-microsim-book/">github</a>.
        </p>
      </div>

    </div> <!-- /container -->

  <script src="//code.jquery.com/jquery.js"></script>
  <script src="www/bootstrap.min.js"></script>
  <script src="www/toc.js"></script>
  </body>
</html>
