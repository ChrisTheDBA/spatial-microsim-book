---
title: "Population synthesis with R"
layout: default
---

```{r, echo=FALSE}
# packages to allow 'knitting' the chapter to html
library(png)
library(grid)
```

# Population synthesis with R {#smsimr}

```{r, echo=FALSE}
# TODO:
# Ensure text is compatible with new use of chapter title as 'population synthesis'
# Discuss population synthesis without an input microdataset, with reference to Bathelemy and Toint (2013)
# Consider splitting this section in 2: reweighting vs CO approaches

# Completed:
# Change integerisation section so it's part of IPF (RL)
# Add IPF theory from Cambridge handout.pdf (mostly done)
# update figure with expansion
```

In this chapter we progress from loading and preparing the input data to
running a spatial microsimulation model. We will begin by describing the
IPF procedure as it is the simplest and most widely used method to allocate
individuals to zones and then progress to discuss alternative
*reweighting algorithms* that do the same job in a different way.

The SimpleWorld data, loaded in the previous chapter,
is used. Being small and simple, the example facilitates understanding
the process on a 'human scale' and allows experimentation
without the worry of overloading your computer.
However, the methods apply equally to larger and more complex projects.
Thus practicing the basic principles and methods of spatial microsimulation
in R is the focus of this chapter.
Time spent mastering these basics will make subsequent steps
much easier.

```{r, echo=FALSE}
# How representative each individual is of each zone is determined by their
# *weight* for that zone. If we have `nrow(cons)` zones and `nrow(ind)`
# individuals (3 and 5, respectively, in SimpleWorld) we will create
# 15 weights. Real world datasets (e.g. that presented in chapter xxx)
# could contain 10,000 individuals
# to be allocated to 500 zones, resulting in an unwieldy 5 million element
# weight matrix but we'll stick with the SimpleWorld dataset here for simplicity.
load("cache-data-prep.RData")
```

How representative each individual is of each zone is represented by their
*weight* for that zone. Each weight links and individual to a zone.
The number
of weights is therefore equal to number of zones multiplied
by the number of individuals
in the microdata, that is the number of rows in individual-level and constraint tables
respectively.
In terms of the SimpleWorld data loaded in the previous chapter we have, in
R syntax, `nrow(cons)` zones and `nrow(ind)` individuals.
(Typing those commands with the data loaded should confirm that there are
3 zones and 5 individuals in the input data for the SimpleWorld example).
This means that `nrow(cons) * nrow(ind)` weights will be estimated
(that is $3 * 5 = 15$ in SimpleWorld). The weights must begin with an initial value. We will create a matrix of 
ones so every
individual is seen as equally representative of every
zone:^[Any
initial weight values could be used here: initial weights do not affect the final weights after many iterations. The value of one is
used as a sensible
convention.]

```{r}
# Create the weight matrix of ones
weights <- matrix(data = 1, nrow = nrow(ind), ncol = nrow(cons))
dim(weights) # dimension of weight matrix: 5 rows by 3 columns
```

The weigth matrix links individual-level data to aggregate-level data.
A weight matrix value of 0 in cell `[i,j]`, for example, suggests that
nobody with the characteristics of individual `i` is present in
zone `j`.
During the IPF procedure these weights are iteratively updated until
they *converge* towards a single result: the final weights which create
a representative population for each zone.^[Fienberg (1970) provides
a geometrical proof that IPF converges to a single result, in the absence
of *empty cells*.]

## Weighting algorithms {#weighting}

A wide range of methods can be used to allocate individuals to zones in spatial microsimulation. As with the majority of procedures for statistical 
analysis, there are *deterministic* and *stochastic* methods.
The results of *deterministic* methods, such as IPF, never vary: no random
or probabilistic numbers are used so the resulting weights will be the same
every time. *Stochastic* methods such as *simulated annealing*, on the other hand, use random numbers.

In the literature, the divide between stochastic and deterministic approaches
is usually mapped onto a wider distinction between *reweighting*
and *combinatorial optimisation* methods. Reweighting methods generally calculate
non-integer weights for every individual-zone combination.
Combinatorial optimisation
methods generally work by randomly allocating individuals from the microdata
survey into each zone, one-by-one, and re-calculating the goodness-of-fit after each change.
If the fit between observed and simulated results improves after an individual
has been 'imported' into the zone in question, the individual will stay.
If the fit deteriorates, the individual will be removed from the zone and
the impact of switching a different individual into the zone is tested.

This distinction between reweighting of fractional weights
and combinatorial optimisation algorithms is important: combinatorial
optimisation methods result in whole individuals being allocated to each zones
whereas reweighting strategies result in fractions of individuals being allocated
to each zone. The latter method means that individual $i$ could have a weight of
0.223 (or any other positive real number) for zone $j$. Of course, this sounds
irrational: a quarter of a person cannot possibly exist: either she is in the zone
or she is not!

However, the distinction between combinatorial optimisation and reweighting
approaches to creating spatial microdata is not as clear cut as it may at first
seem. As illustrated figure 5.1, fractional weights generated by reweighting
algorithms such as IPF can be converted into integer weights (via *integerisation*).
Through the process of *expansion*, the integer weight matrix
produced by integerisation can be converted into the final spatial microdata
output stored in 'long' format represented in the right-hand box of figure 5.1.
Thus the combined processes of integerisation
and expansion allow weight matrices to be translated into the same output
format that combinatorial optimisation algorithms produce directly.
In other words fractional weighting is interchangeable with combinatorial
optimisation approaches to population synthesis.

The reverse process is also possible:
synthetic spatial microdata generated by combinatorial
optimisation algorithms can be converted back in to a more compact weight
matrix in a step that we call *compression*. This integer weight has the same
dimensions as the integer matrix generated through integerisation described above.

```{r, echo=FALSE}
# is the function tidyr:@expand the same as expand here? will test.
```

Integerisation, expansion and compression procedures allow fractional
weighting and combinatorial optimisation approaches to population synthesis
to be seen as essentially the same thing.
This equivalence between different methods of population synthesis is
the reason we have labelled this section
*weighting algorithms*: combinatorial optimisation approaches to population synthesis can
be seen as a special case of fractional weighting and vice versa.
Thus all deterministic and stochastic
(or weighting and combinatorial optimisation) approaches to the generation of
spatial microdata can be seen as different methods, algorithms, for allocating
weights to individuals. Individuals representative of a zone will be given a
high weight (which is equivalent to being replicated many times in
combinatorial optimisation). Individuals who are rare in a zone will
be given a low weight (or not appear at all, equivalent to a weight of zero).
Later in this chapter we demonstrate functions to translate between the
'weight matrix' and 'long spatial microdata' formats generated by each approach.

```{r, fig.cap="Schematic of different approaches for the creation of spatial microdata encapsulating stochastic combinatorial optimisation and deterministic reweighting algorithms such as IPF. Note that integerisation and 'compression' steps make the results of the two approaches interchangeable, hence our use of the term 'reweighting algorithm' to cover all methods for generating spatial microdata.", echo=FALSE}
img <- readPNG("figures/co-vs-ipf-schema.png")
grid.raster(img)
```

The concept of weights is critical to understanding how
population synthesis generates spatial microdata. 
To illustrate the point imagine a parallel SimpleWorld, in which we have
no information about the characteristics its inhabitants, only the
total population of each zone. In this case we could only assume that
the distribution of characteristics found in the sample is representative of the distribution of the whole population. Under this scenario, individuals
would be chosen at random from the sample and allocated to zones at random
and the distribution of characteristics of individuals in each zone would be
asymptotically the same as (tending towards) the microdata.

In R, this case can
be achieved using the `sample()` command. We could use this method to
randomly allocate the 5 individuals of the microdata to zone 1 (which has
a population of 12) with the following code:

```{r}
set.seed(1) # set the seed for reproducibility
sel <- sample(x = 5, size = 12, replace = T) # create selection
ind_z1 <- ind_orig[sel, ]
head(ind_z1, 3)
```

Note the use of `set.seed()` in the above code to ensure the results are reproducible. It is important to emphasise that without 'setting the seed' (determining the starting point of the random number sequence) the results will change each time the code is
run. This is because `sample()` (along with other stochastic functions such as `runif()` and `rnorm()`) is probabilistic and its output therefore depends on a random number
generator
(RNG).^[Without
setting the seed, the results will change each time. In addition, changing
the value inside the brackets of `set.seed()`
will result in a different combination of individuals being selected for each new number --- test this
out in your code. This happens because the method relies on *pseudo random numbers* to select values probabilistically and `set.seed()` specifies where the
random number sequence should begin, ensuring reproducibility. We must really trust the random function used. Note that it is impossible for a computer to
choose *entirely* random numbers, so algorithms to generate *pseudo-random numbers* have been developed. See the documentation provided by `?set.seed`
for more information.
] 

The *reweighting* methods consists in adding a weight to each individual for the zone. This method is good if we have 
a representative sample of the zone and the minority of the population is included in it. In contrary, if we have in
the individual level only the majority of the population and for example, we have not an old man still working, this
kind of individual will not appear in the final data. A proposal to avoid this is to use a *genetic algorithm* or something
similar, that will allow to mix some individual to create a new one (mutation). We do not know for the moment
if this solution has already been developed, it is just a proposition and to inform the reader that new process can
still appear in the future.

## Iterative Proportional Fitting
### IPF in theory {#IpfinTh}

The most widely used and mature *deterministic* method to allocate individuals
to zones is iterative proportional fitting (IPF).
IPF is mature, fast and has a long history: it was demonstrated by
Deming and Stephan (1940)
for estimating internal cells based on known marginals.
IPF involves calculating a series of
non-integer weights that represent how representative each individual is of
each zone. This is *reweighting*.

Regardless of implementation method,
IPF can be used to allocate individuals to zones through the calculation
of *maximum likelihood* values for each zone-individual combination represented
in the weight matrix. This is considered as the most probable configuration of 
individuals in these zones. IPF is a method of *entropy maximisation*
(Cleave et al. 1995). 
The *entropy* is the number of configurations of the underlying spatial microdata
that could result in the same marginal counts.
For in-depth treatment of the mathematics and theory underlying IPF,
interested readers are directed towards Fienberg (1979), Cleave et al. (1995) and an excellent recent review
of methods for maximum likelihood estimation (Fienberg and Rinaldo 2007). For the
purposes of this book, we provide a basic overview of the method for the purposes
of spatial microsimulation.

```{r, echo=FALSE}
# TODO (RL): add Lovelace et al (2015) example when published above
```

In spatial microsimulation, IPF is used to allocate individuals to zones.
The subsequent section implements IPF to create *spatial microdata* for
SimpleWorld, using the data loaded in the previous chapter as a basis. 
Overviews of the use of spatial microsimulation method for spatial microsimulation
are provided recent papers by Lovelace and Ballas (2012) and, for in the
context of transport modelling, by Pritchard and Miller (2012).

```{r, echo=FALSE}
# TODO (MD): add reference above
#(Dumont,2014) Jojo thesis and article given by Eric 
```

Such as with the example of SimpleWorld, in each application have a matrix 
`ind` containing the categorical value of each individual. `ind` is a two 
dimensional array (a matrix) in which each row represents an individual and 
each column a variable. The value of the cell `ind(i,j)` is therefore the 
category of the individual `i` for the variable `j`. A second array containing 
the constraining count data `cons` can, for the purpose of explaining the 
theory, be expressed in 3 dimensions, which we will label `cons_t`: 
`cons_t(i,j,k)` is the number of individuals corresponding to the marginal 
for the zone 'i', in the variable 'j' for the category 'k'. For example, 
'i' could be a municipality, 'j' the gender and 'k' the female. 
Element '(i,j,k)' is the total number of woman in this municipality according 
to the constraint data.

The IPF algorithm will proceed zone per zone. For each zone, each individual 
will have a weight of representativity of the zone. The weights matrix 
will then have the dimension 'number of individual x number of zone'. 
'w(i,j,t)' corresponds to the weight of the individual 'i' in the 
zone 'j' (during the step 't'). For the zone 'z', we will adapt the weight matrix to
each constraint 'c'.This matrix is initialized with a full matrix of 1 
and then, for each step 't', the formula can be expressed as:

$$ w(i,z,t+1) = w(i,z,t) * \displaystyle\frac{cons \_ t(z,c,ind(i,c))}{ \displaystyle\sum_{j=1}^{Nbindiv} w(j,z,t) * I(ind(j,c)=ind(i,c))} $$


where the 'I(x)' function is the indicator function which value is 1 
if x is true and 0 otherwise. We can see that 'ind(i,c)' is the category 
of the individual 'i' for the variable 'c'. The denominator corresponds 
to the sum of the actual weights of all individuals having the same category
in this variable as 'i'. We simply redistribue the weights so that the 
data follows the constraint concerning this variable.



### IPF in R {#IpfinR}

In the subsequent examples, we use IPF to allocate individuals to zones
in SimpleWorld, using the data loaded in
the previous chapter as a basis. IPF is mature,
fast and has a long history.
Interested readers are directed towards recent papers (e.g. 
Lovelace and Ballas, 2012;
Pritchard and Miller,2012)
for more detail on the method.

The IPF algorithm can be written in
R from scratch, as illustrated in Lovelace (2014), and as taught in the
smsim-course online
[tutorial](https://github.com/Robinlovelace/spatial-microsim-book).
We will refer to this implementation of IPF as 'IPFinR'.
The code described in this tutorial 'hard-codes' the IPF algorithm
in R and must be adapted to each new application (unlike the generalized 'ipfp' approach,
which works unmodified on any reweighting problem).
IPFinR works by saving the weight
matrix after every constraint for each iteration. We here develop first IPFinR to 
give you an idea of the algorithm. Indeed, 'ipfp' package is more general, but
is like a "black box", so we can use it without being sure of what it performs.

The aim is to obtain the final weight matrix that will represent how well each individual fit each zone in terms of their characteristics. Each row of this matrix is an individual and each column 
is a zone. The algorithm will operate zone per zone. Thus the weight matrix will be filled in
column per column. To help the understanding of this section, we can rename the totals with 
a more intuitive name.

```{r}
# Create intuitiv names for the totals
NbZones <- nrow(cons) # number of zones
NbIndiv <- nrow(ind) # number of individuals
NbCatAge <-ncol(con_age) # number of categories of "age"
NbCatSex <-ncol(con_sex) # number of categories of "sex"
```

The earlier step before to perform to an algorithm is to initialize each needed object.
Here, we will have the *weight* matrix and the marginal distribution of individuals in each
zone. Thus first we create an object (`ind_agg0`), in which rows are zones and
columns are the different categories of the variables. Then, we duplicate the
weight matrix to keep in memory each step.

```{r}
# Create initial matrix of categorical counts from ind 
ind_agg0 <- t(apply(cons, 1, function(x) x^0 * ind_agg))
weights1 <- weights2 <- weights # create addition weight objects
```

IPFinR begins with a
couple of nested for loops, one to iterate through each zone
(hence `1:NbZones`, which means "from 1 to the number of zones in the
constraint data") and one to iterate through each
category within the constraints (0--49 and 50+ for the first constraint).
Note that this code relies on the `cons` and `ind` objects loaded in the
previous chapter.

```{r, echo=FALSE}
# The long way of doing it 
# # Create initial matrix of categorical counts from ind
# ind_agg0 <- matrix(rep(ind_agg, nrow(cons)), nrow = nrow(cons), byrow = T)
# weights1 <- weights2 <- weights # create addition weight objects for IPFinR
# # Assign values to the previously created weight matrix
# for(j in 1:nrow(cons)){
# for(i in 1:ncol(con_age)){
# weights1[ind_cat[ , i ] == 1, j] <- con_age[j , i] / ind_agg0[j , i]
# }
# }
```


```{r}
# Assign values to the previously created weight matrix 
# to adapt to age constraint
for(j in 1:NbZones){
  for(i in 1:NbCatAge){
weights1[ind_cat[, i] == 1, j] <- con_age[j, i] / ind_agg0[j, i]
    }
print(weights1)
  }
```

The above code updates the weight matrix by dividing each cell in
the census constraint (`con_age`) by the equivalent cell
aggregated version of the individual level data.
The weight matrix is critical to the spatial microsimulation procedure because
it describes how representative each individual is of each zone. To see the
weights that have been allocated to individuals to populate zone 2, for example
you would query the second column of the weights: `weights1[, 2]`. Conversely,
to see the weight allocated for individual 3 for each for the 3 zones, you
need to look at the 3rd column of the weight matrix: `weights1[3, ]`. 

Note that we asked R to write the resulting matrix after the completion of each zone.
As said before, the algorithm proceeds zone per zone and each column of the matrix
corresponds to a zone. This explains why the matrix is filled in per column. We can now 
verify that the weights correspond to the application of the theory seen before.
For the first zone, the age constraint was to have 8 people under 50 years old and 
4 over this age. The first individual is a man of 59 years old, so over 50.
To determine the weight of this person inside the zone 1, we multiply the 
actual weight, 1, by a ratio with a numerator corresponding to the
number of person in this category of age for the constraint, here 4,
and a denominator equal to the sum of the weights of the individual 
having the same age category. Here, there are 3 individuals of more 
than 50 years old and all weights are 1 for the moment. The new weight
is $$ 1 * \frac{4}{1+1+1}=1.33333$$.
We can verify the other weights with a similar reasoning. Now that
we explained the whole process under IPF, we can understand the origin
of the name 'Iterative Proportional Fitting'.

Thanks to the weight matrix, we can see that individual 3 
(who's attributes can be viewed by entering `ind[3, ]`)
is young and has a comparatively low weight of 1 for zone two. Intuitively
this makes sense because zone 3 has only 2 young adult inhabitants (see
the result of `cons[2,]`) but 8 older inhabitants. The reweighting stage is
making sense. Note also that the weights generated are fractional;
see [](#sintegerisation) for methods of converting fractional weights into
integer weights to generate a synthetic small-area population ready for
agent-based modelling applications. 

The next step in IPF, however, is to
re-aggregate the results from individual-level data after they have been
reweighted. For the first zone, the weights of each individual are in the
first column of the weight matrix. Moreover, the characteristics of each
individual are inside the matrix `ind_cat`. When multiplying `ind_cat` by 
the first column of `weights1` we obtain a vector, the values of which
correspond to the number of people in each category for zone 1.
To aggregate all individuals this for the
first zone, we just sum the values in each category. The following `for` loop
re-aggregates the individual-level data, with the new weights for each zone:

```{r}
# Create additional ind_agg objects
ind_agg2 <- ind_agg1 <- ind_agg0 * NA

# Assign values to the aggregated data after con 1
for(i in 1:NbZones){
  ind_agg1[i, ] <- colSums(ind_cat * weights1[, i])
}
```

Congratulations. Assuming you are running the code on you own computer, you
have just reweighted your first individual-level dataset to a geographical
constraint (the age) and have aggregated the results.
At this early stage it is important to do some preliminary checks
to ensure that the code is working correctly. First, are the resulting
populations for each zone correct? We check this for the first constraint variable
(age) using the following code (test: check the populations of the unweighted
and weighted data for the second constraint --- sex):

```{r}
rowSums(ind_agg1[, 1:2]) # the simulated populations in each zone
rowSums(cons[, 1:2]) # the observed populations in each zone
```

The results of these tests show that the new populations are correct, verifying
the technique. But what about the fit between the observed and simulated
results after constraining by age? We will cover goodness of fit in more
detail in subsequent sections. For now, suffice to know that the simplest
way to test the fit is by using the `cor` function on a 1d representation of the
aggregate-level data:

```{r}
vec <- function(x) as.numeric(as.matrix(x))
cor(vec(ind_agg0), vec(cons))
cor(vec(ind_agg1), vec(cons))
```

The point here is to calculate the correlation between the aggregate actual data
and the constraints. This value is between -1 and 1 and in our case, 
the best fit will be 1, meaning that there is a perfect correlation between our data
and the constraints. Note that as well as creating the correct total population for each zone, the
new weights also lead to much better fit. To see how this has worked, let's
look at the weights generated for zone 1:

```{r}
weights1[, 1]
```

The results mean that individuals 3 and 5 have been allocated a weight of 4
whilst the rest have been given a weight of $4/3$. Note that the total of these
weights is 12, the population of zone 1. Note also that individuals 3 and 5 are
in the younger age group (verify this with `ind$age[c(3,5)]`) which are more
commonly observed in zone 1 than the older age group:

```{r}
cons[1, ]
```

Note there are 8 individuals under 50 years old in zone 1, but only 2 individuals
with this age in the individual-level survey dataset. This explains why the
weights allocated to these individuals is 4: 8 divided by 2 = 4.

So far we have only constrained by age. This results in aggregate-level results
that fit the age constraint but not the sex constraint (figure 5.2). The reason
for this should be obvious: weights are selected such that
the aggregated individual-level data fits the age constraint perfectly, but
no account is taken of the sex constraint. This is why IPF must constrain for
multiple constraint variables.

To constrain by sex, we simply repeat the
nested `for` loop demonstrated above for the sex constraint. This is implemented
in the code block below.

```{r}
for(j in 1:NbZones){
  for(i in 1:NbCatSex + NbCatAge){
weights2[ind_cat[, i] == 1, j] <- cons[j , i] / ind_agg1[j, i]
    }
  }
```

Again, the aggregate values need to be calculated in a `for` loop over every
zone. After the first constraint fitting, the weights for zone 1 was:
$$(\frac{4}{3},\frac{4}{3},4,\frac{4}{3},4) $$
We can explain theoretically explain the weights for zone 1 after the
second fitting. For the first individual, its actual weight is $\frac{4}{3}$
and he is a male. In the zone one, the constraint is to have 6 men. The 
three first individuals are men, so the new weight for this person in this
zone is $$weights2[1,1]=\frac{4}{3}*\frac{6}{\frac{4}{3}+\frac{4}{3}+4}=\frac{6}{5}=1.2 $$

With an analogous reasoning, we can find all weights in *weights2*:

```{r}
weights2
```

Note that the final value is calculated by multiplying by
`weights1` *and* `weights2`: the final weight is calculated as the product
of all the weight matrices calculated from all the constraints.

```{r}
for(i in 1:NbZones){
ind_agg2[i, ] <- colSums(ind_cat * weights1[, i] * weights2[, i])
}
```

Note that even after constraining by age and sex, there
is still not a perfect fit between observed and simulated cell values
(figure 5.2). The simulated cell counts for the age categories are far from
the observed, whilst the cell counts for the age categories fit perfectly.
On the other hand, after constraining by age *and* sex, the fit is still not
perfect. This time the age categories do not fit perfectly, whilst the sex
categories fit perfectly. Inspect `ind_agg1` and `ind_agg2` and try to explain
why this is. Note that the *overall fit*, combining age and sex categories,
has improved greatly from iteration 1.1 to 1.2 (from $r = 0.63$ to $r = 0.99$).
In iteration 2.1 (in which we constrain by age again) the fit improves again.

So, each time we reweight to a specific constraint, the fit of this constraint 
is perfect, because, as seen in theory, it is a proportional reallocation. 
Then, we repeat for another constraint and the first one can diverge from his
perfect fit. However, when  operating this process several time, we always
refit to the next constraint and we can converge to a unique weight matrix.

These results show that IPF requires multiple *iterations* before converging on
a single weight matrix. It is relatively simple to iterate the procedure
illustrated in this section multiple times, as described in the smsim-course
tutorial. However, for the purposes of this book, we will move on now to consider
implementations of IPF that automate the fitting procedure and iteration process.
The aim is to make spatial microsimulation as easy and accessible as possible.


```{r valit_plot1, echo=FALSE, fig.cap="Fit between observed and simulated values for age and sex categories (column facets) after constraining a first time by age and sex constraints (iterations 1.1 and 1.2, plot rows). The dotted line in each plot represents perfect fit between the simulated and observed cell values. The overall fit in each case would be found by combining the left and right-hand plots. Each symbol correspond to a category and each category has a couple (observed, simulated) for each zone."}
# Make data long it with tidyr
library(tidyr)
x <- gather(cons, Category, Observed, a0_49:f)
y <- gather(as.data.frame(ind_agg1), cat, Simulated, a0_49:f)
z <- gather(as.data.frame(ind_agg2), cat, Simulated, a0_49:f)
# y$cat <- x$cat # to make categories identical (not needed)
df1 <- cbind(x,y)
df1$Constraint <- "1.1: Age"
df2 <- cbind(x, z)
df2$Constraint <- "1.2: Sex"
df <- rbind(df1, df2)
df$Variable <- "Age"
df$Variable[grep("[mf]", df$Category)] <- "Sex"
library(ggplot2)
qplot(Observed, Simulated, data = df, shape = Category, alpha = 0.5) +
  facet_grid(Constraint ~ Variable) +
  geom_abline(slope = 1, linetype = 3) +
  scale_alpha(guide = F) +
  scale_shape_manual(values = c(1,11,2,12)) +
  theme_bw()
# ggsave("figures/fit-obs-sim-simple-5.png")
# Attempt 2: do it with reshape2
# library(reshape2)
# melt(cons)
```

The advantage of hard-coding the IPF process, as illustrated above, is that
it is helps understand how IPF works and aides diagnosing issues with
the reweighting process as the weight matrix is re-saved after every
constraint and iteration. However, there are more computationally efficient
approaches to IPF. To save computer and researcher time, we 
use in the next sections 
R packages which implement IPF without the user needing to *hard code*
each iteration in R:
**ipfp** and  **mipfp**. We will use each of these methods to generate
fractional weight matrices allocating each individual
to zones. After following this reweighting process with **ipfp**, we will progress to
integerisation: the process of converting the fractional weights into integers. 
This process creates a final contingency table, which is used to generate the
final population. This last step is called the expansion.


```{r, echo=FALSE}
# Possibly more on IPF here. For now, press on
```

### Reweighting with **ipfp** {#ipfp}

IPF runs much faster and with less code using the
**ipfp** package than in pure R. The `ipfp` function runs the IPF algorithm
in the C language, taking aggregate constraints, individual level
data and an initial weight vector (`x0`) as inputs:

```{r}
library(ipfp) # load ipfp library after install.packages("ipfp")
cons <- apply(cons, 2, as.numeric) # to 1d numeric data type
ipfp(cons[1,], t(ind_cat), x0 = rep(1, NbIndiv)) # run IPF
```

It is impressive that the entire IPF process, which takes dozens of lines of
code in pure R can been condensed into two lines: one to
convert the input constraint dataset to `numeric`^[The integer data type fails
because C requires `numeric` data to be converted into its *floating point*
data class.]
and one to perform the IPF operation itself. The whole procedure is hiding
behind the function that is created in C and well optimised. So, it is like a 
magic box where you put your data and that returns the results. This is a good
way to execute the algorithm, but you have to pay attention to well understand 
the format of the input argument to use the function correctly. To be sure, 
type on R *help('ipfp')*.

Note that although
we did not specify how many iterations to run, the above command
ran the default of `maxit = 1000` iterations, despite convergence happening after
10 iterations. This can be seen by specifying the `maxit` and `verbose` arguments
(the latter of which can be referred to lazily as `v`) in `ipfp`, as illustrated below (only the first line of R output is shown):

```{r, eval=FALSE}
ipfp(cons[1,], t(ind_cat), rep(1, NbIndiv), maxit = 20, v = T)
```

```
## iteration 0:   0.141421
## iteration 1:   0.00367328
## iteration 2:   9.54727e-05
## ...
## iteration 9:   4.96507e-16
## iteration 10:  4.96507e-16
```

The `maxit` argument specifies to the function the maximum number of iterations and `verbose` set to True asks
the function to write for each iteration the convergence rate. This corresponds to the distance between
the previous and actual weight matrices. When the two matrix are equal, that means that the algorithm has
converged and the distance will approach 0. Note that when calculating, the computer has to make some approximations
(for example, when calculating the result of $\frac{4}{3}$, the computer cannot save the infinite number of decimals and truncates the number).
Due to this, we never reach a perfect 0, but when we have a very little number, we can consider it enough close to 0.
Usually, we use the precision of the computer that is of order $10^{-16}$ (on R, you can display the precision of the
machine by typing `.Machine$double.eps`). Note that that instead of using `maxit` as stopping condition, 
we can use the `tol` argument, conditional on the distance between two consecutive weight matrices.

Notice also that a *transposed* (via the `t()` function) version of the individual-level
data (`ind_cat`) is used in `ipfp`
to represent the individual-level data, instead of the
`ind_agg` object used in the pure R version. To prevent having to transpose
`ind_cat` every time `ipfp` is called, save the transposed version:

```{r}
ind_catt <- t(ind_cat) # save transposed version of ind_cat
```

Another object that can be saved prior to running `ipfp` on all zones
(the rows of `cons`) is `rep(1, nrow(ind))`, simply a series of ones - one for each individual.
We will call this object `x0` as its argument name representing
the starting point of the weight estimates in `ipfp`:

```{r}
x0 <- rep(1, NbIndiv) # save the initial vector
```

To extend this process to all three zones we can wrap the line beginning
`ipfp(...)` inside a `for` loop, saving the results each time into a
weight variable we created earlier:

```{r}
weights_maxit_2 <- weights # create a copy of the weights object
for(i in 1:ncol(weights)){
  weights_maxit_2[,i] <- ipfp(cons[i,], ind_catt, x0, maxit = 2)
}
```

The above code uses `i` to iterate through the constraints, one row (zone) at
a time, saving the output vector of weights for the individuals into columns
of the weight matrix. To make this process even more concise (albeit
less clear to R beginners), we can use R's internal
`for` loop, `apply`:

```{r}
weights <- apply(cons, MARGIN = 1, FUN = 
    function(x) ipfp(x, ind_catt, x0, maxit = 20))
```

In the above code R iterates through each row
(hence the second argument `MARGIN` being `1`, `MARGIN = 2`
would signify column-wise iteration).
Thus `ipfp` is applied to each zone in turn, as with the `for` loop implementation. 
The speed savings of writing the function 
with different configurations are benchmarked in
'parallel-ipfp.R' in the 'R' folder of the book project directory.
This shows that reducing the maximum iterations of `ipfp` from
the default 1000 to 20 has the greatest performance benefit.^[These
tests also show that any speed gains from using `apply` instead of `for` are negligible, so
whether to use `for` or `apply` can be decided by personal preference.]
To make the code run faster on large datasets, a parallel version of
`apply` called `parApply` can be used. This is also
tested in 'parallel-ipfp.R'.

For your personal applications, take care of using `maxit` alone. Indeed, it is
impossible to predict the number of iterations necessary for all applications.
So, if your argument is to big you will needlessly lose time, but if it is
to small, you will have a result that has not converge! However, using
`tol` alone is also hazardous. Indeed, if you have iteratively the same
matrices, but the approximations on the distance is a number bigger than 
your argument, the algorithm will continue. That's why we use both together and
it stops either if the convergence is achieve nor the maximum number of iterations
is reached. By defaults, `maxit` is 1000 and `tol` is `.Machine$double.eps`. 

```{r, echo=FALSE, eval=FALSE}
# Also discuss what happens when you get a huge dataset, from Stephen's dataset
```

It is important to check that the weights obtained from IPF make sense.
To do this, we multiply the weights of each individual by rows of
the `ind_cat` matrix, for each zone. Again, this can be done using
a for loop, but the apply method is more concise:

```{r}
ind_agg <- t(apply(weights, 2, function(x) colSums(x * ind_cat)))
colnames(ind_agg) <- colnames(cons) # make the column names equal
```

As a preliminary test of fit,
it makes sense to check a sample of the aggregated weighted data
(`ind_agg`) against the same sample of the constraints.
Let's look at the results (one would use a subset of the results, 
e.g. `ind_agg[1:3, 1:5] for the first five values of the first 3
zones for larger constraint tables found in the real world):

```{r}
ind_agg
cons
```

This is a good result: the constraints perfectly match the results
generated using ipf, at least for the sample. To check that this
is due to the `ipfp` algorithm improving the weights with each iteration,
let us analyse the aggregate results generated from the alternative
set of weights, generated with only 2 iterations of IPF:

```{r}
# Update ind_agg values, keeping col names (note '[]')
ind_agg[] <- t(apply(weights_maxit_2, MARGIN = 2, 
  FUN = function(x) colSums(x * ind_cat)))
ind_agg[1:2, 1:4]
```

Clearly the final weights after 2 iterations of IPF represent the constraint
variables well, but do not match perfectly except in the second constraint. This shows the importance of
considering number of iterations in the reweighting stage --- too many iterations
can be wasteful, too few may result in poor results. To reiterate,
20 iterations of IPF are sufficient in most cases for the results
to converge towards their final level of fit. 
More sophisticated ways of evaluating model fit are
presented in Section \ref{svalidation}.
As mentioned at the beginning of this chapter there are alternative methods
for allocating individuals to zones. These are discussed in a subsequent section
[](#alternative-reweighting)
.

For some applications, fractional weight matrices are sufficient for analysis.
Often, however, an individual level resulting dataset is required. To estimate the individual-level
variability in target variables such as income, or for agent-based modelling
applications, a full spatial microdataset, composed of whole individuals is
required. For this, we have to possibly. First, we can consider the weights as
probability and randomly chose the individuals in a distribution corresponding to the weights.
Secondly, we can consider the weights as the number of individual in this category and
the fractional weights must be integerised in a process
known as integerisation (Lovelace and Ballas, 2013). This is the subject of
the next section.

## Reweighting with **mipfp** {#mipfp}

```{r, echo=FALSE}
# Morgane: please make this example work with the SimplWorld data
# DONE 
```


The R package **mipfp** is a more generalized implementation of the IPF algorithm
than **ipfp**, and was designed for population synthesis.
**ipfp** generates a two-dimensional weight matrix based on mutually
exclusive (non cross-tabulated) constraint tables.
This is useful for applications using constraints, which are only marginals and 
which are not cross-tabulated.
**mipfp** is more flexible, allowing multiple cross-tabulations
in the constraint variables, such as age/sex and age/class combinations.

```{r, echo=FALSE}
# we have a new column with, at most, as many categories as the product of the number of categories for the variable age
# and 2 (Male or Female). However, in this case, we need to have the cross table of the age and the sex available to 
# proceed this way.
```

**mipfp** is therefore a multidimensional implementation of 
IPF, which can 
update an initial $N$-dimensional array with respect to given 
target marginal distributions. These, in turn, can be multidimensional. 
In this sense **mipfp** is more advanced than
**ipfp** which solves only the 2 dimensional case.

The main function of **mipfp** is `Ipfp()`, which
fills a $N$-dimensional weight 
matrix based on a range of aggregate-level constraint table options. 
Let's test the package on some example data.
The first step is to load the package into the workspace:

```{r}
library(mipfp) # after install.packages("mipfp")
```

To illustrate the use of `Ipfp`, we will create a fictive example. 
The case of SimpleWorld is here too simple to really illustrate the power 
of this package. The solving of SimpleWorld with `Ipfp` is included in 
the next section containing the comparison between the two packages.

The example case is as follows: to determine the contingency table 
of a population characterized by categorical variables for age (0-17, 18-50, 50+),
gender (male, female) and educational level (level 1 to level 4).
We consider a zone with 50 inhabitants. The classic spatial microsimulation 
problem consists in having all marginal distributions and the cross-tabulated
result (age/gender/education in this case) only for a non-geographical sample. 

We consider the variables in the following 
order: sex (1), age (2) and diploma (3).
Our constraints could for example be:

```{r}
sex <- c(23,27) # the number in each sex category
names(sex) <- c("Male","Female")

age <- c(16,20,14) # the number in each age category
names(age) <- c("Less18","18-50","More50")

diploma <- c(20,18,6,6) # the number in each education category
names(diploma) <- c("Level1","Level2","Level3","Level4")
```

We can verify that the population is equal in each constraint (50 people). Note also
the order in which each category is encoded --- a common source of
error in population synthesis.
For this reason we have labelled each category of each constraint.
The constraints are the target margins and need to be stored as a list.
To tell the algorithm 
which elements of the list correspond to which constraint,
a second list with the description 
of the target must be created. By printing *target* before to run the algorithm, 
we can verify that we have encoded everything well.

```{r}
target <- list (sex, age, diploma)
descript <- list (1, 2, 3)
target
```

Now that all constraint variables have been encoded, let us define 
the initial array to be updated, also referred as the seed or the weight matrix. The dimension
of this matrix must be identical to that of the constraint tables:
$(2 \times3 \times 4)$. Each cell of the array represents 
a combination of the attributes' values, and thus defines a particular 
category of individuals. In our case, we will consider that the weight
matrix contains 0 when the category is not possible and 1 otherwise. In this example we 
will assume that it is impossible for an individual being less than 18 
years old to hold a diploma level higher than 2. 
The corresponding cells are then set to 0, while the cells of the 
feasible categories are set to 1.

```{r}
names <- list (names(sex), names(age), names(diploma))
weight <- array (1, c(2,3,4), dimnames = names)
weight[, c("Less18"), c("Level3","Level4")] <- 0
```

Everything being well defined, we can execute
*Ipfp*. As with the package **ipfp**, we can chose a stoping criterion defined by *tol* and/or a maximum
number of iterations. Finally, setting the argument `print` to `TRUE` 
ensures we will see the evolution of the
of the procedure. After reaching either the maximum number of iterations or convergence 
(whichever comes first), the function will return a list containing the updated 
array, as well as other informations about the convergence of the algorithm. 
Note that if the target margins are not consistent, the input data is then 
normalised by considering probabilities instead of frequencies.

```{r}
result <- Ipfp(weight, descript, target, iter = 50, 
               print = TRUE, tol = 1e-5)
```

We can observe that the stoping criterion becomes always smaller and attains the *tol* after 8 
iterations. The `result`contains the final weight matrix
and some information about the convergence. 
We have a resulting table and we can validate the total number of 50 inhabitants 
in the zone. Note that, thanks to the definitions of names in the array, we can easily 
interpret the result. There are a total 50 people and nobody of less than 18 years old
own a diploma level 3 or 4, as desired.

```{r}
# printing the result
result$xi.hat 
# checkin the total number of persons
sum(result$xi.hat)
```

The quality of the margins with each constraints is contained in the variable `check.margins`
of the resulting list. In our case, we fit all constaints.

```{r}
# printing the resulting margins
result$check.margins
```

This reasoning works zone per zone and we can generate a 3-dimensional weight matrix. Another
advantage of **mipfp** is that it allows cross-tabulated constraints to be added.
In our example, we could add as target the contingency of the age and the diploma. 
We can define this cross table:

```{r}
# define the cross table
cross <- cbind(c(11,5,0,0), c(3,9,4,4), c(6,4,2,2))
rownames (cross) <- names (diploma)
colnames (cross) <- names(age)

# print the cross table
cross
```

When having several constraints concerning the same variable, we have to insure 
the consistency across these targets (otherwise convergence might not be reached). 
For instance, we can display the margins of `cross` along with `diploma` and `age`
to validate the consistency assumption. There are
both equal, meaning that the constraints are coherent.

```{r}
# check pertinence for diploma 
rowSums(cross)
diploma

# check pertinence for age 
colSums(cross)
age
```

The `target` and `descript` have to be updated to include the cross table.
Pay attention to the order
of the arguments by declaring a contingency table. Then, we can execute
the task and run *Ipfp* again:

```{r}
# defining the new target and associated descript
target <- list(sex, age, diploma, cross)
descript <- list(1, 2, 3, c(3,2))

# running the Ipfp function
result <- Ipfp(weight, descript, target, iter = 50, 
               print = TRUE, tol = 1e-5)
```

The addition of a constraint leave less freedom to the algorithm, implying that the algorithm 
converges faster. By printing the results, we see that all constraints are respected and
we have a 3-dimensional contingency table.

Similarly to the **ipfp** package, the result is in a non-integer array. Consequently 
we can generate a synthetic population either by considering the array cells 
as probabilities for random draws or by applying steps similar to 
the integerisation and expansion steps defined in the `ipfp` Section.

```{r, echo=FALSE}
# TODO: (MD) : put a real reference to the section of integerisation and expansion
```

Able to deal with bigger problems, the package presented here can also manage to 
solve smaller problem such as SimpleWorld. We have simply chosen to develop here a 
different example to show you the vast application of this function. However, 
to compare **ipfp** with **mipfp**, we will focus on this simpler example :
SimpleWorld.


### Comparing **ipfp** with **mipfp** {#Compareipf}

```{r, echo=FALSE}
# We need to establish which is faster (RL)
```

A major difference between the **ipfp** package and **mipfp** is that the
former is written in C, while the latter is written in R. With this 
package, some coding and manipulations of 
data has to be performed to consider more than two variables. Note that it is 
normal, since this package was initially designed to resolve linear problem of
the form $Ax=b$. On the other hand, the aim of **mipfp** is to execute an iterative 
proportional fitting. This function allows the solving of spatial microsimulation
whatever the number of variables and the form of the constraints (cross-table or marginal 
distributions). Moreover, the code below this "black box" is written directly in R
and does not require another programming language.

As explained before, **ipfp** gives weights to each individual in the sample. 
On the other hand, **mipfp** works differently. Indeed, individuals having
the same caracteristics could appear only once and have a starting weight of 2
instead of 1. The logic behind **mipfp** is to determine the number of people 
in each possible different category. This can be explained for SimpleWorld. 
We have constraints about sex and age. What will be defined is the number
of persons in each category so that the contingency table respects the constraints.
In other worlds, we want, for zone 1 of SimpleWorld, to fill in the table such as:


Table: Aim of Mipfp for SimpleWorld.

|sex - age | 0-49 yrs| 50 + yrs| Total|
|:------|-----:|-----:|-----:|
|m      |     ?|     ?|     6|
|f      |     ?|     ?|     6|
|Total  |     8|     4|      |

When having this table available and respecting the constraint, we know how many
individuals we need from each category. From this, it is easy to generated the 
individual dataset. For SimpleWorld, an microdata is available. We can start with 
this table and then generate the final contingency table (sex x age) thanks to formula
determined in the theoretical part of ipfp. 

For first step, we begin with :

Table: Initial table of Mipfp for SimpleWorld.

|sex - age | 0-49 yrs| 50 + yrs| Total| Wanted total |
|:------|-----:|-----:|-----:|-----:|
|m      |     1|     2|     3|     6|
|f      |     1|     1|     2|     6|
|Total  |     2|     3|      |      |
|Wanted total  |     8|     4|      |      |


The IPF procedure makes a proportion of the existing table to fit each constraint
turn by turn. First, if we want to fit the age constraint, we need to totalise
8 persons under 50 years old and 4 above. Thus, we calculate a proportional 
weight so that this constraint is respected:

Table: First step of Mipfp for SimpleWorld - age constraint.

|sex - age | 0-49 yrs| 50 + yrs| Total| Wanted total |
|:------|-----:|-----:|-----:|-----:|
|m      |     4|     2.7|     6.7|     6|
|f      |     4|     1.3|     5.3|     6|
|Total  |     8|     4|      |      |
|Wanted total  |     8|     4|      |      |

First cell corresponds to $1*\frac{8}{2}$. Now, the current matrix fits exactly 
the age constraint. We will proceed to a similar reasoning for the sex constraint:

Table: First step of Mipfp for SimpleWorld - sex constraint.

|sex - age | 0-49 yrs| 50 + yrs| Total| Wanted total |
|:------|-----:|-----:|-----:|-----:|
|m      |     3.6|     2.4|     6|     6|
|f      |     4.5|     1.5|     6|     6|
|Total  |     8.1|     3.9|      |      |
|Wanted total  |     8|     4|      |      |

This ends the first iteration of ipfp, in the form of contingency table. We just 
follow the same logic, but by watching it in terms of tables.


Both methods has to do a similar simulation. We will compare here the results and time used 
to solve the simple problem of SimpleWorld.

```{r, echo=FALSE}
# TODO: (MD) compare with simpleworld
#       (MD) if mipfp is better, justify why we use ipfp with cakemap... Or compare also with cakemap

# TODO (RL): cross reference image of CO vs reweighting approaches
```

The whole process using **ipfp** has been developed. Now, we solve the SimpleWorld 
problem with **mipfp**. We proceed zone per zone. To explain the process, the generation 
of the first zone is here explained. The constraints for this area are reach as
followed :

```{r}
# SimpleWorld constraints for zone 1
con_age[1,]
con_sex[1,]

# list the constraints for zone 1
target <- list (data.matrix(con_age[1,]), data.matrix(con_sex[1,])[c(2,1)])
descript <- list (1, 2)
```

Remark that we have to take the sex data in the other order to be coherent with the
seed.
The sample available establishes the seed of the algorithm, also kn owned as 
the initial weight matrix. We can take into account the entire individual level
data, with the chosen categories. Thus, we keep the separation between ages 
under and above 50, but we also consider the income information, even if it 
does not appear in the constraints.

```{r}
# Add income to the categorised individual data
ind_income <- cbind(ind, ind_full[,c("income")])
weight_init <- table(ind_income[, c(2,3,4)])
```

Having the initial weight initialised to the aggregation of the 
individual level data available and having the constraints well established, 
we can now execute the `Ipfp` function.

```{r}
# Perform Ipfp on SimpleWorld example
weight_mipfp <- Ipfp( weight_init, descript, target, print = TRUE, tol = 1e-5)

# Printing the resulting weight matrix for age and sex
apply(weight_mipfp, c(1,2), sum)
```

We can see that we reach the convergence after 5 iterations
and the results are fractional, as the one funded with **ipfp**. 
Now that the process is understood, we can generate the weight
matrix for each zone. The aim is here to transform the output
into a form similar to the one generated in the section **ipfp** 
to allow an easy comparison.

Note that for each area, the initial weight matrix will be the same.
The final table will be called `weights_mipfp`. To complete its cells,
we will need the number of individual in the sample in each category.
Remark that the output of the function of **mipfp** can be used 
directly to perform steps such as integerisation and expansion.
However, for this example, we would like to verify that both algorithm
calculated the same weights. For this, one output has to be transform 
into the form of the output of the other function. We here
adapt the `Ipfp` result to fit the `ipfp` final matrix. 
To clearly identify what is added to have comparable matrices, 
you can see the comment "- FOR COMPARISON".

```{r}
# Matrix which rows are individual and columns are zones - FOR COMPARISON
weights_mipfp <- matrix(data = NA, nrow = nrow(ind), ncol = nrow(cons))

# Table of ind - FOR COMPARISON
Ind_Tab <- table(ind[,c(2,3)])
```

Now, we can loop on the zones and generate the whole process with the function
of the package **mipfp**. This is done thanks to iterations that calculate
the weights per category for the zone and then transform them into weights
per individual, depending on the category of each person.

```{r}
# Loop on the zones
for (zone in 1:NbZones){
  # Adapt the constraint 
  target <- list (data.matrix(con_age[zone,]), data.matrix(con_sex[zone,])[c(2,1)])

  # Calculate the weights
  res <- Ipfp( weight_init, descript, target, tol = 1e-5)
  
  # Table of calculated weights
  Mipfp_Tab <- apply(res,c(1,2),sum)

  # Transformation into individual weights - FOR COMPARISON
  for (i in 1:NbIndiv) {
    weight_ind <- Mipfp_Tab[ind[i,c(2)],ind[i,c(3)]]
    sample_ind <- Ind_Tab[ind[i,c(2)],ind[i,c(3)]]
    weights_mipfp[i,zone] <- weight_ind / sample_ind
  }
}
```

The simulation of SimpleWorld with **mipfp** is now finished, 
since we generated weights for each individual in each zone. The
resulting matrix is exactly the same as the one created thanks to 
the package **ipfp** and the integerisation and expansion step can
be the same. However, there is a more direct way to transform the
weights of `Ipfp` into individual data, which is explained in the 
chapter *Population synthesis without input microdata*.

Both packages being executed for the same example and both 
result having retranscribe to the same form, we can compare the 
results. We observe that the bigger difference is of the order
$10^{-7}$, which is very little and can be caused by some
computational approximations during the calculus. Thus, 
both functions generate the same weights.

```{r}
# Difference between weights of ipfp and mipfp
abs(weights-weights_mipfp)
```

One package is written in C and the other in R. The current question
is : Which package is more efficient? The answer
will depend on your application. We have seen that both 
algorithms do not consider the same form of input. For **ipfp**
you need an initial individual level data, coming from example from 
a survey and constraints. For **mipfp** the constraints and a contingency table of 
a survey is enough. Thus, when having no microdata, you will have 
to use **mipfp** (described in a subsequent section). Moreover, 
their results are also in different structures. Weights are created for each 
individual with **ipfp** and weights for the contingency table with **mipfp**
(that can be retranscribe in the other form if needed).

In conclusion to this comparison, **mipfp** is more adaptable
than **ipfp**. Which to use will depend on the structure
of the data available. In terms of computational time,
the comparison demonstrates that **mipfp** was slower
for the first zone of SimpleWorld. 

```{r, eval=FALSE}
# Measure time ipfp
system.time(ipfp(cons[1,], ind_catt, x0 = rep(1,NbIndiv),  tol = 1e-5))
```

This code measure the time taken to execute the `ipfp` function for the 
zone 1. It takes 0.001 second on a modern computer[The tests were performed on a
computer with Intel i7-4930K processer running at 3.40GHz.]
The code below make a similar analysis
for the `Ipfp` function and takes 0.002 second on the same computer.
Thus, for this very little example, the package **ipfp** written in C
is faster.

```{r, eval=FALSE}
# Chose right constraint for mipfp
target <- list (data.matrix(con_age[zone,]), data.matrix(con_sex[zone,])[c(2,1)])

# Equitable calculus - remove income 
weight_init_no_income <- table( ind[, c(2,3)])

# Measure time mipfp
system.time(Ipfp( weight_init_no_income, descript, target, tol = 1e-5))
```

An advantage of the **mipfp** package for problems with a lot 
of input individuals is in terms of memory. Indeed, **ipfp**
needs a table with as many rows as individuals, whereas **mipfp**
needs a table of a dimension corresponding to the number of 
different categories. For SimpleWorld, the inputs of **ipfp** 
and **mipfp** are respectively:

```{r}
# input for ipfp
ind_cat

# input for mipfp
Ind_Tab
```

Imagine that we have a similar problem, but with a total 
population of 2000 and 500 initial individuals. The dimension 
of `ind_cat` will become 500 rows for 4 columns, where the
one of `Ind_Tab` will not change. 

By testing the time for SimpleWorld when considering having 25,000
individuals (we replicate the data 5000 times) and generating 
12,000,000 individuals (we multiply the constraints by 1000), we obtain
on the same computer that **ipfp** and **mipfp** take, respectively,
0.012 and 0.005 seconds.

```{r, echo=FALSE}
#### time tests for bigger databases

ind_cat2 <- ind_cat

for (i in 1:4999){
  ind_cat2<-rbind(ind_cat2,ind_cat)
}

Ind_Tab2 <- 5000 * Ind_Tab

cons2 <- cons * 1000000

# Measure time ipfp
system.time(ipfp(cons2[1,], t(ind_cat2), x0 = rep(1,NbIndiv * 5000),  tol = 1e-5))

# Chose right constraint for mipfp
target <- list (data.matrix(con_age[zone,] *1000000), data.matrix(con_sex[zone,]*1000000)[c(2,1)])

# Equitable calculus - remove income 
weight_init_no_income <- table( ind[, c(2,3)])*5000

# Measure time mipfp
system.time(Ipfp( weight_init_no_income, descript, target, tol = 1e-5))
```

In conclusion to this comparison, **mipfp** is more adaptable
than **ipfp**. Which to use will depend on the structure
of the data available. In terms of computational time,
the comparison demonstrates that **mipfp** was slower
for small problems and faster for bigger problems. 
In all cases, they calculate the same results.



## The GREGWT algorithm

```{r, echo=FALSE}
# TODO: make reproducible example, compare performance with IPF
```

As described in the Introduction, IPF is just one strategy for obtaining
a spatial microdataset. However, researchers tend to
select one method that they are comfortable and stick with that for their models.
This is understandable because setting-up the method is usually time consuming:
most researchers rightly focus on applying the methods to the real world rather
than fretting about the details. On the other hand, if alternative methods
work better for a particular application, resistance to change can result
in poor model fit. In the case of very large datasets, spatial microsimulation
may not be possible unless certain methods, optimised to deal with large
datasets, are used. Above all, there is no consensus about which methods
are 'best' for different applications, so it is worth experimenting to identify
which method is most suitable for each application.

An interesting alternative to IPF method is the GREGWT algorithm.
First implemented in the SAS language by the Statistical Service area of
the Australian Bureau of Statistics (ABS), the algorithm reweighs a set of
initial weights using a
Generalized Regression Weighting procedure (hence the name GREGWT).
The resulting weights ensure that, when aggregated, the individuals selected
for each small area fit the constraint variables. Like IPF, the GREGWT
results in non-integer weights, meaning some kind of integerisation 
algorithm will be needed to obtain a final individual-level population, so
for example, if the output is to be used in ABM. The macro
developed by ABS adds a weight restriction in their GREGWT macros to ensure
positive weights. The ABS uses
the Linear Truncated Method described in Singh and Mohl (1996) to enforce these
restrictions.

```{r, echo=FALSE}
# A problem with this
# approach is that the geneated weights produced by GREG do not have any
# restrictions, this leads to the generation of negative weights.
```

A clear simplified example of this algorithm (and other algorithms) can be
found in Rahman (2009).  In their paper Tanton et.al (2011) make a full
description of the algorithm. For a deeper understanding of the SAS macros see
Bell (1999). An R implementation of GREGWT, has been
created by Esteban Muñoz, and can be found in the GitLab repository
[mikrosim](https://gitlab.com/emunozh/mikrosim).

## Population synthesis as an optimization problem

In general terms, an *optimization problem* consists of a function,
the result of which must be minimised or maximised, called an *objective function*. This function is not necessarily defined 
for the entire domain of possible inputs. The domain where this function is defined is called
the *solution space* (or the *feasible space* in formal mathematics).
This implies that optimization problems can be *unconstrained* or *constrained*, by
limits on the values that arguments (or that a function of the arguments) of the function can take
([Boyd 2004](http://stanford.edu/~boyd/cvxbook/)). If there are constraints, the solution space
include only a part of the domain of the objective funcion. The constraints define the
solution space. Under this framework,
population synthesis can be seen as a *constrained optimisation* problem.
Suppose $x$ is a vector of length $n$ $(x_1,x_2,..,x_n)$ whose values are to be adjusted. In this case
the value of the objective function is $f_0(x)$, depends on $x$. The possible values of $x$ are defined
thanks to *par*, a vector of predefined arguments or parameters of length $m$ ($m$ is the number of constraints)
($par_1,par_2,..,par_m$). 
This
kind of problems can be expressed as:

$$\left\{
\begin{array}{l}
  min \hspace{0.2cm}f_0(x_1,x_2,..,x_n) \\
  \\
  s.c. \hspace{0.2cm} f_i(x) \leq par_i,\  i = 1, ..., m
\end{array}
\right.$$

Applying this to the problem of population synthesis,
the parameters $par_i$ represent 0, since all cells have to be positive.
The $f_0(x)$, to be minimised, is the distance between the actual weight matrix and
the aggregate constraint variable
`cons`. $x$ represents the weights which will
be calculated to minimise
to minimise $f_0(x)$.

To illustrate the concept further, consider the case of aircraft design.
Imagine that the aim (the objective function) is to minimise
weight by changing its shape and materials. But these modifications
must proceed subject to some constraints, because the
airplane must be safe and sufficiently voluminous to 
transport people. Constrained optimisation in this case would involve
searching combinations of shape and material (to include in $x$) that minimise
the weight (the result of $f_0$, is a single value depending on $x$). This search
must take place under constraints relating to
volume (depending on the shape) and safety
($par_1$ and $par_2$ in the above notation). Thus $par$ values
define the domain of the *solution space*. We search inside this domain
the combination of $x_i$ that minimise weight.

The case of spatial microsimulation has relatively
simple constraints: all weights must be positive or zero:

$$
\left\{ weight_{ij} \in \mathbb{R}^+  \cup \{0\} \hspace{0.5cm} \forall i,j \right\}
$$

Seeing spatial microsimulation as an optimisation problem
allows solutions to be found using established
techniques of *constrained optimisation*.
The main advantage of this re-framing
is that it allows any optimisation algorithm to perform the reweighting.

To see population synthesis as a constrained optimization problem analogous to
aircraft design, we must define the problem to optimise, the variable $x$ and then
set the constraints.  
Intuitively, we search the number of occurrences we want of each individual to take
form the final population that fits the best the constraints. We could take
the weight matrix as $x$ and as the objective function the difference between the 
population with this weight matrix and the constraint. However, we want to include
the information of the distribution of the sample. 
Thus, we want to find a vector of parameters `par` that will multiply the `indu` matrix,
which is similar to `ind_cat`, but with only different rows and the cells contain
the number of time that this kind of individual appears in the sample. We want the result
of this multiplication to be as closer as possible to the constraints.

As for the IPF proposition, we will proceed zone per zone. So, our 
optimization problem for the first zone can be written as follows:

$$\left\{
\begin{array}{l}
  min \hspace{0.2cm} f(par_1,..,par_m) = DIST(sim, cons[1,]) \\
  \hspace{0.8cm} where  \hspace{0.2cm} sim=colSums(indu * par)\\
  \\
  s.c. \hspace{0.2cm} par_i \geq 0,\  i = 1, ..., m
\end{array}
\right.$$

Key to this is interpreting individual weights as parameters (the vector $par$,
of length $m$ above)
that are iteratively modified to optimise the fit between individual and
aggregate-level data. Remarks that in comparison with the theoretical 
definition of an optimisation problem, our `par` are the theorical $x$. 
The measure of fit, so the distance, we use in this context is
Total Absolute Error (TAE).

$$\left\{
\begin{array}{l}
  min \hspace{0.2cm} f(par_1,..,par_m) = TAE(sim, cons[1,]) \\
  \hspace{0.8cm} where  \hspace{0.2cm} sim=colSums(ind\_cat * par)\\
  \\
  s.c. \hspace{0.2cm} par_i \geq 0,\  i = 1, ..., m
\end{array}
\right.$$

We have chosen the distance "TAE", but we could imagine to do the same with 
other metrics.


```{r, echo=FALSE}
# (MD) Compare with different metrics?
```

Note that in the above, $par$ is equivalent to the `weights` object
we have created in previous sections to represent how representative
each individual is of each zone. 
The main issue with this definition of reweighting is therefore the large
number of free parameters: equal to the number of individual-level dataset.
Clearly this can be very very large. To overcome this issue,
we must 'compress' the individual level dataset to its essence, to contain
only unique individuals with respect to the constraint variables
(*constraint-unique* individuals).

The challenge is to convert the binary 'model matrix' form of the
individual-level data (`ind_cat` in the previous examples) into
a new matrix (`indu`) that has fewer rows of data. Information about the
frequency of each constraint-unique individual is kept by increasing the
value of the '1' entries for each column for the replicated individuals
by the number of other individuals who share the same combination of attributes.
This may sound quite simple, so let's use the example of SimpleWorld to
illustrate the point.

### Reweighting with optim and GenSA

The base R function `optim` provides a general purpose optimization framework
for numerically solving objective functions. Based on the objective function
for spatial microsimulation described above,
we can use any general optimization algorithm for reweighting the
individual-level dataset. But which to use?

Different reweighting strategies are suitable in different contexts and there
is no clear winner for every occasion. However, testing a range of
strategy makes it clear that certain algorithms are more efficient than
others for spatial microsimulation. Figure x demonstrates this variability
by plotting total absolute error as a function of number of iterations for
various optimization algorithms available from the base function
`optim` and the **GenSA** package. Note that the comparisons 
are performed only for zone 1.

```{r, fig.cap="Relationship between number of iterations and goodness-of-fit between observed and simulated results for different optimisation algorithms", fig.width=4, fig.height=4, echo=FALSE}
# commented as pdf version looks better! 
# img <- readPNG("figures/TAEOptim_GenSA_Mo.png")
# grid.raster(img)
```

\begin{figure}
\includegraphics{figures/TAEOptim_GenSA_Mo.pdf}
\caption{Relationship between number of iterations and goodness-of-fit between observed and simulated results for different optimisation algorithms.}
\end{figure}


```{r, eval=FALSE, echo=FALSE}
source("R/optim-tests-SimpleWorld.R", echo = FALSE, print.eval = FALSE )
qplot(data = opt_res, time, fit, color = algorithm, geom="line") +
  ylab("Total Absolute Error") + xlab("Time (microseconds)") + scale_color_brewer(palette = 2, type = "qual") + theme_classic() + xlim(c(0, 15000))
```

Figure x shows that, in a first step, all algorithm reach a real improvement during the first iteration. The advantage of the IPF algorithm we have been using, which
converges rapidly to an error very close to zero (as seen before, zero is not
reachable with a computer performing calculus) is observable after only the first iteration.
On the other end of the spectrum is R's default optimization algorithm,
the Nelder-Mead method. Although the graph shows no improvement
from one iteration to the next, it should be stated that the algorithm
is just 'warming up' at this stage and than each iteration is very
fast, as we shall see. After 400 iterations (which happen
in the same time that other algorithms take for a single iteration!),
the Nelder-Mead begins to converge: it works effectively. 
However, it requires far more iterations to
converge to a value approximating zero than IPF.
Next best in terms of iterations is `GenSA`, the Generalized Simulated
Annealing Function from the **GenSA** package. `GenSA`
attained a near-perfect fit after only two full iterations.

The remaining algorithms shown are, like Nelder-Mead, available from within R's
default optimisation function `optim`. The implementations with `method =` set
to `"BFGS"` (short for the Broyden–Fletcher–Goldfarb–Shanno algorithm),
`"CG"` ('conjugate gradients') performed roughly the same, steadily approaching
zero error and fitting to `"IPF"` and `"GenSA"` after 10 iterations. Finally, the `SANN` method,
also available in `optim`, performed most erratically of the methods tested.
This is another implementation of simulated annealing which demonstrates that
optimisation functions that depend on random numbers do not always lead to
improved fit from one iteration to the next. If we look until 200 iterations, 
the fit will continue to oscillate and not be improved at all.

The code used to test these alternative methods for reweighting are provided
in the script 'R/optim-tests-SimpleWorld.R'. The results
should be reproducible on
any computer, provided the book's supplementary materials have been downloaded.
There are many other optimisation algorithms available in R through a wide
range of packages and new and improved functions are being made available all the time.
Enthusiastic readers are encouraged to experiment with the methods presented here:
it is possible that an algorithm exists which outperforms all of
those tested for this book. Also, it should be noted that the algorithms
were tested on the extremely simple and rather contrived example dataset
of SimpleWorld. Some algorithms may perform better with larger datasets than others
and may be sensitive to changes to the initial conditions
such as the problem of 'empty cells'.

```{r, echo=FALSE}
# TODO: cite performance testing paper here
```

Therefore these results, as with any modelling exercise,
should be interpreted with a healthy dose of skepticism: just because an
algorithm converges after few 'iterations' this does not mean it is
inherently any faster or more useful than another. The results are context
specific, so it is recommended that the tested framework
in 'R/optim-tests-SimpleWorld.R' is used as a basis for further tests
on algorithm performance on the datasets you are using.
IPF has performed well in the situations I have tested it in (especially
via the `ipfp` function, which performs disproportionately faster
than the pure R implementation on large datasets) but this does not mean
that it is always the best approach.

To overcome the caveat that the meaning of an 'iteration' changes dramatically
from one algorithm to the next, further tests measured the time taken
for each reweighting algorithm to run. To have a readable graph, we 
do not represent the error as a function of the time, but the time per
algorithm in function of the `maxit` argument (Figure xx). This figure
demonstrates that an iteration of GenSA take a long time in comparison 
with the other algorithm. Moreover, `"BFGS"` and `"CG"` are still following a
similar curve under GenSA. Nelder-Mead, SANN and ipf contains iterations that 
take less time. Until, it appears that ipf and the best in terms of iterations
and the time needed for few iterations is good.

```{r, fig.cap="Relationship between processing time and goodness-of-fit between observed and simulated results for different optimisation algorithms", fig.width=4, fig.height=4, echo=FALSE}
# img <- readPNG("figures/TimeOptim_GenSA_Mo.png")
# grid.raster(img)
# # ![](figures/optim-time.png)
```

\begin{figure}
\includegraphics{figures/TimeOptim_GenSA_Mo.pdf}
\caption{Relationship between processing time and goodness-of-fit between observed and simulated results for different optimisation algorithms.}
\end{figure}

Nelder-Mead is fast at reaching a good
approximation of the constraint data, despite taking many iterations.
`GenSA`, on the other hand, is shown to be much slower than the others,
despite only requiring 2 iterations to arrive at a good level of fit.

Note that these results are biased by the example that is pretty small 
and runs only for the first zone.

### Combinatorial optimisation

Combinatorial optimisation is a complete field consisting in a different
method to result optimisation problem. Instead of having one candidate that
evolve through the iterations, combinatorial optimisation forms a set
of feasible candidates and then evaluate them thanks to the objective
function to be minimised. There are several types of combinatorial optimisation
depending on how we chose the combination of candidates.

This technique can be seen as an alternative to IPF for allocating individuals
to zones. This strategy is *probabilistic*
and results in integer weights (since it is a combination of individuals),
as opposed to the fractional weights generated by IPF. Combinatorial optimisation
may be more appropriate for applications where input individual microdatasets are 
very large: the speed benefits of using the deterministic IPF algorithm shrink as 
the size of the survey dataset increases. As seen before, IPF creates non integer 
weights, but we have proposed two solutions to transform them into the final 
individual-level population. So, the proportionality of IPF is more intuitive,
but need to calculate the whole weights matrix in each iteration, where CO
just proposes candidates. However, if the objective function takes a long time
to be calculated, CO will have to perform this evaluation for each candidate.

Genetic algorithms are included in this field and become popular in some 
domains, such as industry, for the moment. These kind of algorithms can be 
really effective when the objective function has several local minimum and
we want to find the global one. (Hermes and Poulsen, 2012)

There are two approaches for reweighting using combinatorial optimisation
in R: shuffling individuals in and out of each area and combinatorial optimisation,
the *domain* of the solution space set to allow inter-only results...

The second approach to combinatorial optimisation in R depends on methods
that allow only integer solutions to the general constrained optimisation
formulae demonstrated in the previous section. *Integer programming* is the
branch of computer science dedicated to this area, and it is associated with
its own algorithms and approaches, some of which have been implemented in R
(Zubizarreta, 2012).

To illustrate how the approach works in general terms, we can use the
`data.type.int` argument of the `genoud` function in the **rgenoud** package.
This ensures only integer results for
a genetic algorithm to select parameters are selected:

```{r, eval=FALSE}
# Set min and maximum values of constraints with 'Domains'
m <- matrix(c(0, 100), ncol = 2)[rep(1, nrow(ind)),]
set.seed(2014)
genoud(nrow(ind), fn = fun, ind_num = ind, con = cons[1,],
  control = list(maxit = 1000), data.type.int = TRUE, D = m)
```

This command, implemented in 'optim-tests-SimpleWorld.R',
results in weights for the unique individuals 1 to 4 of 1, 4, 2 and 4 respectively.
This means a final population with aggregated data equal to
(as seen in the previous section):


```{r, echo=FALSE}
umat_count <- function(x) {
 xp <- apply(x, 1, paste0, collapse = "") # "pasted" version of constraints
 freq <- table(xp) # frequency of occurence of each individual
 xu <- unique(x) # save only unique individuals
 rns <- as.integer(row.names(xu)) # save the row names of unique values of ind
 xpu <- xp[rns]
 o <- order(xpu, decreasing = TRUE) # the order of the output (to rectify table)
 cbind(xu, data.frame(ind_num = freq[o], rns = rns)) # outputs
}

umat <- umat_count(ind_cat)
indu <- apply(umat[1:ncol(ind_cat)], 2, function(x) x * umat$ind_num)
```


```{r}
colSums(indu * c(1, 4, 2, 4))
```

Note that we performed the test only for zone 1 and this aggregated
data are equally the same as the first constraint. Moreover, 
thanks to the fact that the algorithm directly consider only
integer weights, we do not have the issue of fractional weights
associated with IPF, 
which only generates perfect fit for non-integer weights.
Combinatorial optimisation algorithms for population
synthesis do not rely on integerisation,
which can damage model fit. The fact that the gradient contains "NA"
in the end of the algorithm is not a problem, because it just means
that they have not been calculated.

```{r, echo=FALSE}
# Not entirely sure what is meant by "the gradient contains "NA""
# Please explain! (RL)
```

Note that there can be several solutions which attain
a perfect fit. This result depends on the random seed chosen for
the random draw. Indeed, if we chose a seed of 0
(by writing `set.seed(0)`), as before, we obtain the weights 
`(0, 6, 4, 2)` which results also in a perfect fit for zone 1:

```{r}
colSums(indu * c(0 ,6 ,4 ,2))
```

These two potential synthetic populations reach a perfect fit, but 
are quite different. Indeed, we can observe the two populations:

```{r}
indu * c(1 ,4 ,2 ,4)
indu * c(0 ,6 ,4 ,2)
```

An example of comparison is that the second proposition contains 
no male being more than 50 years old, but the first one has 2.
With this method, there cannot be a population with 1 male of
over 50, because we take integer weights and there are two 
men in this category in the sample. This is the disadvantage of
algorithms reaching directly integer weights. With IPF, if the 
weights of this individual is between 0 and 1, there is a possibility 
to have a person in this category.

```{r, echo=FALSE}
# I really like this description (RL)
```

`genoud` is used here only
to provide a practical demonstration of the possibilities of
combinatorial optimisation using existing R packages.

For combinatorial optimisation algorithms designed for spatial microsimulation
we must, for now, look for programs outside the R 'ecosystem'.
Harland (2013) provides a practical tutorial
introducing the subject based on the Flexible Modelling Framework (FMF)
Java program.

