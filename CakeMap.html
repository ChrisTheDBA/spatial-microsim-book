<!DOCTYPE html>
<html>
  <head>
    <title>CakeMap &middot; Spatial Microsimulation with R.</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="www/bootstrap.min.css" rel="stylesheet">
    <link href="www/highlight.css" rel="stylesheet">

    <link href='http://fonts.googleapis.com/css?family=Inconsolata:400,700'
      rel='stylesheet' type='text/css'>
  </head>

  <body>

    <div class="container">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown">
              Table of contents<b class="caret"></b>
            </a>
            <ul class="dropdown-menu pull-right" role="menu">
              <li><a href="introduction.html">Introduction</a></li>
<li><a href="what-is-smsim.html">What is spatial microsimulation?</a></li>
<li><a href="SimpleWorld.html">SimpleWorld</a></li>
<li><a href="data-prep.html">Preparing input data</a></li>
<li><a href="smsim-in-R.html">Spatial microsimulation in R</a></li>
<li><a href="alternative-approaches.html">Spatial microsimulation in R</a></li>
<li><a href="no-microdata.html">Spatial microsimulation in R</a></li>
<li><a href="CakeMap.html">CakeMap</a></li>
<li><a href="validation.html">Model checking and evaluation</a></li>
<li><a href="visualising.html">Visualising spatial microdata</a></li>
<li><a href="smsim-for-abm.html">Spatial microsimulation for agent-based models</a></li>
<li><a href="additional.html">Additional tools and techniques</a></li>
<li><a href="appendix.html">Appendix</a></li>
<li><a href="glossary.html">Glossary</a></li>
<li><a href="references.html">References</a></li>

            </ul>
          </li>
        </ul>

        <h3 class="muted"><a href="http://robinlovelace.net/spatial-microsim-book/">Spatial Microsimulation with R</a> <small>by Robin Lovelace</small></h3>
        <hr>
      </div>

      <div class="row">
        <div class="col-sm-3" id="nav">
        <div class="well">
          Available soon as a physical book</a>.
        </div>

        <h4>Contents</h4>
          <ul class="list-unstyled" id="toc"></ul>

          <hr>
          <!--<p><a href="/contribute.html">How to contribute</a></p>-->

          <p><a class="btn btn-primary" href="https://github.com/RobinLovelace/spatial-microsim-book/edit/master/CakeMap.Rmd">Edit this page</a></p>
        </div>

        <div id="content" class="col-sm-8 pull-right">
          <h1 id="CakeMap">Spatial microsimulation in the wild</h1>
<p>By now we have developed a good understanding of what spatial microsimulation is, its applications and how it works. We have seen something of its underlying theory and its implementation in R. But how can the method be applied ‘in the wild’, on real datasets?</p>
<p>The purpose of this chapter is to answer this question using real data to estimate cake consumption in different parts of Leeds, UK. The example is deliberately rather absurd to make it more memorable. The steps are presented in a generalisable way, to be applicable to a wide range of datasets.</p>
<p>The input microdataset is a randomized (‘jumbled’) subset of the 2009 <a href="http://data.gov.uk/dataset/adult_dental_health_survey">Dental Health Survey</a>, (DHS) which covers England, Northern Ireland and Wales. 1173 variables are available in the DHS, many of which are potentially interesting target variables not available at the local level. These include weekly income, plaque buildup, and oral health behaviours. Potential linking variables include socio-economic classification, and dozens of variables related to oral health.</p>
<p>In terms of constraint variables, we are more limited: the Census is the only survey that provides count data at the small area level in the UK. Thus the ‘domain’ of available input data, related to our research question involves two sources:</p>
<ol style="list-style-type: decimal">
<li>Non-geographical individual-level survey, DHS — the <em>microdata</em>.</li>
<li>Geographically aggregated categorical count data from the census — the <em>constraint tables</em>.</li>
</ol>
<p>As discussed in Chapter must first decide which variables should be used to link the two. We must select the constraints from available linking variables.</p>
<h2>Selection of constraint variables</h2>
<p>The selection of linking variables should not be arbitrary pre-ordained by preconceptions. The decision of which constraints to use to allocate individuals to zones should be context dependent. If the research is on social exclusion, for example, many variables could potentially be of interest: car ownership, house tenancy, age, gender and religion could all affect the dependent variable. Often constraint variables must be decided not based on what would be ideal, but what datasets are available. The selection criteria will vary from one project to the next, but there are some overriding principles that apply to most projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>More the merrier</strong>: each additional constraint used for will further differentiate the spatial microdata from the input microdata. If gender is the only constraint used, for example, the spatial microdata will simply be a repetition of the input microdata but with small differences in the gender ratio from one zone to the next. If five constraints are used (e.g. age, gender, car ownership, tenancy and religion), the differences between the spatial microdata from one zone to the next will be much more pronounced and probably useful.</p></li>
<li><p><strong>Relevance to the target variable</strong>: often spatial microsimulation is used to generate local estimates of variables about which little geographically disaggregated information is available. Income is a common example: we have much information about income distributions, but little information about how average values (let alone the distribution) of income varies from one small area to the next. In this case income is the target variable. Therefore constraints must be selected which are closely related to income for the output to resemble reality. This is analogous to multiple regression (which can also be used to estimate average income at the local level), where the correct <em>explanatory variables</em> (i.e. constraint variables in spatial microsimulation) must be selected to effectively predict the <em>dependent variable</em>. As with regression models, there are techniques which can be used to identify the most suitable constraint variables for a given target variable.</p></li>
<li><p><strong>Simplicity</strong>: this criterion to some extent contradicts the first. Sometimes more constraints do not result in better spatial microdata and problems associated with ‘over-fitting’ can emerge. Spatial microsimulation models based on many tens of constraint categories will take longer to run and require more time to develop and modify. In addition, the chances of an error being introduced during every phase of the project is increased with each additional constraint. The extent to which increasing the number of constraint categories improves the results of spatial microsimulation, either with additional variables or by using cross-tabulated constraints (e.g. age/sex) instead of single-variable constraints, has yet to be explored. It is therefore difficult to provide general rules of thumb regarding simplicity other than ‘do not over-complicate the model with excessive constraint variables and constraints’.</p></li>
</ol>
<p>So, we always need to reach an equilibrium between these principles. Indeed, we have to take into account <em>enough</em> and <em>pertinent</em> variables, without making the population synthesis process too complex</p>
<p>To exemplify these principles, let us consider the constraint variables available in the CakeMap datasets. Clearly only variables available both in the individual-level and aggregate-level datasets can be chosen from. Suppose our aim is to analyze the consumption of cakes depending on sociodemographic variables. Five interesting variables assigned to each of the 916 individuals are available from the individual-level data :</p>
<ul>
<li>‘Car’: The number of working cars in the person’s household.</li>
<li>‘Sex’ and ‘ageband4’: Gender and age group, in two separate variables. Age is divided into 6 groups ranging from ‘16–24’ to ‘65–74’.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>‘NSSEC’: National Statistics Socio-economic Classification: an categorical variable classifying the individual’s work into one of 10 groups including ‘97’, which means ‘no answer’ (<code>NA</code>).</li>
<li>‘NCakes’: the target variable, reported number of times that the respondent consumes cake each week.</li>
</ul>
<p>All of these variables, except for ‘NCakes’, have a corresponding constraint variable to be loaded for the 124 Wards that constitute the Leeds Local Authority in the UK. In real datasets it is rarely the case that the categories of the individual and aggregate level data match perfectly from the outset. This is the first problem we must overcome before running a spatial microsimulation model of cake consumption in Leeds.</p>
<p>The code needed to run the main part of the example is contained within ‘CakeMap.R’. Note that this script makes frequent reference to files contained in the folder ‘data/CakeMap’, where input data and processing scripts for the project are stored.</p>
<h2 id="CakePrep">Preparing the input data</h2>
<p>Often spatial microsimulation is presented in a way that suggests the data arrived in a near perfect state, ready to be inserted directly into the model. This is rarely the case: usually, one must spend time loading the data into R, dealing with missing values, re-coding categorical variables and column names, binning continuous variables and subsetting from the microdataset. In a typical project, data preparation can take as long as the analysis stage. This section builds on Chapter 3 to illustrate strategies for data cleaning on a complex project. To learn about the data cleaning steps that may be useful to your data, we start from the beginning in this section, with a real (anonymised) dataset that was downloaded from the internet.</p>
<p>The raw constraint variables for CakeMap were downloaded from the Infuse website (<a href="http://infuse.mimas.ac.uk/">http://infuse.mimas.ac.uk/</a>). These, logically enough, are stored in the ‘cakeMap/data/’ directory as .csv files and contain the word ‘raw’ in the file name to identify the original data. The file ‘age-sex-raw.csv’, for example is the raw age and sex data that was downloaded. As the screenshot in Figure 6.1 illustrates, these datasets are rather verbose and require pre-processing. The resulting ‘clean’ constraints are saved in files such as ‘con1.csv’, which stands for ‘constraint 1’.</p>
<p><img src="figures/raw-data-screenshot.jpeg" alt="Example of raw aggregate-level input data for CakeMap aggregate data, downloaded from http://infuse.mimas.ac.uk/." /></p>
<p>To ensure reproducibility in the process of converting the raw data into a form ready for spatial microsimulation, all the steps have been saved. Take a look at the R script files ‘process-age.R’, ‘process-nssec.R’ and ‘process-car.R’. The contents of these scripts should provide an insight into methods for data preparation in R. Wickham (2014b) provides a more general introduction to data reformatting. The most difficult input dataset to deal with, in this example, is the age/sex constraint data. The steps used to clean it are saved in ‘process-age.R’, in the <code>data/CakeMap/</code> folder. Take a look through this file and try to work out what is going on: the critical stage is grouping single year age bands into larger groups such as 16–24.</p>
<p>The end result of ‘process-age.R’ is a ‘clean’ .csv file, ready to be loaded and used as the input into our spatial microsimulation model. Note that the last line of ‘process-age.R’ is <code>write.csv(con1, &quot;con1.csv&quot;, row.names = F)</code>. This is the first constraint that we load into R to reweight the individual-level data in the next section. The outputs from these data preparation steps are named ‘con1.csv’ to ‘con3.csv’. For simplicity, all these were merged (by ‘load-all.R’) into a single dataset called ‘cons.csv’. All the input data for this section are thus loaded with only two lines of code:</p>
<pre class="sourceCode r"><code class="sourceCode r">ind &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/CakeMap/ind.csv&quot;</span>)
cons &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/CakeMap/cons.csv&quot;</span>)</code></pre>
<p>Take a look at these input data using the techniques learned in the previous section. To test your understanding, try to answer the following questions:</p>
<ul>
<li>What are the constraint variables?</li>
<li>How many individuals are in the survey microdataset?</li>
<li>How many zones will we generate spatial microdata for?</li>
</ul>
<p>For bonus points that will test your R skills as well as your practical knowledge of spatial microsimulation, try constructing queries in R that will automatically answer these questions.</p>
<p>It is vital to understand the input datasets before trying to model them, so take some time exploring the input. Only when these datasets make sense (a pen and paper can help here, as well as R!) is it time to generate the spatial microdata.</p>
<p>As explained in the previous chapters, there are different methods to perform a spatial microsimulation. We will here use a reweighting method, since we have spatial aggregated data and non spatial individual data, both containing some common variables. In this category, the more often used and intuitiv method is the IPF.</p>
<p>We mentionned in the SimpleWorld example that there are two different points of view of IPF. First, weights are assigned to each individual for each zone (<code>ipfp</code> package). Second, weights are assigned to each possible category of individuals for each zone (<code>mipfp</code> package). The next sections developp the whole procedure with each package.</p>
<h2>Using the <code>ipfp</code> package</h2>
<h3 id="CakeIPF">Performing IPF on CakeMap data</h3>
<p>The <code>ipfp</code> reweighting strategy is concise, generalisable and computationally efficient. On a modern laptop, the <code>ipfp</code> method was found to be <em>almost 40 times faster</em> than the ‘IPFinR’ method (section 4.1; Lovelace, 2014) over 20 iterations on the CakeMap data, completing in 2 seconds instead of over 1 minute. This is a huge time saving!<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Thanks to the preparatory steps described above, the IPF stage can be run on a single line. After the datasets are loaded in the first half of ‘CakeMap.R’, the following code creates the weight matrix:</p>
<pre><code>#&gt; 
#&gt; Attaching package: &#39;dplyr&#39;
#&gt; 
#&gt; The following object is masked from &#39;package:stats&#39;:
#&gt; 
#&gt;     filter
#&gt; 
#&gt; The following objects are masked from &#39;package:base&#39;:
#&gt; 
#&gt;     intersect, setdiff, setequal, union</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">weights &lt;-<span class="st"> </span><span class="kw">apply</span>(cons, <span class="dv">1</span>, function(x)
  <span class="kw">ipfp</span>(x, ind_catt, x0, <span class="dt">maxit =</span> <span class="dv">20</span>))</code></pre>
<p>As with the SimpleWorld example, the correlation<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> between the constraint table and the aggregated results of the spatial microsimulation can be checked to ensure that the reweighting process has worked correctly. This demonstrates that the process has worked with an <em>r</em> value above 0.99:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="kw">as.numeric</span>(cons), <span class="kw">as.numeric</span>(ind_agg))
<span class="co">#&gt; [1] 0.9969</span></code></pre>
<p>This value is very close to 1, so we can consider that there is a big linear correlation between the result and the constraint. We can verify it by plotting:</p>
<p><img src="figures/unnamed-chunk-11-1.png" title="Scatter plot of the relationship between observed cell counts for all categories and all zones from the census (x axis) and the simulated cell counts after IPF (y axis)." alt="Scatter plot of the relationship between observed cell counts for all categories and all zones from the census (x axis) and the simulated cell counts after IPF (y axis)." width="384" /></p>
<p>With the best fit, we would have all constraints equal to the simulation, so a perfect line (this is why we consider a <code>linear</code> correlation). Only few points are outside the area of the line. Note that we have 1,623,797 inhabitants<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> in the simulation and in the constraints, <code>con1</code>and <code>con2</code> have 1,623,800, while <code>con3</code> contains 1,623,797. As seen before, with IPF, the result depends on the order of the constaints. Thus, it is logical to put as last constraint one really reliable, also in terms of total number of people. An alternative is to rescale all constraints to be consistence. This will be done in the next section, with <code>mipfp</code> and in the comparison for <code>ipfp</code>.</p>
<p>For the comparison category per category, we can take the absolute value of the differences of the two table. This gives a table of differences. The worst category and zone will be the maximum of this matrix.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Maximum error</span>
<span class="kw">max</span>(<span class="kw">abs</span>(ind_agg-cons))
<span class="co">#&gt; [1] 4960</span>

<span class="co"># Index of the maximum error</span>
<span class="kw">which</span>(<span class="kw">abs</span>(ind_agg-cons) ==<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(ind_agg-cons)), <span class="dt">arr.ind =</span> <span class="ot">TRUE</span>)
<span class="co">#&gt;      row col</span>
<span class="co">#&gt; [1,]  84  13</span>
<span class="co">#&gt; [2,]  84  14</span></code></pre>
<p>The maximum error is now know and corresponds to the zone 84 and 13 columns of the constraints, which is <code>Car</code>.</p>
<p>This is for the basic validation, a more detailed analysis of the quality of the results is present in the next chapter.</p>
<h3 id="CakeINT">Integerisation</h3>
<p>As before, weights of the IPF procedure are fractional, so must be <em>integerised</em> to create whole individuals. The code presented in chapter 4 requires little modification to do this: it is your task to convert the weight matrix generated by the above lines of code into a spatial microdataset called, as before, <code>ints_df</code> (hint: the <code>int_trs</code> function in ‘R/functions.R’ file will help). The spatial microdata generated in ‘R/CakeMapInts.R’ contains the same information as the individual-level dataset, but with the addition of the ‘zone’ variable, which specifies which zone each individual inhabits.</p>
<p>The spatial microdata is thus <em>multilevel</em> data, operating at one level on the level of individuals and at another at the level of zones. To generate summary statistics about the individuals in each zone, functions must be run on the data, one zone (group) at a time. <code>aggregate</code> provides one way of doing this. After converting the ‘NSSEC’ socio-economic class variable into a suitable numeric variable, <code>aggregate</code> can be used to identify the variability in social class in each zone. The <code>by =</code> argument is used to specify how the results are grouped depending on the zone each individual inhabits:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;R/CakeMapInts.R&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">aggregate</span>(ints_df$NSSEC, <span class="dt">by =</span> <span class="kw">list</span>(ints_df$zone), sd,
  <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code>##    Group.1        x
## 1         1 1.970570
## 2         2 2.027638
## 3         3 2.019839</code></pre>
<p>In the above code the third argument refers to the function to be applied to the input data. The fourth argument is simply an argument of this function, in this case instructing the standard deviation function (<code>sd</code>) to ignore all <code>NA</code> values. An alternative way to perform this operation, which is faster and more concise, is using <code>tapply</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tapply</span>(ints_df$NSSEC, ints_df$zone, sd, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre>
<p>Note that operations on <code>ints_df</code> can take a few seconds to complete. This is because the object is large, taking up much RAM on the computer. This can be seen by asking <code>object.size(ints_df)</code> or <code>nrow(ints_df)</code>. The latter shows we have created a spatial microdataset of 1.6 million individuals! Try comparing this result with the size of the original survey dataset ‘ind’. Keeping an eye on such parameters will ensure that the model does not generate datasets too large to handle.</p>
<p>Next chapter, we move on to a vital consideration in spatial microsimulation models such as CakeMap: model checking and validation. First, we developp the spatial microsimulation using <code>mipfp</code>.</p>
<h2>Using the <code>mipfp</code> package</h2>
<h3 id="CakeIPF">Performing IPF on CakeMap data</h3>
<p>As in the previous section, the first stage is to load the data. An additional step required by the <code>Ipfp</code> function ensuring the categories of <code>cons</code> and <code>ind</code> correspond. In the variable associated with car ownership, <code>ind</code> contains “1” and “2”, but <code>cons</code> contains “Car” and “NoCar”.</p>
<p>To undertake this operation, we convert the category labels in the individual-level variable <code>ind</code> so that they conform to the variable names of the count data contained in constraint data <code>cons</code>. Continuing with the car ownership example, every “1” in <code>ind$Car</code> should be converted to “Car” and every “2” to “NoCar”. Likewise, the correct labels of “f” and “m” must be created for the Sex variable and likewise for the other variables in <code>ind</code>. The code that performs this operation is included in the R file <code>CakeMapMipfpData.R</code> file. Take a look at this file (e.g. by typing <code>file.edit(&quot;R/CakeMapMipfpData.R&quot;)</code> from RStudio) and note the use of the <code>switch</code> function in combination with the recursive function <code>sapply</code> to <em>switch</em> the values of the individual-level variables. The entire script is run below using the <code>source</code> command.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;R/CakeMapMipfpData.R&quot;</span>)</code></pre>
<p>The data are now loaded correctly. This can be verified by printing the first lines of of the individual-level data and comparing this with <code>colnames(con1)</code>. Note that this step was not necessary for the <strong>ipfp</strong> package. To use **mipfp* the user must encode the constraints in the right order and only the order. In practice this is a good idea. Ensuring same variable names match category labels will help subsequent steps of for validation and interpretation. Moreover, the structure of some constraints will have to change and this process will be easier if the names are the same. To re-affirm our starting point, a sample of the new individual-level data is displayed below.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(ind)
<span class="co">#&gt;   NCakes   Car Sex NSSEC8 ageband4</span>
<span class="co">#&gt; 1    3-5 NoCar   m     X7    25_34</span>
<span class="co">#&gt; 2    1-2 NoCar   f     X2    55_64</span>
<span class="co">#&gt; 3    1-2   Car   f     X2    45_54</span>
<span class="co">#&gt; 4     6+   Car   m     X5    45_54</span>
<span class="co">#&gt; 5    1-2 NoCar   m     X2    45_54</span>
<span class="co">#&gt; 6    1-2   Car   f     X2    45_54</span></code></pre>
<p>Defining the initial weight matrix is the next step. For each zone, we simply take the cross table (or contingency table) of the individual data. It is important to check the order of the variables inside this table. We have in our case (NCakes, Car, Sex, NSSEC8, ageband4). This information will be necessary for the definition of the target and description for <code>mipfp</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Initial weight matrix for each zone</span>
weight_init_1zone &lt;-<span class="st"> </span><span class="kw">table</span>(ind)

<span class="co"># Check order of the variables</span>
<span class="kw">dimnames</span>(weight_init_1zone) 
<span class="co">#&gt; $NCakes</span>
<span class="co">#&gt; [1] &quot;&lt;1&quot;     &quot;1-2&quot;    &quot;3-5&quot;    &quot;6+&quot;     &quot;rarely&quot;</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $Car</span>
<span class="co">#&gt; [1] &quot;Car&quot;   &quot;NoCar&quot;</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $Sex</span>
<span class="co">#&gt; [1] &quot;f&quot; &quot;m&quot;</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $NSSEC8</span>
<span class="co">#&gt;  [1] &quot;X1.1&quot;  &quot;X1.2&quot;  &quot;X2&quot;    &quot;X3&quot;    &quot;X4&quot;    &quot;X5&quot;    &quot;X6&quot;    &quot;X7&quot;   </span>
<span class="co">#&gt;  [9] &quot;X8&quot;    &quot;Other&quot;</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $ageband4</span>
<span class="co">#&gt; [1] &quot;16_24&quot; &quot;25_34&quot; &quot;35_44&quot; &quot;45_54&quot; &quot;55_64&quot; &quot;65_74&quot;</span></code></pre>
<p>To use the full power of the <code>mipfp</code> package, we choose to add the spatial dimension into the weight matrix instead of performing a <code>for</code> loop over the zones. For this purpose, we repeat the table for each area. We define the names of the new dimension and create an array with the correct cells and names.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Adding the spatial dimension</span>
<span class="co"># Repeat the initial matrix n_zone times</span>
init_cells &lt;-<span class="st"> </span><span class="kw">rep</span>(weight_init_1zone, <span class="dt">each =</span> <span class="kw">nrow</span>(cons))

<span class="co"># Define the names</span>
names &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">list</span>(<span class="kw">rownames</span>(cons)), <span class="kw">as.list</span>(<span class="kw">dimnames</span>(weight_init_1zone)))

<span class="co"># Structure the data</span>
weight_init &lt;-<span class="st"> </span><span class="kw">array</span>(init_cells, <span class="dt">dim =</span> 
                <span class="kw">c</span>(<span class="kw">nrow</span>(cons), <span class="kw">dim</span>(weight_init_1zone)),
                <span class="dt">dimnames =</span> names)</code></pre>
<pre><code>#&gt; Loading required package: cmm
#&gt; Loading required package: Rsolnp
#&gt; Loading required package: truncnorm
#&gt; Loading required package: parallel
#&gt; Loading required package: numDeriv</code></pre>
<p>An advantage of <strong>mipfp</strong> over <strong>ipfp</strong> is that the algorithm first checks the consistency of the constraints, before reweighting. If the total number of individuals in a zone is not exactly the same from one constraint to another, <code>mipfp</code> will return the following warning: ‘Target not consistents - shifting to probabilities!’ In this case, the cells of the resulting table become probabilities, such as the sum of all cells is 1. This means that the algorithm does not know how many people per zone you want. This is a very useful feature of <strong>mipfp</strong>: creating weights corresponding to the correct total population is as simple as multiply the probabilities by this number.</p>
<p>An alternative to this is to verify by your own the totals per zone and re-scale the constraints that are different from your purpose population. In our case, for each constraint, we calculate the number of persons in the zone. For each zone, we compare the totals of the different constraints. The result is <code>TRUE</code> if the totals are the same and <code>FALSE</code> otherwise. It gives <code>nrow(cons)</code> boolean values (TRUE or FALSE), of which we print the table.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Check the constraint&#39;s totals per zone</span>
<span class="kw">table</span>(<span class="kw">rowSums</span>(con2) ==<span class="st"> </span><span class="kw">rowSums</span>(con1))
<span class="co">#&gt; </span>
<span class="co">#&gt; TRUE </span>
<span class="co">#&gt;  124</span>
<span class="kw">table</span>(<span class="kw">rowSums</span>(con3) ==<span class="st"> </span><span class="kw">rowSums</span>(con1))
<span class="co">#&gt; </span>
<span class="co">#&gt; FALSE  TRUE </span>
<span class="co">#&gt;    72    52</span>
<span class="kw">table</span>(<span class="kw">rowSums</span>(con2) ==<span class="st"> </span><span class="kw">rowSums</span>(con3))
<span class="co">#&gt; </span>
<span class="co">#&gt; FALSE  TRUE </span>
<span class="co">#&gt;    72    52</span></code></pre>
<p>Constraints 1 and 2 have exactly the same marginals per zone. Constraint 3 has 52 zones with different totals. Since NSSEC (<code>con3</code>) is anomolous in this respect we consider the totals of the two first constraints to be valid. To perform (non-probabilistic) reweighting with <strong>mipfp</strong>, we must first re-scale <code>con3</code> to keep its proportions but with updated totals per zone.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Re-scale the constraint 3</span>
con3_prop &lt;-<span class="st"> </span>con3 *<span class="st"> </span><span class="kw">rowSums</span>(con2) /<span class="st"> </span><span class="kw">rowSums</span>(con3)

<span class="co"># Check the new totals</span>
<span class="kw">table</span>(<span class="kw">rowSums</span>(con2) ==<span class="st"> </span><span class="kw">rowSums</span>(con3_prop))
<span class="co">#&gt; </span>
<span class="co">#&gt; TRUE </span>
<span class="co">#&gt;  124</span></code></pre>
<p>The rescaling operation above solved one problem, but there is another, more difficult-to-solve issue with the input data. The second and third constraints are simple marginals per zone with a single, clearly-defined variable each. The first constraint (<code>con1</code>), by contrast, is cross-tabulated, a combination of two variables (sex and age, or age/sex as cross-tabulations are often referred to as). We must change structure of the first constraint to execute <code>mipfp</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># View the content of the first constraint</span>
con1[<span class="dv">1</span>:<span class="dv">3</span>, <span class="dv">1</span>:<span class="dv">8</span>]
<span class="co">#&gt;   m16_24 m25_34 m35_44 m45_54 m55_64 m65_74 f16_24 f25_34</span>
<span class="co">#&gt; 1    671    771   1033   1160   1165    772    679    760</span>
<span class="co">#&gt; 2    887   1254   1217   1344   1229    752    832   1169</span>
<span class="co">#&gt; 3    883    864   1195   1382   1170    878    811    962</span></code></pre>
<p>The result of the above command show that <code>con1</code> is, like the other constraints, a two dimensional table. Because it actually contains 2 constraints, the information contained within needs to be represented as a three dimensional array (a cube), the dimensions of which are zone, sex and age. This conversion is done by taking taking the cells of <code>con1</code> and adding them to a three dimensional array in a known order. The code to do this is included into the file <code>R/CakeMapMipfpCon1Convert.R</code> and the resulting array is called <code>con1_convert</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;R/CakeMapMipfpCon1Convert.R&quot;</span>)</code></pre>
<p>All constraints being converted and all the margins being re-scaled, the target and its ‘description’ are ready to be used in <code>Ipfp()</code>. The target is the set of constraints and the description is the figures of the corresponding dimensions into the weight matrix. The order of the weight’s variables is here important. Remember this order is (Zone, NCakes, Car, Sex, NSSEC8, ageband4). For this reason, the constraint 1, containing (zone, Sex, ageband4) has the description <code>c(1,4,6)</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Definition of the target and descript</span>
<span class="co"># For the execution of the mipfp</span>
target &lt;-<span class="st"> </span><span class="kw">list</span>(con1_convert,
               <span class="kw">as.matrix</span>(con2),
               <span class="kw">as.matrix</span>(con3_prop))

descript &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">6</span>), <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>))</code></pre>
<p>The remaining step involves in executing the <code>Ipfp</code> function on these data.</p>
<pre class="sourceCode r"><code class="sourceCode r">weight_mipfp &lt;-<span class="st"> </span><span class="kw">Ipfp</span>(weight_init, descript, target)</code></pre>
<p>Note that <code>Ipfp</code> does not print the <code>tol</code> result by default. This was a wise decision by the developer of <strong>mipfp</strong> in this case: more than 100 iterations were needed for the weights to converge.</p>
<p><img src="figures/unnamed-chunk-31-1.png" title="Scatter plot of the relationship between observed cell counts for all categories and all zones from the census (x axis) and the simulated cell counts after IPF (y axis)." alt="Scatter plot of the relationship between observed cell counts for all categories and all zones from the census (x axis) and the simulated cell counts after IPF (y axis)." width="384" /></p>
<p>The third constraint is perfectly fitted. The first and second constraints are globally well fitted, but some points, representing a category and a zone, are not so good. Analysis of quality of fit are provided in the next chapter. In the next section we focus on comparing <strong>ipfp</strong> and <strong>mipfp</strong> for real-world microsimulation applications involving large datasets, such as CakeMap.</p>
<h2>Comparing methods of reweighting large datasets</h2>
<p>We mentionned in the <strong>ipfp</strong> part of this chapter that the three constraints do not contain the same number of individuals. To compare both methods, we will run a second time <code>ipfp</code>, but with the rescaled constraints. Note that in case of non-consistent constraints, <code>mipfp</code> warns the user, but <code>ipfp</code> does not. Thus, with <code>ipfp</code>, it is recommended that the user checks constaint totals before reweighting. Note that instead of number of iterations, we chose to fix the tolerance threshold to <span class="math">10<sup> − 10</sup></span>, the default tolerance argument used in the <code>mipfp</code> function (remember, such details can be verified by typing <code>?mipfp</code>).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Re-running ipfp with the rescaled constraints</span>
cons_prop &lt;-<span class="st"> </span><span class="kw">cbind</span>(con1, con2, con3_prop)
weights &lt;-<span class="st"> </span><span class="kw">apply</span>(cons_prop, <span class="dv">1</span>, function(x) <span class="kw">ipfp</span>(x, ind_catt, x0, <span class="dt">tol =</span> <span class="fl">1e-10</span>))

<span class="co"># Convert back to aggregates</span>
ind_agg &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(weights, <span class="dv">2</span>, function(x) <span class="kw">colSums</span>(x *<span class="st"> </span>ind_cat)))</code></pre>
<p>This section is divided in two parts. One that analyses the distance between the results generated by the two different packages and the other that focus on the time differences.</p>
<h3>Comparison of results</h3>
<p>Both results being based on the rescaled data, we can compare them. The structure of both resulting tables are different, however. To compare them easily, we transform the <code>ipfp</code> final matrix into an array with the same structure as the one of the result of <code>mipfp</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;R/ConvertIpfpWeights.R&quot;</span>)</code></pre>
<p>The total number of people per zone is exactly the same in both weights matrices (maximum of differences being of order <span class="math">10<sup> − 10</sup></span>). The size of the population are thus in both cases the one desired.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Comparison of the totals per zone</span>
<span class="kw">max</span>( <span class="kw">apply</span>(weight_mipfp$x.hat, <span class="dv">1</span>, sum) 
       -<span class="st"> </span><span class="kw">apply</span>(weight_ipfp, <span class="dv">1</span>, sum) )
<span class="co">#&gt; [1] 0</span></code></pre>
<p>To visualize all the weights, we plot the different counts per constraint. We observe that the constraint 3 is very well fitted with both methods. For the two other constraints, some categories in some zones are more apart. However, both methods seems to return very similar weights, since the crosses are included inside the circles.</p>
<p><img src="figures/unnamed-chunk-35-1.png" title="Scatter plot of the relationship between observed cell counts for all categories and all zones from the census (x axis) and the simulated cell counts after IPF (y axis)." alt="Scatter plot of the relationship between observed cell counts for all categories and all zones from the census (x axis) and the simulated cell counts after IPF (y axis)." width="384" /></p>
<p>By analysing the absolute differences between the two resulting weights, we discover that the largest absolute distance is 3.45608e-11.</p>
<p>We can consider that both methods give very similar results. However, there are both distant from the constraints for few zones and categories. A longer analyzis of this is done in the next chapter.</p>
<h3>Comparison of times</h3>
<p>The weights produced by <strong>ipfp</strong> and <strong>mipfp</strong> packages are the same. Thus, we must use other criteria to assess which is more appropriate in different settings. For applications involving very large microdatasets and numbers of zones, it speed of execution may be an important criterion. For this reason, the time is an important characteristics to compare the packages. However, other considerations such as flexibility and user-friendliness should also be taken into account. The question of ‘scalability’ (how large can the simulation be before the software crashes) is also important.</p>
<p>When reweighting individuals for the of CakeMap example, <code>mipfp</code> was slower than <code>ipfp</code>. However, when accounting for wider considerations the speed advantage of <code>ipfp</code> is reduced. Tests show that as the size of input microdata increases, any performance advantages of <code>ipfp</code> declines. This phenomena should not appear in the <code>mipfp</code> package. To demonstrate this we will run the CakeMap example with variable numbers of individuals in the microdata. To do this, <code>ind</code> objects with more than 916 persons in the data are created, by copying the database a second time. The code used to create new <code>ind</code> objects can be seen by entering <code>file.edit(&quot;R/CakeMapTimeAnalysis.R&quot;)</code> in RStudio from inside the ‘spatial-microsimulation-book’ project.</p>
<p><img src="figures/unnamed-chunk-36-1.png" title="Time necessary to perform the generation of the weight matrix depending on the number of individuals inside the microdata." alt="Time necessary to perform the generation of the weight matrix depending on the number of individuals inside the microdata." width="384" /></p>
<p>Figure x confirms that for problems with few individuals in the microdata, <code>ipfp</code> is better. However, having few people means the increase of the probability to have non represented categories. Of course, we can optimise <code>ipfp</code> to take only one time the individuals that appear several times and add a count in the <code>ind_cat</code> instead of a “1”. However, this process is possible only if the added persons are with the same characteristics as a previous one.</p>
<p>In conclusion, <code>mipfp</code> is more flexible and could be use without microdata, but <code>ipfp</code> is faster for problems with less individuals in the microdata.</p>
<p>These results suggest that to perform population synthesis for spatial microspatial simulation in contexts where the input microdataset is large, one should use <code>mipfp</code>. Note that when having very few microdata, it may be advantageous to consider a method without microdata. This way one avoids biasing the results with the (non-random) category included in the microdata. The extent to which the input microdata represent the study area is critical to obtaining valid results.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>R tip: This information can be seen, once the dataset is loaded, by entering <code>unique(ind$ageband4)</code> or, to see the counts in each category, <code>summary(ind$ageband4)</code>. Because the variable is of type <code>factor</code>, <code>levels(ind$ageband4)</code> will also provide this information.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>These tests were conducted using the <code>microbenchmark()</code> commands found towards the end of the ‘CakeMap.R’ file. The second of these benchmarks depends on files from <code>smsim-course</code> (Lovelace, 2014), the repository of which can be downloaded from (<a href="https://github.com/Robinlovelace/smsim-course">https://github.com/Robinlovelace/smsim-course</a>).<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The function “cor” of R calculate the correlation coefficient of Pearson that measure the force of a linear correlation between the two variables. Included between -1 and 1, the best value in our case is 1.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>To obtain it, code <code>sum(ind_agg)/3</code>, because you have 3 constraints.<a href="#fnref4">↩</a></p></li>
</ol>
</div>

        </div>
      </div>

      <div class="footer">
        <hr>
        <p>&copy; Site design: Hadley Wickham. Powered by <a href="http://jekyllrb.com/">jekyll</a>,
          <a href="http://yihui.name/knitr/">knitr</a>, and
          <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>. Source
          available on <a href="https://github.com/robinlovelace/spatial-microsim-book/">github</a>.
        </p>
      </div>

    </div> <!-- /container -->

  <script src="//code.jquery.com/jquery.js"></script>
  <script src="www/bootstrap.min.js"></script>
  <script src="www/toc.js"></script>
  </body>
</html>
