<!DOCTYPE html>
<html>
  <head>
    <title>CakeMap &middot; Spatial Microsimulation with R.</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="www/bootstrap.min.css" rel="stylesheet">
    <link href="www/highlight.css" rel="stylesheet">

    <link href='http://fonts.googleapis.com/css?family=Inconsolata:400,700'
      rel='stylesheet' type='text/css'>
  </head>

  <body>

    <div class="container">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown">
              Table of contents<b class="caret"></b>
            </a>
            <ul class="dropdown-menu pull-right" role="menu">
              <li><a href="introduction.html">Introduction</a></li>
<li><a href="what-is-smsim.html">What is spatial microsimulation?</a></li>
<li><a href="SimpleWorld.html">SimpleWorld</a></li>
<li><a href="data-prep.html">Preparing input data</a></li>
<li><a href="smsim-in-R.html">Spatial microsimulation in R</a></li>
<li><a href="CakeMap.html">CakeMap</a></li>
<li><a href="smsim-for-abm.html">Spatial microsimulation for agent-based models</a></li>
<li><a href="additional.html">Additional tools and techniques</a></li>
<li><a href="appendix.html">Appendix</a></li>
<li><a href="glossary.html">Glossary</a></li>
<li><a href="references.html">References</a></li>

            </ul>
          </li>
        </ul>

        <h3 class="muted"><a href="http://robinlovelace.net/spatial-microsim-book/">Spatial Microsimulation with R</a> <small>by Robin Lovelace</small></h3>
        <hr>
      </div>

      <div class="row">
        <div class="col-sm-3" id="nav">
        <div class="well">
          Comming soon as a physical book</a>.
        </div>

        <h4>Contents</h4>
          <ul class="list-unstyled" id="toc"></ul>

          <hr>
          <!--<p><a href="/contribute.html">How to contribute</a></p>-->

          <p><a class="btn btn-primary" href="https://github.com/RobinLovelace/spatial-microsim-book/edit/master/CakeMap.Rmd">Edit this page</a></p>
        </div>

        <div id="content" class="col-sm-8 pull-right">
          <h1 id="CakeMap">Spatial microsimulation in the wild</h1>
<p>By now we have developed a good understanding of what spatial microsimulation is, its applications and how it works. We have seen something of its underlying theory and its implementation in R. But how can the method can be applied ‘in the wild’, on real datasets?</p>
<p>The purpose of this chapter is to answer this question using real data to estimate cake consumption in different parts of Leeds, UK. The example is deliberately rather absurd to make it more memorable. The steps are presented in a generalisable way, to be applicable to a wide range of datasets.</p>
<p>The input microdataset is a randomised (‘jumbled’) subset of the 2009 <a href="http://data.gov.uk/dataset/adult_dental_health_survey">Dental Health Survey</a>, (DHS) which covers England, Northern Ireland and Wales. xx variables are available in the DHS including xx, xx and xxx. In terms of constraint variables, we are more limited: the Census is the only survey that provides count data at the small area level. From this ‘domain’ of available input data, with the non-geographical individual-level DHS on one side — the <em>microdata</em> — and the geographically aggregated categorical count data from the census — the <em>constraint tables</em> — we must first decide which variables should be used to link the two. We must select the ‘linking variables’, otherwise know as constraint variables.</p>
<h2>Selection of constraint variables</h2>
<p>The selection of linking variables should not be arbitrary pre-ordained by preconceptions. The decision of which constraints to use to allocate individuals to zones should be context dependent: if the research is on social exclusion, for example, many variables could potentially be of interest, ranging from car ownership and house tenancy though to age, gender and religion. Often constraint variables must be decided not based on what would would be ideal, but what datasets are available. The selection criteria will vary from one project to the next, but there are some overriding principles that apply to most projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>More the merrier</strong>: each additional constraint used for will further differentiate the spatial microdata from the input microdata. If gender is the only constraint used, for example, the spatial microdata will simply be a repetition of the input microdata but with small differences in the gender ratio from one zone to the next. If five constraints are used (e.g. age, gender, car ownership, tenancy and religion), the differences between the spatial microdata from one zone to the next will be much more pronounced and probably useful.</p></li>
<li><p><strong>Relevance to the target variable</strong>: often spatial microsimulation is used to generate local estimates of variables about which little geographically disaggregated information is available. Income is a common example: we have much information about income distributions, but little information about how average values (let alone the distribution) of income varies from one small area to the next. In this case income is the target variable. Therefore constraints must be selected which are closely related to income for the output to resemble reality. This is analogous to multiple regression (which can also be used to estimate average income at the local level), where the correct <em>explanatory variables</em> (i.e. constraint variables in spatial microsimulation) must be selected to effectively predict the <em>dependent variable</em>. As with regression models, there are techniques which can be used to identify the most suitable constraint variables for a given target variable.</p></li>
<li><p><strong>Simplicity</strong>: this criterion to some extent contradicts the first. Sometimes more constraints do not result in better spatial microdata and problems associated with ‘over-fitting’ can emerge. Spatial microsimulation models based on many tens of constraint categories will take longer to run and require more time to develop and modify. In addition, the chances of an error being introduced during every phase of the project is increased with each additional constraint. The extent to which increasing the number of constraint categories improves the results of spatial microsimulation, either with additional variables or by using cross-tabulated constraints (e.g. age/sex) instead of single-variable constraints, has yet to be explored. It is therefore difficult to provide general rules of thumb regarding simplicity other than ‘do not overcomplicate the model with excessive constraint variables and constraints’.</p></li>
</ol>
<p>To exemplify these principles, let us consider the constraint variables available in the CakeMap datasets. Clearly only variables available both in the individual-level and aggreate datasets can be chosen from. Five variables assigned to each of the 916 individuals are available from the individual-level data, about which data is also available from the census:</p>
<ul>
<li>‘Car’: The number of working cars in the person’s household.</li>
<li>‘Sex’ and ‘ageband4’: Gender and age group, in two separate variables. Age is divided into 6 groups ranging from ‘16–24’ to ‘65–74’.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>‘NSSEC’: National Statistics Socio-economic Classification: an categorical variable classifying the individual’s work into one of 10 groups including ‘97’, which means ‘no answer’ (<code>NA</code>).</li>
<li>‘NCakes’: the target variable, reported number of times that the respondent consumes cake each week.</li>
</ul>
<p>All of these variables, except for ‘NCakes’, have a corresponding constraint variable to be loaded for the 124 Wards that constitute the Leeds Local Authority in the UK. In real datasets it is rarely the case that the categories of the individual and aggregate level data match perfectly from the outset and this is the first problem we must overcome before running a spatial microsimulation model of cake consumption in Leeds.</p>
<p>The code needed to run the main part of the example is contained within ‘CakeMap.R’. Note that this script makes frequent reference to files contained in the folder ‘data/CakeMap’, where input data and processing scripts for the project are stored.</p>
<h2 id="CakePrep">Preparing the input data</h2>
<p>Often spatial microsimulation is presented in a way that suggests the data arrived in a near perfect state, ready to be inserted directly into the model. This is rarely the case: usually, one must spend time loading the data into R, re-coding categorical variables and column names, binning continuous variables and subsetting from the microdataset. In a typical project, data preparation can take as long as the analysis stage. This section builds on Chapter 3 to illustrate strategies for data cleaning on a complex project. To learn about the data cleaning steps that may be useful to your data, we start from the beginning in this section, with a real (anonymised) dataset that was downloaded from the internet.</p>
<p>The raw constraint variables for CakeMap were downloaded from the Infuse website (<a href="http://infuse.mimas.ac.uk/">http://infuse.mimas.ac.uk/</a>). These, logically enough, are stored in the ‘cakeMap/data/’ directory as .csv files and contain the word ‘raw’ in the file name to identify the original data. The file ‘age-sex-raw.csv’, for example is the raw age and sex data that was downloaded. As the screenshot in Figure 3 illustrates, these datasets are rather verbose and require pre-processing. The resulting ‘clean’ constraints are saved in files such as ‘con1.csv’, which stands for ‘constraint 1’.</p>
<p><img src="figures/raw-data-screenshot.jpeg" alt="Example of raw aggregate-level input data for CakeMap aggregate data, downloaded from http://infuse.mimas.ac.uk/." /></p>
<p>To ensure reproducibility in the process of converting the raw data into a form ready for spatial microsimulation, all of the steps have been saved. Take a look at the R script files ‘process-age.R’, ‘process-nssec.R’ and ‘process-car.R’. The contents of these scripts should provide an insight into methods for data preparation in R. Wickham (2014b) provides a more general introduction to data reformatting. The most difficult input dataset to deal with is the age/sex constraint data. The steps used to clean it are saved in ‘process-age.R’, in the <code>data/CakeMap/</code> folder. Take a look through this file and try to work out what is going on: the critical stage is grouping single year age bands into larger groups such as 16–24.</p>
<p>The end result of ‘process-age.R’ is a ‘clean’ .csv file, ready to be loaded and used as the input into our spatial microsimulation model. Note that the last line of ‘process-age.R’ is <code>write.csv(con1, &quot;con1.csv&quot;, row.names = F)</code>. This is the first constraint that we load into R to reweight the individual-level data in the next section. The outputs from these data preparation steps are named ‘con1.csv’ to ‘con3.csv’. For simplicity, all these were merged (by ‘load-all.R’) into a single dataset called ‘cons.csv’. All the input data for this section are thus loaded with only two lines of code:</p>
<pre class="sourceCode r"><code class="sourceCode r">ind &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/CakeMap/ind.csv&quot;</span>)
cons &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/CakeMap/cons.csv&quot;</span>)</code></pre>
<p>Take a look at these input data using the techniques learned in the previous section. To test your understanding, try to answer the following questions:</p>
<ul>
<li>What are the constraint variables?</li>
<li>How many individuals are in the survey microdataset?</li>
<li>How many zones will we generate spatial microdata for?</li>
</ul>
<p>For bonus points that will test your R skills as well as your practical knowledge of spatial microsimulation, try constructing queries in R that will automatically answer these questions.</p>
<p>It is vital to understand the input datasets before trying to model them, so take some time exploring the input. Only when these datasets make sense (a pen and paper can help here, as well as R!) is it time to generate the spatial microdata.</p>
<h2>Performing IPF on CakeMap data {CakeIPF}</h2>
<p>The <code>ipfp</code> reweighting strategy is concise, generalisable and computationally efficient. On a modern laptop, the <code>ipfp</code> method was found to be <em>almost 40 times faster</em> than the ‘IPFinR’ method (section 4.1; Lovelace, 2014) over 20 iterations on the CakeMap data, completing in 2 seconds instead of over 1 minute. This is a huge time saving!^[These tests were conducted using the <code>microbenchmark()</code> commands found towards the end of the ‘CakeMap.R’ file. The second of these benchmarks depends on files from <code>smsim-course</code> (Lovelace, 2014), the repository of which can be downloaded from (<a href="https://github.com/Robinlovelace/smsim-course">https://github.com/Robinlovelace/smsim-course</a>).</p>
<p>Thanks to the preparatory steps described above, the IPF stage can be run on a single line. After the datasets are loaded in the first half of ‘CakeMap.R’, the following code creates the weight matrix:</p>
<pre><code>#&gt; 
#&gt; Attaching package: &#39;dplyr&#39;
#&gt; 
#&gt; The following object is masked from &#39;package:stats&#39;:
#&gt; 
#&gt;     filter
#&gt; 
#&gt; The following objects are masked from &#39;package:base&#39;:
#&gt; 
#&gt;     intersect, setdiff, setequal, union</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">weights &lt;-<span class="st"> </span><span class="kw">apply</span>(cons, <span class="dv">1</span>, function(x)
  <span class="kw">ipfp</span>(x, ind_catt, x0, <span class="dt">maxit =</span> <span class="dv">20</span>))</code></pre>
<p>As with the SimpleWorld example, the correlation between the constraint table and the aggregated results of the spatial microsimulation can be checked to ensure that the reweighting process has worked correctly. This demonstrates that the process has worked with an <em>r</em> value above 0.99:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="kw">as.numeric</span>(cons), <span class="kw">as.numeric</span>(ind_agg))
<span class="co">#&gt; [1] 0.9968529</span></code></pre>
<h2>Integerisation {CakeINT}</h2>
<p>As before, weights of the IPF procedure are fractional, so must be <em>integerised</em> to create whole individuals. The code presented in chapter 4 requires little modification to do this: it is your task to convert the weight matrix generated by the above lines of code into a spatial microdataset called, as before, <code>ints_df</code> (hint: the <code>int_trs</code> function in ‘R/functions.R’ file will help). The spatial microdata generated in ‘R/CakeMapInts.R’ contain the same information as the individual-level dataset, but with the addition of the ‘zone’ variable, which specifies which zone each individual inhabits.</p>
<p>The spatial microdata is thus <em>multilevel</em> data, operating at one level on the level of individuals and at another at the level of zones. To generate summary statistics about the individuals in each zone, functions must be run on the data, one zone (group) at a time. <code>aggregate</code> provides one way of doing this. After converting the ‘NSSEC’ socio-economic class variable into a suitable numeric variable, <code>aggregate</code> can be used to identify the variability in social class in each zone, using the <code>by =</code> argument to specify how the results are grouped depending on the zone each individual inhabits:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;R/CakeMapInts.R&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">aggregate</span>(ints_df$NSSEC, <span class="dt">by =</span> <span class="kw">list</span>(ints_df$zone), sd,
  <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code>##    Group.1        x
## 1         1 1.970570
## 2         2 2.027638
## 3         3 2.019839</code></pre>
<p>In the above code the third argument refers to the function to be applied to the input data. The fourth argument is simply an argument of this function, in this case instructing the standard deviation function (<code>sd</code>) to ignore all <code>NA</code> values. An alternative way to perform this operation, which is faster and more concise, is using <code>tapply</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tapply</span>(ints_df$NSSEC, ints_df$zone, sd, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre>
<p>Note that operations on <code>ints_df</code> can take a few seconds to complete. This is because the object is large, taking up much RAM on the computer. This can be seen by asking <code>object.size(ints_df)</code> or <code>nrow(ints_df)</code>. The latter shows we have created a spatial microdataset of 1.6 million individuals! Try comparing this result with the size of the original survey dataset ‘ind’. Keeping an eye on such parameters will ensure that the model does not generate datasets too large to handle.</p>
<p>Next we move on to a vital consideration in spatial microsimulation models such as CakeMap: validation.</p>
<h2 id="svalidation">Model checking and validation</h2>
<p>To make an analogy with food safety standards, openness about mistakes is conducive to high standards. Transparency in model verification is desirable for similar reasons. The two main strategies are:</p>
<ol style="list-style-type: decimal">
<li>comparing the model results with knowledge of how it <em>should</em> perform a-priori (model checking), and</li>
<li>comparison between the model results and empirical data (validation).</li>
</ol>
<p>Pearson’s coefficient of correlation (<span class="math"><em>r</em></span>) provides a fast and simple insight into the fit between the simulated data and the constraints. In most cases <span class="math"><em>r</em></span> values greater than 0.9 should be sought in spatial microsimulation and in many cases <span class="math"><em>r</em></span> values exceeding 0.99 are possible, even after integerisation. As a very basic test, let’s observe the correlation between the constraint table cells and the corresponding simulated cell values:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="kw">as.numeric</span>(cons), <span class="kw">as.numeric</span>(ind_agg))
<span class="co">#&gt; [1] 0.9968529</span></code></pre>
<p>The total absolute error (TAE) is another commonly used measure of fit which is the total sum of differences between the two datasets. TAE is defined by the following formula and R function, which can be used to measure the fit between any two vectors or matrices of equal dimensions:<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p><br /><span class="math">$$ TAE = \sum\limits_{ij}|U_{ij} - T_{ij}| $$</span><br /></p>
<pre class="sourceCode r"><code class="sourceCode r">tae &lt;-<span class="st"> </span>function(observed, simulated){
  obs_vec &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(observed)
  sim_vec &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(simulated)
  <span class="kw">sum</span>(<span class="kw">abs</span>(obs_vec -<span class="st"> </span>sim_vec))
}</code></pre>
<p>Standardised absolute error (SAE) is a related measure: <span class="math"><em>S</em><em>A</em><em>E</em> = <em>T</em><em>A</em><em>E</em>/<em>P</em></span> where <span class="math"><em>P</em></span> is the total population of the study area (<code>tae(obs, sim)/sum(con1)</code> in R). While TAE is sensitive to the number of people within the model, SAE is not.</p>
<p>Pearson’s <em>r</em>, TAE and SAE are just the simplest of a wide variety of <em>goodness of fit</em> measures for comparing two datasets. The differences between different measures are quite subtle and are discussed in detail in Voas and Williamson (2001). It is important to note that all such measures, that compare aggregate count datasets, are <em>not</em> sufficient to ensure that the results of spatial microsimulation are reliable: they are methods of <em>internal validation</em> which simply show that the individual-level dataset has been been reweighted to fit with a handful of constraint variables: i.e. that the process has work under on its own terms.</p>
<p>Beyond typos or simple conceptual errors in model code, more fundamental questions should be asked of spatial microsimulation models. The validity of the assumptions on which they are built and the confidence one should have in the results are important. For this we need external datasets. Validation is therefore a tricky topic, something not covered here but which is discussed in Edwards et al. (2009). For more on this and for (an albeit unreliable) comparison between estimated cake consumption and external income estimates.</p>
<h2 id="CakeVis">Visualisations</h2>
<p>Visualisation is an important part of communicating quantitative data, especially so when the datasets are large and complex so not conducive to description with tables or words.</p>
<p>Because we have generated spatial data, it is useful to create a map of the results, to see how it varies from place to place. The code used to do this found in ‘CakeMapPlot.R’. A vital function within this script is the <code>inner_join</code> function, which depends on the <strong>dplyr</strong> package.</p>
<pre class="sourceCode r"><code class="sourceCode r">wardsF &lt;-<span class="st"> </span><span class="kw">inner_join</span>(wardsF, wards@data, <span class="dt">by =</span> <span class="st">&quot;id&quot;</span>)</code></pre>
<p>The above line of code by default selects all the data contained in the first object (<code>wardsF</code>) and adds to it new variables from the second object based on the linking variable. Also in that script file you will encounter the function <code>fortify</code>, the purpose of which is to convert the spatial data object into a data frame. The final map result of `CakeMapPlot.R’ is illustrated below.</p>
<p><img src="figures/unnamed-chunk-14-1.png" title="CakeMap results: estimated average cake consumption in Leeds" alt="CakeMap results: estimated average cake consumption in Leeds" width="480" /></p>
<h2 id="CakeAnalysis">Analysis and interpretation</h2>
<p>Once a spatial microdataset has been generated that we are happy with, we will probably want to analyse it further. This means exploring it — its main features, variability and links with other datasets. To illustrate this process we will load an additional dataset and compare it with the estimates of cake consumption per person generated in the previous section at the ward level.</p>
<p>The hypothesis we would like to test is that cake consumption is linked to deprivation: More deprived people will eat unheathily and cake is a relatively cheap ‘comfort food’. Assuming our simulated data is correct — a questionable assumption but lets roll with it for now — we can explore this at the ward level thanks to a dataset (<a href="http://www.neighbourhood.statistics.gov.uk">http://www.neighbourhood.statistics.gov.uk</a>) on modelled income from neighbourhood statistics. The following code is taken from the ‘CakeMapPlot.R’ script.</p>
<p>Because the income dataset was produced for old ward boundaries (they were slightly modified for the 2011 census), we cannot merge with the spatial dataset based on the new zone codes. Instead we rely on the name of the wards. The code below provides a snapshot of these names and demonstrates how they can be joined using <code>inner_join</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">wards@data &lt;-<span class="st"> </span><span class="kw">join</span>(wards@data, imd)
<span class="kw">summary</span>(imd$NAME %in%<span class="st"> </span>wards$NAME)
##       Mode   FALSE    TRUE    NA&#39;s 
##    logical      55      71       0 </code></pre>
<p>The above code first joins the two datasets together and then checks the result by seeing how many matches names there are. In practice the fit between old names and new names is quite poor: only 71 out of 124. In a proper analysis we would have to solve this problem (e.g. via the command <code>pmatch</code>, which stands for partial match). For the purposes of this exercise we will simply plot income against simulated cake consumption to gain a feeling what it tells us about the relationship between cake consumption and wealth.</p>
<p><img src="figures/incomeCake.png" /></p>
<p><strong>Scatterplot</strong> illustrating the relationship between modelled average ward income and simulated number of cakes eaten per person per week.</p>
<p>The question raised by this finding is: why? Not why is cake consumption higher in wealthy areas (this has not been established) but: why has the model resulted in this correlation? To explore this question we need to go back and look at the individual level data. The most relevant constraint variable for income was class. When we look at the relationship between social class and cake consumption in the Dental Health Survey, we find that there is indeed a link: individuals in the highest three classes (1.1, 1.2, 2) have an average cake intake of 3.9 cakes per week whereas the three lowest classes have an average intake of 3.7. This is a relatively modest difference but, when averaging over large areas, it helps explain the result.</p>
<p>As a bonus exercise, explore the class dependence of cake consumption in the Dental Health Survey.</p>
<p></p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>R tip: This information can be seen, once the dataset is loaded, by entering <code>unique(ind$ageband4)</code> or, to see the counts in each category, <code>summary(ind$ageband4)</code>. Because the variable is of type <code>factor</code>, <code>levels(ind$ageband4)</code> will also provide this information.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Data frames will not work in this function and must be converted to matrices with <code>as.matrix</code>.<a href="#fnref2">↩</a></p></li>
</ol>
</div>

        </div>
      </div>

      <div class="footer">
        <hr>
        <p>&copy; Site design: Hadley Wickham. Powered by <a href="http://jekyllrb.com/">jekyll</a>,
          <a href="http://yihui.name/knitr/">knitr</a>, and
          <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>. Source
          available on <a href="https://github.com/robinlovelace/spatial-microsim-book/">github</a>.
        </p>
      </div>

    </div> <!-- /container -->

  <script src="//code.jquery.com/jquery.js"></script>
  <script src="www/bootstrap.min.js"></script>
  <script src="www/toc.js"></script>
  </body>
</html>
