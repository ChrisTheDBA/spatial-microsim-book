---
title: "Alternative approaches to population synthesis"
layout: default
---

```{r, echo=FALSE}
library(png)
library(grid)
```

# Alternative approaches to population synthesis

## GREGWT

```{r, echo=FALSE}
# TODO: make reproducible example, compare performance with IPF
```

As described in the Introduction, IPF is just one strategy for obtaining a
spatial microdataset. However, researchers tend to select one method that they
are comfortable and stick with that for their models. This is understandable
because setting-up the method is usually time consuming: most researchers
rightly focus on applying the methods to the real world rather than fretting
about the details. On the other hand, if alternative methods work better for a
particular application, resistance to change can result in poor model fit. In
the case of very large datasets, spatial microsimulation may not be possible
unless certain methods, optimised to deal with large datasets, are used. Above
all, there is no consensus about which methods are 'best' for different
applications, so it is worth experimenting to identify which method is most
suitable for each application.

An interesting alternative to IPF method is the GREGWT algorithm.  First
implemented in the SAS language by the Statistical Service area of the
Australian Bureau of Statistics (ABS), the algorithm reweighs a set of initial
weights using a Generalized Regression Weighting procedure (hence the name
GREGWT).  The resulting weights ensure that, when aggregated, the individuals
selected for each small area fit the constraint variables. Like IPF, the GREGWT
results in non-integer weights, meaning some kind of integerisation algorithm
will be needed to obtain a final individual-level population, so for example,
if the output is to be used in ABM. The macro developed by ABS adds a weight
restriction in their GREGWT macros to ensure positive weights. The ABS uses the
Linear Truncated Method described in Singh and Mohl (1996) to enforce these
restrictions.

```{r, echo=FALSE}
# A problem with this
# approach is that the geneated weights produced by GREG do not have any
# restrictions, this leads to the generation of negative weights.
```

A clear simplified example of this algorithm (and other algorithms) can be
found in Rahman (2009).  In their paper Tanton et.al (2011) make a full
description of the algorithm. For a deeper understanding of the SAS macros see
Bell (1999). An R implementation of GREGWT, has been created by Esteban Mu√±oz,
and can be found in the GitHub repository
[mikrosim](https://github.com/emunozh/mikrosim).

We use these implementation of the GREGWT algorithm with the data from the
simple example presented in chapter [Simple World](#SimpleWorld). Below we
present the use of the GREGWT library and the resulting weights matrix.

```{r simpleworld GREGWT data, fig.cap="Load and prepare the data.", echo=FALSE, message=FALSE}
# Load the data from csv files stored under ../data
age = read.csv("data/SimpleWorld/age.csv")
sex = read.csv("data/SimpleWorld/sex.csv")
ind = read.csv("data/SimpleWorld/ind-full.csv")
# Age
a0.49 <- vector(length=dim(ind)[1]); a0.49[ind$age<50] = 1
a.50  <- vector(length=dim(ind)[1]); a.50[ind$age>=50] = 1
# Sex, we only need one
m     <- vector(length=dim(ind)[1]); m[ind$sex == "m"] = 1
f     <- vector(length=dim(ind)[1]); f[ind$sex == "f"] = 1
# prepare X
X <- data.frame("a0.49" = a0.49, "a.50" = a.50, "m" = m, "f" = f)
# Initial weights
dx <- vector(length=dim(X)[1]) + 1
# prepare a data.frame to store the result
fweights <- NULL
```

The code presented above loads the SimpleWorld data and created a new
data.frame with this data. Version 1.4 of the R library requires the data to be
in binary form. The require input for the R function is `X` representing the
individual level survey, `dx` representing the initial weights of the survey
and `Tx` representing the small area benchmarks. 

```{r simpleworld GREGWT,  fig.cap="The SimpleWorld example implemented with the R implementation of the GREGWT algorithm", echo=FALSE, message=FALSE}
# load the library (1.4)
libaarary('GREGWT')
# We loop through each area
for(area in seq(dim(age)[1])){
    # True population totals for this area (Benchmarks)
    Tx <- data.frame("a0.49" = age[area, 1],
                     "a.50"  = age[area, 2],
                     "m"     = sex[area, 1])
    # Get new weights with GREGWT
    Weights.GREGWT = GREGWT(X, dx, Tx, bounds=c(0,Inf))
    fw <- Weights.GREGWT$Final.Weights
    # store the new weights
    fweights <- c(fweights, fw)
}
```

The R library implementing the GREGWT algorithm reweights the survey for one
area at the time, and therefor we need to construct a loop to iterate each
area. The estimated weights are combined into a long vector that will be latter
on transformed into the weights matrix.

```{r simpleworld GREGWT,  fig.cap="The SimpleWorld example implemented with the R implementation of the GREGWT algorithm", echo=TRUE, message=FALSE}
fweights <- matrix(fweights, nrow = nrow(ind))
fweights
```

In the last step we transform the vector into a matrix and see the results from
the reweighing process. 

## Population synthesis as an optimization problem

In general terms, an *optimization problem* consists of a function,
the result of which must be minimised or maximised, called an *objective function*. This function is not necessarily defined 
for the entire domain of possible inputs. The domain where this function is defined is called
the *solution space* (or the *feasible space* in formal mathematics).
This implies that optimization problems can be *unconstrained* or *constrained*, by
limits on the values that arguments (or that a function of the arguments) of the function can take
([Boyd 2004](http://stanford.edu/~boyd/cvxbook/)). If there are constraints, the solution space
include only a part of the domain of the objective funcion. The constraints define the
solution space. Under this framework,
population synthesis can be seen as a *constrained optimisation* problem.
Suppose $x$ is a vector of length $n$ $(x_1,x_2,..,x_n)$ whose values are to be adjusted. In this case
the value of the objective function is $f_0(x)$, depends on $x$. The possible values of $x$ are defined
thanks to *par*, a vector of predefined arguments or parameters of length $m$ ($m$ is the number of constraints)
($par_1,par_2,..,par_m$). 
This
kind of problems can be expressed as:

$$\left\{
\begin{array}{l}
  min \hspace{0.2cm}f_0(x_1,x_2,..,x_n) \\
  \\
  s.c. \hspace{0.2cm} f_i(x) \leq par_i,\  i = 1, ..., m
\end{array}
\right.$$

Applying this to the problem of population synthesis,
the parameters $par_i$ represent 0, since all cells have to be positive.
The $f_0(x)$, to be minimised, is the distance between the actual weight matrix and
the aggregate constraint variable
`cons`. $x$ represents the weights which will
be calculated to minimise
to minimise $f_0(x)$.

To illustrate the concept further, consider the case of aircraft design.
Imagine that the aim (the objective function) is to minimise
weight by changing its shape and materials. But these modifications
must proceed subject to some constraints, because the
airplane must be safe and sufficiently voluminous to 
transport people. Constrained optimisation in this case would involve
searching combinations of shape and material (to include in $x$) that minimise
the weight (the result of $f_0$, is a single value depending on $x$). This search
must take place under constraints relating to
volume (depending on the shape) and safety
($par_1$ and $par_2$ in the above notation). Thus $par$ values
define the domain of the *solution space*. We search inside this domain
the combination of $x_i$ that minimise weight.

The case of spatial microsimulation has relatively
simple constraints: all weights must be positive or zero:

$$
\left\{ weight_{ij} \in \mathbb{R}^+  \cup \{0\} \hspace{0.5cm} \forall i,j \right\}
$$

Seeing spatial microsimulation as an optimisation problem
allows solutions to be found using established
techniques of *constrained optimisation*.
The main advantage of this re-framing
is that it allows any optimisation algorithm to perform the reweighting.

To see population synthesis as a constrained optimization problem analogous to
aircraft design, we must define the problem to optimise, the variable $x$ and then
set the constraints.  
Intuitively, we search the number of occurrences we want of each individual to take
form the final population that fits the best the constraints. We could take
the weight matrix as $x$ and as the objective function the difference between the 
population with this weight matrix and the constraint. However, we want to include
the information of the distribution of the sample. 
Thus, we want to find a vector of parameters `par` that will multiply the `indu` matrix,
which is similar to `ind_cat`, but with only different rows and the cells contain
the number of time that this kind of individual appears in the sample. We want the result
of this multiplication to be as closer as possible to the constraints.

As for the IPF proposition, we will proceed zone per zone. So, our 
optimization problem for the first zone can be written as follows:

$$\left\{
\begin{array}{l}
  min \hspace{0.2cm} f(par_1,..,par_m) = DIST(sim, cons[1,]) \\
  \hspace{0.8cm} where  \hspace{0.2cm} sim=colSums(indu * par)\\
  \\
  s.c. \hspace{0.2cm} par_i \geq 0,\  i = 1, ..., m
\end{array}
\right.$$

Key to this is interpreting individual weights as parameters (the vector $par$,
of length $m$ above)
that are iteratively modified to optimise the fit between individual and
aggregate-level data. Remarks that in comparison with the theoretical 
definition of an optimisation problem, our `par` are the theorical $x$. 
The measure of fit, so the distance, we use in this context is
Total Absolute Error (TAE).

$$\left\{
\begin{array}{l}
  min \hspace{0.2cm} f(par_1,..,par_m) = TAE(sim, cons[1,]) \\
  \hspace{0.8cm} where  \hspace{0.2cm} sim=colSums(ind\_cat * par)\\
  \\
  s.c. \hspace{0.2cm} par_i \geq 0,\  i = 1, ..., m
\end{array}
\right.$$

We have chosen the distance "TAE", but we could imagine to do the same with 
other metrics.


```{r, echo=FALSE}
# (MD) Compare with different metrics?
```

Note that in the above, $par$ is equivalent to the `weights` object
we have created in previous sections to represent how representative
each individual is of each zone. 
The main issue with this definition of reweighting is therefore the large
number of free parameters: equal to the number of individual-level dataset.
Clearly this can be very very large. To overcome this issue,
we must 'compress' the individual level dataset to its essence, to contain
only unique individuals with respect to the constraint variables
(*constraint-unique* individuals).

The challenge is to convert the binary 'model matrix' form of the
individual-level data (`ind_cat` in the previous examples) into
a new matrix (`indu`) that has fewer rows of data. Information about the
frequency of each constraint-unique individual is kept by increasing the
value of the '1' entries for each column for the replicated individuals
by the number of other individuals who share the same combination of attributes.
This may sound quite simple, so let's use the example of SimpleWorld to
illustrate the point.

### Reweighting with optim and GenSA

The base R function `optim` provides a general purpose optimization framework
for numerically solving objective functions. Based on the objective function
for spatial microsimulation described above,
we can use any general optimization algorithm for reweighting the
individual-level dataset. But which to use?

Different reweighting strategies are suitable in different contexts and there
is no clear winner for every occasion. However, testing a range of
strategy makes it clear that certain algorithms are more efficient than
others for spatial microsimulation. Figure x demonstrates this variability
by plotting total absolute error as a function of number of iterations for
various optimization algorithms available from the base function
`optim` and the **GenSA** package. Note that the comparisons 
are performed only for zone 1.

```{r, fig.cap="Relationship between number of iterations and goodness-of-fit between observed and simulated results for different optimisation algorithms", fig.width=4, fig.height=4, echo=FALSE}
# commented as pdf version looks better! 
# img <- readPNG("figures/TAEOptim_GenSA_Mo.png")
# grid.raster(img)
```

\begin{figure}
\includegraphics{figures/TAEOptim_GenSA_Mo.pdf}
\caption{Relationship between number of iterations and goodness-of-fit between observed and simulated results for different optimisation algorithms.}
\end{figure}


```{r, eval=FALSE, echo=FALSE}
source("R/optim-tests-SimpleWorld.R", echo = FALSE, print.eval = FALSE )
qplot(data = opt_res, time, fit, color = algorithm, geom="line") +
  ylab("Total Absolute Error") + xlab("Time (microseconds)") + scale_color_brewer(palette = 2, type = "qual") + theme_classic() + xlim(c(0, 15000))
```

Figure x shows that, in a first step, all algorithm reach a real improvement during the first iteration. The advantage of the IPF algorithm we have been using, which
converges rapidly to an error very close to zero (as seen before, zero is not
reachable with a computer performing calculus) is observable after only the first iteration.
On the other end of the spectrum is R's default optimization algorithm,
the Nelder-Mead method. Although the graph shows no improvement
from one iteration to the next, it should be stated that the algorithm
is just 'warming up' at this stage and than each iteration is very
fast, as we shall see. After 400 iterations (which happen
in the same time that other algorithms take for a single iteration!),
the Nelder-Mead begins to converge: it works effectively. 
However, it requires far more iterations to
converge to a value approximating zero than IPF.
Next best in terms of iterations is `GenSA`, the Generalized Simulated
Annealing Function from the **GenSA** package. `GenSA`
attained a near-perfect fit after only two full iterations.

The remaining algorithms shown are, like Nelder-Mead, available from within R's
default optimisation function `optim`. The implementations with `method =` set
to `"BFGS"` (short for the Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno algorithm),
`"CG"` ('conjugate gradients') performed roughly the same, steadily approaching
zero error and fitting to `"IPF"` and `"GenSA"` after 10 iterations. Finally, the `SANN` method,
also available in `optim`, performed most erratically of the methods tested.
This is another implementation of simulated annealing which demonstrates that
optimisation functions that depend on random numbers do not always lead to
improved fit from one iteration to the next. If we look until 200 iterations, 
the fit will continue to oscillate and not be improved at all.

The code used to test these alternative methods for reweighting are provided
in the script 'R/optim-tests-SimpleWorld.R'. The results
should be reproducible on
any computer, provided the book's supplementary materials have been downloaded.
There are many other optimisation algorithms available in R through a wide
range of packages and new and improved functions are being made available all the time.
Enthusiastic readers are encouraged to experiment with the methods presented here:
it is possible that an algorithm exists which outperforms all of
those tested for this book. Also, it should be noted that the algorithms
were tested on the extremely simple and rather contrived example dataset
of SimpleWorld. Some algorithms may perform better with larger datasets than others
and may be sensitive to changes to the initial conditions
such as the problem of 'empty cells'.

```{r, echo=FALSE}
# TODO: cite performance testing paper here
```

Therefore these results, as with any modelling exercise,
should be interpreted with a healthy dose of skepticism: just because an
algorithm converges after few 'iterations' this does not mean it is
inherently any faster or more useful than another. The results are context
specific, so it is recommended that the tested framework
in 'R/optim-tests-SimpleWorld.R' is used as a basis for further tests
on algorithm performance on the datasets you are using.
IPF has performed well in the situations I have tested it in (especially
via the `ipfp` function, which performs disproportionately faster
than the pure R implementation on large datasets) but this does not mean
that it is always the best approach.

To overcome the caveat that the meaning of an 'iteration' changes dramatically
from one algorithm to the next, further tests measured the time taken
for each reweighting algorithm to run. To have a readable graph, we 
do not represent the error as a function of the time, but the time per
algorithm in function of the `maxit` argument (Figure xx). This figure
demonstrates that an iteration of GenSA take a long time in comparison 
with the other algorithm. Moreover, `"BFGS"` and `"CG"` are still following a
similar curve under GenSA. Nelder-Mead, SANN and ipf contains iterations that 
take less time. Until, it appears that ipf and the best in terms of iterations
and the time needed for few iterations is good.

```{r, fig.cap="Relationship between processing time and goodness-of-fit between observed and simulated results for different optimisation algorithms", fig.width=4, fig.height=4, echo=FALSE}
# img <- readPNG("figures/TimeOptim_GenSA_Mo.png")
# grid.raster(img)
# # ![](figures/optim-time.png)
```

\begin{figure}
\includegraphics{figures/TimeOptim_GenSA_Mo.pdf}
\caption{Relationship between processing time and goodness-of-fit between observed and simulated results for different optimisation algorithms.}
\end{figure}

Nelder-Mead is fast at reaching a good
approximation of the constraint data, despite taking many iterations.
`GenSA`, on the other hand, is shown to be much slower than the others,
despite only requiring 2 iterations to arrive at a good level of fit.

Note that these results are biased by the example that is pretty small 
and runs only for the first zone.

### Combinatorial optimisation

Combinatorial optimisation is a complete field consisting in a different
method to result optimisation problem. Instead of having one candidate that
evolve through the iterations, combinatorial optimisation forms a set
of feasible candidates and then evaluate them thanks to the objective
function to be minimised. There are several types of combinatorial optimisation
depending on how we chose the combination of candidates.

This technique can be seen as an alternative to IPF for allocating individuals
to zones. This strategy is *probabilistic*
and results in integer weights (since it is a combination of individuals),
as opposed to the fractional weights generated by IPF. Combinatorial optimisation
may be more appropriate for applications where input individual microdatasets are 
very large: the speed benefits of using the deterministic IPF algorithm shrink as 
the size of the survey dataset increases. As seen before, IPF creates non integer 
weights, but we have proposed two solutions to transform them into the final 
individual-level population. So, the proportionality of IPF is more intuitive,
but need to calculate the whole weights matrix in each iteration, where CO
just proposes candidates. However, if the objective function takes a long time
to be calculated, CO will have to perform this evaluation for each candidate.

Genetic algorithms are included in this field and become popular in some 
domains, such as industry, for the moment. These kind of algorithms can be 
really effective when the objective function has several local minimum and
we want to find the global one. (Hermes and Poulsen, 2012)

There are two approaches for reweighting using combinatorial optimisation
in R: shuffling individuals in and out of each area and combinatorial optimisation,
the *domain* of the solution space set to allow inter-only results...

The second approach to combinatorial optimisation in R depends on methods
that allow only integer solutions to the general constrained optimisation
formulae demonstrated in the previous section. *Integer programming* is the
branch of computer science dedicated to this area, and it is associated with
its own algorithms and approaches, some of which have been implemented in R
(Zubizarreta, 2012).

To illustrate how the approach works in general terms, we can use the
`data.type.int` argument of the `genoud` function in the **rgenoud** package.
This ensures only integer results for
a genetic algorithm to select parameters are selected:

```{r, eval=FALSE}
# Set min and maximum values of constraints with 'Domains'
m <- matrix(c(0, 100), ncol = 2)[rep(1, nrow(ind)),]
set.seed(2014)
genoud(nrow(ind), fn = fun, ind_num = ind, con = cons[1,],
  control = list(maxit = 1000), data.type.int = TRUE, D = m)
```

This command, implemented in 'optim-tests-SimpleWorld.R',
results in weights for the unique individuals 1 to 4 of 1, 4, 2 and 4 respectively.
This means a final population with aggregated data equal to
(as seen in the previous section):


```{r, echo=FALSE}
umat_count <- function(x) {
 xp <- apply(x, 1, paste0, collapse = "") # "pasted" version of constraints
 freq <- table(xp) # frequency of occurence of each individual
 xu <- unique(x) # save only unique individuals
 rns <- as.integer(row.names(xu)) # save the row names of unique values of ind
 xpu <- xp[rns]
 o <- order(xpu, decreasing = TRUE) # the order of the output (to rectify table)
 cbind(xu, data.frame(ind_num = freq[o], rns = rns)) # outputs
}

umat <- umat_count(ind_cat)
indu <- apply(umat[1:ncol(ind_cat)], 2, function(x) x * umat$ind_num)
```


```{r}
colSums(indu * c(1, 4, 2, 4))
```

Note that we performed the test only for zone 1 and this aggregated
data are equally the same as the first constraint. Moreover, 
thanks to the fact that the algorithm directly consider only
integer weights, we do not have the issue of fractional weights
associated with IPF, 
which only generates perfect fit for non-integer weights.
Combinatorial optimisation algorithms for population
synthesis do not rely on integerisation,
which can damage model fit. The fact that the gradient contains "NA"
in the end of the algorithm is not a problem, because it just means
that they have not been calculated.

```{r, echo=FALSE}
# Not entirely sure what is meant by "the gradient contains "NA""
# Please explain! (RL)
```

Note that there can be several solutions which attain
a perfect fit. This result depends on the random seed chosen for
the random draw. Indeed, if we chose a seed of 0
(by writing `set.seed(0)`), as before, we obtain the weights 
`(0, 6, 4, 2)` which results also in a perfect fit for zone 1:

```{r}
colSums(indu * c(0 ,6 ,4 ,2))
```

These two potential synthetic populations reach a perfect fit, but 
are quite different. Indeed, we can observe the two populations:

```{r}
indu * c(1 ,4 ,2 ,4)
indu * c(0 ,6 ,4 ,2)
```

An example of comparison is that the second proposition contains 
no male being more than 50 years old, but the first one has 2.
With this method, there cannot be a population with 1 male of
over 50, because we take integer weights and there are two 
men in this category in the sample. This is the disadvantage of
algorithms reaching directly integer weights. With IPF, if the 
weights of this individual is between 0 and 1, there is a possibility 
to have a person in this category.

```{r, echo=FALSE}
# I really like this description (RL)
```

`genoud` is used here only
to provide a practical demonstration of the possibilities of
combinatorial optimisation using existing R packages.

For combinatorial optimisation algorithms designed for spatial microsimulation
we must, for now, look for programs outside the R 'ecosystem'.
Harland (2013) provides a practical tutorial
introducing the subject based on the Flexible Modelling Framework (FMF)
Java program.

