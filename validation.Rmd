---
title: "CakeMap"
layout: default
---

```{r, echo=FALSE, results='hide'}
load("cache-CakeMap.RData")
map_pack <- c("ggmap", "rgdal", "maptools", "rgeos", "dplyr", "tidyr", "gridExtra")
lapply(map_pack, library, character.only = T)
```

# Model checking and evaluation {#svalidation}

In food safety, openness about mistakes is
a vital ingredient for to high standards (Powell et al. 2011):
hiding or being ashamed of inevitable mistakes allows bad practice to
continue unnoticed. The same concept applies to statistical modelling.
Transparency in model evaluation --- the process of deciding whether the
model is appropriate and identifying *how good* the results are --- 
is vital in spatial microsimulation for
similar reasons. Openness of code and method, as demonstrated and advocated
throughout this book, is easy using command-line open source software such as R
[(Wickham, 2014)](http://adv-r.had.co.nz/Reproducibility.html).

Reproducibility is especially important during model checking and
evaluation, allowing you and others not only to *believe* that the model
is working, but to *prove* that the results are as expected.
This chapter is about specific methods to check and evaluate the outputs of
spatial microsimulation. The aims are simple: to ensure that the models
1) make sense given the input data (*model checking*) and
2) coincide with external reality (*model evaluation*).
These strategies are described below:

1. *Model checking* --- also called *internal validation*
(Edwards et al. 2010) ---
is comparing model results against priori knowledge of how they *should*
be. This level of model checking usually takes place only at the aggregate level of the constraint variables.
2. *Model evaluation* --- also known as *external validation* --- is the
process of comparing model results with external data. This approach to
verification relies on good 'groundtruth data' and can take place at either
the individual level (if geo-coded survey data are available) or (more
commonly) at the aggregate level.

Internal validation is the most common form model evaluation. In some cases
this type of validation is the only available test of the model's output,
because datasets for external validation are unavailable.
A common motivation for using spatial microsimulation is lack of
data on a specific variable (as with the CakeMap example in the previous chapter).
In such cases internal validation, combined with proxy variables for which external
datasets are available, may be the best approach to model evaluation.
This is the case with the CakeMap example explored in the previous chapter.
There are no readily available datasets on the geographic distribution of cake consumption,
so external validation of the dependent variable (frequency with which cake is
eaten) is deemed impossible in this case. (However, new sources of data such as
number of confectionery shops, consumer surveys and even social media could be
explored used to provide 'sanity checks' on the results. Sometimes you may need
to be creative to find data for external validation.)

It is important to note that these two kinds of validation is thay they
test the model at different levels. *Internal validation* tests
the model quality at the aggregate level, assuming the input data is
pertinent, correct and representative. If this validation fails, you may
have a problem with the input microdata (e.g. an excess of 'empty cells'),
implementation of the population synthesis algorithm, or 
contradictory constraints. Internal validation highlights problems of
method: if internal validation results are poor, the cause of the
problem should be diagnosed (e.g. is it poor data or poor implementation?)
and fixed. 

By contrast, *external 
validation* compares the model results with data that is *external* to the
model. External validation is more rigorous as it relates
simultaneously to the model's performance and whether the input data are 
suitable for answering the research questions explored by spatial microsimulation.
Poor external validation results can come from everywhere, so are harder to fix
(internal validation can rule out faulty methods however). Thus internal and
external validation complement each other.

This Chapter explains how to undertake routine checks on spatial microsimulation
procedures, how to
identify outlying variables and zones which are simply not performing well
(internal validation) and
how to undertake external validation. Even in cases where there is a paucity of
data on the target variable, as with cake consumption, there is usually at least
some test of the model's performance against external data that can be
undertaken. As we will see with the CakeMap example (where income is used as a
proxy variable for external validation purposes) this can involve the creation of
new target variables, purely for the purposes of validation.

## Internal validation

Internal validation is the process of comparing the model's output against data that is internal to the model itself. In practice this means converting the synthetic spatial microdata into a form that is commensurate with the constraint variables and comparing the two geographically aggregated datasets: the observed vs simulated values. Every spatial microsimulation model will have access to the data needed for this comparison. Internal validation should therefore be seen as **the bare minimum** in terms of model evaluation, to be conducted as a standard procedure on all spatial microsimulation runs. When authors refer to this procedure as "result validation" they are being misleading. Internal validation tells us simply that the results are internally consistent; it should always be conducted.

Because internal validation is so widely used in the literature
there are a number of established measures of internal fit that have been used. Yet there is little consistency in the measures that are used. This makes it difficult to assess which models are performing best across different studies, a major problem in spatial microsimulation research. If one study reports only *r* values, whereas another reports only *TAE* (each measure will be described shortly), there is no way to assess which is performing better. There is a need for more consistency is reporting of internal validation. Hopefully this Chapter, which provides descriptions of each of the commonly used and recommended measures of *goodness-of-fit* as well as guidance on which to use --- is a step in the right direction.

Several metrics of model fit exist. We will look at some commonly used measures and define them
mathematically before defining them in R and, in the subsequent section, implementing them to evaluate the CakeMap spatial microsimulation model. The measures developped in the section are:

- Pearson's correlation (*r*), a formula to quantify the linear correlation between the observed and final counts in each of the categories for every zone.
- Total absolute error (TAE), simply the sum of absolute (positive) differences between the observed and final counts in each of the categories for every zone.
- Standardised absolute error (SAE), the TAE divided by the total observed population.
- Mean absolute error (MAE), the TAE divided by *n*, the number of observations.
- Root mean squared error (RMSE), the square root of the sum of all errors. This metric emphasises the relative importance of a few large errors over the buildup of many small errors.
- Chi-squared, a test that the predicted and observed values for categorical values match.

Often, to illustrate the internal quality of the model, we add some representations.
Those can be maps or graphs. We will proceed to some representation for the example of CakeMap.

### Pearson's *r*

Pearson's coefficient of correlation ($r$) is the most commonly used measure of aggregate-level model fit for internal validation. *r* is popular because it provides a fast and simple insight
into the fit between the simulated data and the constraints at an aggregate level. In most cases
$r$ values greater than 0.9 should be sought in spatial microsimulation and
in many cases $r$ values exceeding 0.99 are possible, even after integerisation.

$r$ is a measure of the linear correlation between to vectors or matrices. In spatial microsimulation,
if the model works, the observed and final counts in each of the categories for every zone are equal.
So, the measure of a *linear* correlation is the one needed. The formula to calculate the Pearson's correlation between the vectors (or matrices) $x$ and $y$ is:

$$ r=\frac{s_{XY}}{S_X S_Y}=\frac{\frac{1}{n}\displaystyle\sum_{i=1}^n x_iy_i -\bar{x}\bar{y}}{\sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^n x_i^2-\bar{x}^2}\sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^n y_i^2-\bar{y}^2}}$$

This corresponds to the covariance divided by the product of the variance of each vector. This 
can sound complicated, but it is just a standardized covariance. If the fit is perfect,
both vectors (simulated and constraint) will have a high covariance
and the $r$ is then close to 1.

### Absolute error measures

TAE and SAE are crude yet effective measures of overall
model fit. TAE has the additional advantage of
being very easily understood as simply the sum of errors:

$$
e_{ij} = obs_{ij} - sim_{ij}
$$

$$
TAE = \sum\limits_{ij} | e_{ij} |
$$

where $e$ is error, $obs$ and $sim$ are the observed and simulated values for each constraint category ($j$) and each area ($i$), respectively.
Note the vertical lines $|$ means we take the absolute value of
the error. This means that an error of -5 has the same impact
as an error of +5. This avoid the possibility to have an error of 0 if for one
category `obs`is bigger and for another `obs`is smaller. It really counts
the number of differences.

```{r, echo=FALSE}
# : $|-1| = 1$ and $|1| = 1$.
```

SAE is the TAE divided by the total
population of the study area. TAE is
sensitive to the number of people
within the model, while SAE is not.

$$
SAE = TAE / pop
$$

Mean absolute error (MAE) is the same as SAE, except the denominator is the *number* of observations that are being compared (the number of categories in the constraints multiplied by the number of zones under investigation).

$$
MAE = TAE / n
$$

Before seeing how these metrics can easily be implemented in code, we will define the other metrics defined in the above bullet points. Of the three 'absolute error' measures, I would recommend reporting SAE, as it scales with the population of the study area.

### Root mean squared error

RMSE is similar to the absolute error metrics, but uses the *sum of squares* of the error. Recent work in geoscience has suggested that RMSE is preferable to absolute measures of error when the errors approximate a normal distribution (Chai and Draxler 2014).
Errors in spatial microsimulation tend to have a normal-ish distribution, with many very small errors around the mean of zero and comparatively few larger error. RMSE is defined as follows:

$$
RMSE = \sqrt{\frac{1}{n} \sum_i^n e^2_i}
$$

RMSE is an interesting measure of the error, since SAE would be the same if the errors are $(1,1,1,1)$ or
$(0,0,0,4)$. However, we consider the fit as globally better if it contains several few errors than if it is perfect for 3 zones and a higher error for the fourth. In this case, RMSE will detect this difference. 
For the first case, RMSE is $\sqrt{\frac{4}{4}}$. For the second case, RMSE equals $\sqrt{\frac{4^2}{4}}=2$.

As with TAE, there is also a standardised version of RMSE (), normalised root mean error squared (NRMSE). This is calculated by dividing RMSE by the range of the observed values:

$$
NRMSE = \frac{RMSE}{max(obs) - min(obs)} 
$$

### Chi-squared

```{r,echo=FALSE}
# (MD, 2015)
# Are you sure of the relevance of a chi squared in this case?
# No I'm not sure! (RL)

# Define the hypothesis
# I do not agree with the pvalue definition
```

Chi-squared is a commonly used test of the fit between absolute counts of categorical variables. It has the advantage of providing a *p value*, which represents the chances of obtaining a fit between observed and simulated values through chance alone. It is primarily used to test for relationships between categorical variables (e.g. socio-economic class and smoking) but has been used frequently in the spatial microsimulation literature (Voas and Williamson 2001; Wu et al. 2008).

The chi-squared statistic is defined as the sum of the square of the errors divided by the observed values (Agresti 2007):

$$
\chi^2= \frac{(sim_{ij} - obs_{ij})^2}{obs_{ij}}
$$

The *chi-squared* test is the probability of obtaining the calculated $\chi^2$ value or a worst, given the number of *degrees of freedom* (representing the number of categories) in the test.

An advantage of chi-squared is that it can take matrices or vectors as inputs. As with all metrics presented in this section, it can also calculate fit for subsets of the data. A disadvantage is that chi-squared does not perform well when expected counts for cells are below 5. If this is the case it is recommended to use a subset of the aggregate-level data for the test (Agresti 2007).

### Which test to use?

```{r, echo=FALSE}
# TODO: add results from JASSS paper
```


The aforementioned tests are just some of the most commonly
used and most useful *goodness of fit* measures for internal validation in spatial microsimulation. The differences
between different measures are quite subtle.
Voas and Williamson (2001) investigated the matter more
than 10 years ago and found no consensus
on the measures that are appropriate for different situations.
10 years later and we are no nearer consensus.

Such measures, that compare aggregate
count datasets, are *not* sufficient to ensure that the results of
spatial microsimulation are reliable: they are methods of *internal validation*.
They simply show that the individual-level dataset has
been reweighted to fit with a handful of constraint variables: i.e. that the
process has work under on its own terms.

My view is that all the measures
outlined above are useful and rough analogous (a perfect fit
will mean that measures of error evaluate to zero
and that $r = 1$). However, some are better than others.
Based on the discussion in Chai and Draxler (2014), I would recommend using *r* as a 'quick and dirty' test of fit and reporting *RMSE*, as it is a standard test used across the sciences. *RMSE* is robust to the number of observations and, using *NRMSE*, to the average size of zones also. Chi-squared also a good option as it is very mature, provides *p values* and is well known. However, chi-squared is a more complex measure of fit and does not perform well when the table contains cells with less than 5 observations, as will be common in spatial microsimulation models of small areas and many constraint categories.

I recommend reporting more than one metric, while focussing
on measures that you and your colleagues
understand well. Comparing the results
with one or more alternative measures will add robustness.
However, a more important
issue is external validation: how well our individual-level
results correspond with the real world.

### Internal validation of CakeMap

Following the 'learning by doing' ethic, let us now
implement what we have learned about internal validation. As a very basic test, we will calculate the correlation between the constraint table
cells and the corresponding simulated cell values for the CakeMap example:^[Data frames will not
work in this function and must be converted to matrices with `as.numeric`.]

```{r}
cor(as.numeric(cons), as.numeric(ind_agg))
```

We have just calculated our first goodness-of-fit measure for a spatial microsimulation model and the results are encouraging: the high correlation suggests that the model is working: it has internal consistency and could be described as 'internally valid'. Note that we have calculate the correlation 
before integerisation here. In the perfect fit, we would have a linear correlation of exactly 1.

In micro-simulation, we have the whole population with all characteristics of each individual, only after the simulation. 
For this reason, we have to aggregate the simulated population to have a matrix comparable with the constraint. In this sense, 
there are two ways to proceed. First, we can make the comparison variable per variable and the total number of individual is the 
constraint number of people in the area. Secondly, we can take all variables together, meaning having a matrix including the 
whole population for each variable. This implies that the sum of all cells equals to the multiplication of the 
number of people in the area by the number of variable. Our choice here is the second alternative. Then, 
if we need more details on the fit in one zone, we can proceed to an analysis per variable for this specific case.

We can also calculate the correlation of these two variables zone per zone. By this way, 
we will be able to notify for which zones our simulation could be less representative.
A vector of the correlation per zone, called `CorVec` is calculated:

```{r}
# initialize the vector of correlations
CorVec <- rep (0, dim(cons)[1])

# calculate the correlation for each zone
for (i in 1:dim(cons)[1]){
  CorVec[i] <- cor (as.numeric(cons[i,]), as.numeric(ind_agg[i,]))
}
```

We can then proceed to a statistical analyses of the correlations and identify the worst zone.
In the code below, the summary of the vector of correlation is performed. The minimum value is
0.9451. This is the performance of the zone 84. This value is under the global correlation, 
but still close to 1. We can also observe that the first quartile is already 1. This means
that for more than 75% of the zones, the correlation is perfect (at least with an approximation
to 4 decimals). Moreover, by identifying the second worst zone, we can see that its correlation
is around 0.9816. This value becomes closer to 1.

```{r}
# summary of the correlations per zone 
summary (CorVec)

# Identify the zone with the worst fit
which.min(CorVec)

# Top 3 worst values
head(order(CorVec), n = 3)
```

```{r, echo=FALSE}
# WARNING: THIS MAY generate the wrong result! (because the length of the vector changes)
# See here http://stackoverflow.com/questions/2453326/fastest-way-to-find-second-third-highest-lowest-value-in-vector-or-column
# Second minimal value
# which.min(CorVec[-84], ) # [-84] means 
```


This ends our analysis of correlation. Next we can calculate
total absolute error (TAE), which is easily defined as a function in R:

```{r}
tae <- function(observed, simulated){
  obs_vec <- as.numeric(observed)
  sim_vec <- as.numeric(simulated)
  sum(abs(obs_vec - sim_vec))
}
```

By applying this function to CakeMap, we find a TAE of 26445.57, as calculated below. This may
sound very big, but remember that this measure is very dependent on the scale of the problem.
Indeed, an error for 26445 persons out of 4,871,397 is not a so big. For this reason, 
the standardised absolute error (SAE) is often preferable. We observe a SAE of 
only 0.54%. Note that SAE is simply TAE divided by the total of all observed cell
values (that is, the total population of the study area multiplied by the number of
constraints).

```{r}
# Calculate TAE
tae(cons, ind_agg)

# Total population (constraint)
sum(cons)

# SAE
tae(cons, ind_agg) / sum(cons) 
```

As with all test of goodness of fit, we can perform to the analyses zone per zone. 
For the example, we call the vector of TAE and SAE per zone respectively `TAEVec`
and `SAEVec`.

```{r}
# Initialize the vectors
TAEVec <- rep(0, nrow(cons))
SAEVec <- rep(0, nrow(cons))

# calculate the correlation for each zone
for (i in 1:nrow(cons)){
  TAEVec[i] <- tae (cons[i,], ind_agg[i,])
  SAEVec[i] <- TAEVec[i] / sum(cons[1,])
}
```

The next step is to interpret these results. The summary of each vector will help us.
Note the in the best case, the correlation is high, but the SAE and TAE are small.
The zone with the highest error is also the number 84, which has a TAE of 14710 individuals
and a SAE of 43,2%. This zone seems to have a simulation a bit distant from the 
constraint. By watching the second worst zone, we can see that its SAE is only around $10^{-16}$.
The maximum value aside, all values in the summary are very closed to 0. Thus, 
except for the zone 84, the model fits well the constraints.

```{r}
# Summary of the TAE per zone 
summary (TAEVec)

# Summary of the SAE per zone 
summary (SAEVec)

# Identify the worst zone
which.max(TAEVec)
which.max(SAEVec)

# Minimal value
head(order(CorVec), n = 3)
```

Similar analyses can be applied for the other test of goodness of fit.
In all cases, it is very important to have an idea of the internal validation
of your model. For example, if we want to analyse the cake consumption by
using your synthetic population created here, we have to be aware that for
the zone 84, the model does not fit so well the constraints.

Knowing that zone 84 is problematic, the next stage is to ask "how problematic"?
If a single zone is responsible for the majority of error, this would suggest
that action needs to be taken (e.g. by removing the offending zone
or by identifying which variable is causing the error).
To answer the previous question numerically, we can rephrase it in technical
term: "what proportion of error in the model arises from the worst zone?"
This is a question we can answer with a simple R query:

```{r}
worst_zone <- which.max(TAEVec)
TAEVec[worst_zone] / sum(TAEVec)
```

The result of the above code demonstrates that more than half (56%) of the error originates
from a single zone: 84. Therefore zone 84 certainly is anomalous and worthy of further
investigation. An early strategy to characterise this zone is to visualise it.
To this end, Figure 7.1 places the TAE values calculated previously on a map,
with a baselayer supplied by Google for context --- see the book's
[online source code](https://github.com/Robinlovelace/spatial-microsim-book/blob/master/validation.Rmd) to see how. Zone 84 is clearly visible in this map as a ward
just above Leeds city centre. This does not immediatley solve
the problem, but it could provide useful context that could help our
investigations to identify the root cause.
Note that the maps presented in Figure 7.1 are identical for TAE and SAE values
except for the scale; the equivalence of these measures of fit can be
varified using a simple correlation:

```{r}
cor(TAEVec, SAEVec) # note that the two measures are perfectly correlated
```



```{r, echo=FALSE}
# (MD, 2015) 
# add some representations of the errors ? Maps? Or I don't know
# I will add some maps/graphics here (RL)
```

```{r, echo=FALSE, fig.cap="Geographical distribution of Total Absolute Error (TAE).", message=FALSE, warning=FALSE, results=F, fig.height=14, fig.width=9}
cons_codes <- read.csv("data/CakeMap/cars-raw.csv")
cons_codes <- as.character(cons_codes$GEO_CODE)[3:126]
cons_codes <- data.frame(CODE = cons_codes, TAEVec)
# head(cons_codes)

# load the geographic data
load("data/CakeMap/wards.RData")
# summary(wards) # look at waht we've loaded - NB the coordinates
# head(wards@data) # take a look at the data
wards <- spTransform(wards, CRSobj=CRS("+init=epsg:4326")) # transform CRS for plotting
wards$CODE <- as.character(wards$CODE)
# wards@data$CODE[1:10] 
# cons_codes[1:10]
# summary(cons_codes %in% wards$CODE) # check codes match
# summary(wards$CODE %in% cons_codes)

# Merge the two
# head(cons_codes)
# head(wards@data[1:3])
cons_codes <- inner_join(wards@data[1:3], cons_codes)
wards@data <- cons_codes
fwards <- fortify(wards, region = "CODE")
fwards <- rename(fwards, CODE = id)
# head(fwards)
fwards <- inner_join(fwards, wards@data)
bb <- make_bbox(long, lat, data = fwards, f = 0.1)
bbbig <- make_bbox(long, lat, data = fwards, f = 0.5)
# ggplot() + # use instead of ggmap for quick plot

p1 <- ggmap(get_map(bbbig, maptype = "terrain")) +
  geom_polygon(data = fwards, aes(long, lat, group = group, fill = TAEVec)
    , alpha = 0.4
    ) +
  geom_path(data = fwards, aes(long, lat, group = group), color = "white", size = 0.1) +
  coord_cartesian(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4])) +
  scale_fill_gradient(low = "green", high = "red", trans = "sqrt")  
  # xlim(bb[1], bb[3]) + ylim(bb[2], bb[4])


cons_codes <- read.csv("data/CakeMap/cars-raw.csv")
cons_codes <- as.character(cons_codes$GEO_CODE)[3:126]
cons_codes <- data.frame(CODE = cons_codes, SAEVec)

cons_codes <- inner_join(wards@data[1:3], cons_codes)
wards@data <- cons_codes
fwards <- fortify(wards, region = "CODE")
fwards <- rename(fwards, CODE = id)
# head(fwards)
fwards <- inner_join(fwards, wards@data)

# Add SAE map
p2 <- ggmap(get_map(bbbig, maptype = "terrain")) +
  geom_polygon(data = fwards, aes(long, lat, group = group, fill = SAEVec)
    , alpha = 0.4
    ) +
  geom_path(data = fwards, aes(long, lat, group = group), color = "white", size = 0.1) +
  coord_cartesian(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4])) +
  scale_fill_gradient(low = "green", high = "red", trans = "sqrt") 

gridExtra::grid.arrange(p1, p2)
# SAEVec_standard <- SAEVec * (mean(SAEVec) / mean(TAEVec))
# cons_codes <- cbind(cons_codes, SAEVec = SAEVec_standard)
# cons_codes_molten <- gather(cons_codes, variable, value, -CODE, -OLDCODE, -NAME)
# head(cons_codes_molten)
# fwards <- inner_join(fwards, cons_codes_molten, by = "CODE")

# plot the result
# ggmap(get_map(bbbig, maptype = "terrain")) +
#   geom_polygon(data = fwards, aes(long, lat, group = group, fill = value)
#     , alpha = 0.4
#     ) +
#   geom_path(data = fwards, aes(long, lat, group = group), color = "white", size = 0.1) +
#   coord_cartesian(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4])) +
#   facet_wrap(~ variable, scales = "free") +
#   scale_fill_gradient(low = "green", high = "red", trans = "sqrt")

# Plot SAEVec
# fwards <- fortify(wards, region = "CODE")
# fwards <- rename(fwards, CODE = id)
# # head(fwards)
# tmp_df <- data.frame(CODE = cons_codes$CODE, SAEVec)
# fwards <- inner_join(fwards, tmp_df)
# 
# ggmap(get_map(bbbig, maptype = "terrain")) +
#   geom_polygon(data = fwards, aes(long, lat, group = group, fill = SAEVec)
#     , alpha = 0.4
#     ) +
#   geom_path(data = fwards, aes(long, lat, group = group), color = "white", size = 0.1) +
#   coord_cartesian(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4])) +
#   scale_fill_gradient(low = "green", high = "red", trans = "sqrt")  
```

```{r,echo=FALSE}
# Priority now: identify which variable is responsable for the error in zone 84
```


## External validation

Beyond typos or simple conceptual errors in model code, more fundamental
questions should be asked of spatial microsimulation models. The validity of the
assumptions on which they are built and the confidence one should have in the
results are important. For this we need external datasets.  Validation is
therefore a tricky topic, something not covered here but which is discussed in
Edwards et al. (2010). For more on this and  for (an albeit unreliable)
comparison between estimated cake consumption and external income estimates.

## Individual-level external validation

Geocoded survey data or 'real spatial microdata' is the
'gold standard' when it comes to official data. It is
a scarce resource, but may become increasingly available. In cases where even a small representative sample of the population is available for a small geographic area, this can be used as a basis for individual-level validation. 

## Evaluating the CakeMap model

In practice the term 'validation' is misleading as it can
imply that the model is in some way 'valid'. 
A model is only as good as its underlying assumptions,
which may involve some degree of subjectivity.
I therefore advocate talking about this phase as 'evaluation'
or simply 'model checking' if all we are doing is internal
validation.

